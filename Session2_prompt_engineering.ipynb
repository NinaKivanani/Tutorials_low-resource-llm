{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "467d9f0f",
      "metadata": {},
      "source": [
        "# Session 2: Pretrained Models and Prompt Engineering ðŸ¤–\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "**ðŸ“š Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/Session2_prompt_engineering.ipynb)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "Welcome to **systematic LLM-based prompt engineering** for dialogue summarization and cross-lingual tasks! This session combines rigorous methodology with practical applications, focusing on low-resource language challenges.\n",
        "\n",
        "**ðŸŽ¯ Focus:** Systematic prompt engineering, multilingual evaluation, dialogue summarization  \n",
        "**ðŸ’» Requirements:** GPU recommended for large models OR API access for best results  \n",
        "**ðŸ”¬ Methodology:** Research-grade systematic evaluation with pandas DataFrames\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "**ðŸ“‹ Recommended learning path:**\n",
        "1. **Session 0:** Setup and tokenization basics âœ…  \n",
        "2. **Session 1:** Systematic baseline techniques âœ…\n",
        "3. **This session (Session 2):** Systematic LLM prompt engineering â† You are here!\n",
        "\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this session, you will:\n",
        "- âœ… **Systematically compare** pretrained model families using structured evaluation\n",
        "- âœ… **Design culturally-aware prompts** for dialogue summarization and classification\n",
        "- âœ… **Implement systematic prompt engineering** with quantitative tracking\n",
        "- âœ… **Evaluate cross-lingual performance** using research-grade metrics\n",
        "- âœ… **Generate actionable insights** for production deployment decisions\n",
        "- âœ… **Export structured findings** for research and business applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51338706",
      "metadata": {},
      "source": [
        "## ðŸ“‹ Session Roadmap & Table of Contents\n",
        "\n",
        "**Part 0:** Setup & Prerequisites\n",
        "- 0.1 Access Pattern Decision Framework\n",
        "- 0.2 Package Installation & Imports\n",
        "\n",
        "**Part 1:** Experimental Framework \n",
        "- 1.1 Define Experimental Scope\n",
        "- 1.2 Systematic Test Data Framework\n",
        "\n",
        "**Part 2:** Model Loading & Management\n",
        "- 2.1 Advanced Model Loading\n",
        "- 2.2 Systematic Task Framework\n",
        "\n",
        "**Part 3:** Basic Prompt Engineering\n",
        "- 3.1 Zero-shot, Few-shot, Chain-of-Thought\n",
        "- 3.2 Evaluation Framework\n",
        "\n",
        "**Part 4:** Free API Integration\n",
        "- 4.1 FREE API Setup for Students\n",
        "\n",
        "**Part 5:** Advanced Techniques\n",
        "- 5.1 Self-Consistency Prompting  \n",
        "- 5.2 LLM Hyperparameters Deep Dive\n",
        "\n",
        "**Part 6:** Wrap-up & Next Steps\n",
        "- 6.1 Session Complete & Summary\n",
        "- 6.2 Practical Next Steps\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31db6028",
      "metadata": {},
      "source": [
        "## 0. Systematic Setup and Model Access Framework\n",
        "\n",
        "**Strategic Decision:** Choose your model access pattern based on systematic evaluation of your requirements.\n",
        "\n",
        "### 0.1 Access Pattern Decision Framework\n",
        "\n",
        "| **Access Pattern** | **Pros** | **Cons** | **Best For** | **Cost** |\n",
        "|-------------------|----------|----------|--------------|----------|\n",
        "| **ðŸ”¥ Local Models (mT5)** | Privacy, offline, customizable | GPU required, setup time | Research, sensitive data | Hardware only |\n",
        "| **â˜ï¸ API Access (GPT-4)** | State-of-art, no setup, scalable | Token costs, internet needed | Production, experiments | $0.01-0.03/1K tokens |\n",
        "| **ðŸŒ Hosted (Colab/HF)** | Free tiers, easy setup | Limited resources, usage caps | Learning, prototyping | Free-$10/month |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1efff16a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ðŸ“¦ SECTION 1: Import System Libraries and Configure Environment\n",
        "# ================================================================\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ================================================================\n",
        "# ðŸ”§ SECTION 2: Define Package Installation Function\n",
        "# ================================================================\n",
        "\n",
        "def install_packages_systematic(packages):\n",
        "    \"\"\"Install packages with better error handling and progress tracking\"\"\"\n",
        "    installed = []\n",
        "    failed = []\n",
        "    \n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "            installed.append(package.split(\">=\")[0].split(\"==\")[0])\n",
        "            print(f\"âœ… {package}\")\n",
        "        except Exception as e:\n",
        "            failed.append((package, str(e)[:50]))\n",
        "            print(f\"âŒ {package}: {str(e)[:50]}...\")\n",
        "    \n",
        "    return installed, failed\n",
        "\n",
        "# ================================================================\n",
        "# ðŸš€ SECTION 3: Begin Package Installation Process\n",
        "# ================================================================\n",
        "\n",
        "print(\"ðŸš€ SYSTEMATIC SETUP: Installing advanced packages...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 3.1 Define Required Packages\n",
        "# --------------------------------------------------------\n",
        "\n",
        "# Core packages for systematic evaluation\n",
        "core_packages = [\n",
        "    \"pandas>=1.5.0\",        # Data analysis and manipulation\n",
        "    \"matplotlib>=3.5.0\",    # Plotting and visualization\n",
        "    \"seaborn>=0.11.0\",      # Statistical data visualization\n",
        "    \"numpy>=1.21.0\"         # Numerical computing\n",
        "]\n",
        "\n",
        "# LLM packages (grouped for better dependency management)\n",
        "llm_packages = [\n",
        "    \"transformers>=4.35.0\", # Hugging Face transformers library\n",
        "    \"torch>=1.13.0\",        # PyTorch deep learning framework\n",
        "    \"sentencepiece\",        # Text tokenization\n",
        "    \"accelerate\",           # Training acceleration\n",
        "    \"datasets\"              # Dataset loading utilities\n",
        "]\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 3.2 Install Package Groups\n",
        "# --------------------------------------------------------\n",
        "\n",
        "print(\"ðŸ“Š Installing data science packages...\")\n",
        "core_installed, core_failed = install_packages_systematic(core_packages)\n",
        "\n",
        "print(\"ðŸ¤– Installing LLM packages...\")\n",
        "llm_installed, llm_failed = install_packages_systematic(llm_packages)\n",
        "\n",
        "# Essential imports for systematic evaluation\n",
        "try:\n",
        "    import torch\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "    from typing import List, Dict, Optional, Tuple\n",
        "    import time\n",
        "    import json\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Set plotting style for professional visualizations\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "    \n",
        "    print(f\"\\nðŸŽ¯ SYSTEM CONFIGURATION:\")\n",
        "    print(f\"   Python: {sys.version.split()[0]}\")\n",
        "    print(f\"   PyTorch: {torch.__version__}\")\n",
        "    print(f\"   Pandas: {pd.__version__}\")\n",
        "    print(f\"   GPU Available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"   GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    \n",
        "    print(f\"\\nðŸ“Š EXPERIMENTAL FRAMEWORK READY:\")\n",
        "    print(f\"   âœ… Structured data collection with pandas\")\n",
        "    print(f\"   âœ… Statistical analysis and visualization\")\n",
        "    print(f\"   âœ… Export capabilities for research\")\n",
        "    print(f\"   âœ… Systematic model comparison framework\")\n",
        "    \n",
        "    setup_success = True\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"\\nâŒ IMPORT ERROR: {e}\")\n",
        "    print(\"ðŸ”„ Try restarting the runtime and running this cell again\")\n",
        "    setup_success = False\n",
        "\n",
        "# Verification and troubleshooting\n",
        "if core_failed or llm_failed:\n",
        "    print(f\"\\nâš ï¸  INSTALLATION ISSUES DETECTED:\")\n",
        "    for pkg, error in core_failed + llm_failed:\n",
        "        print(f\"   âŒ {pkg}: {error}\")\n",
        "    print(f\"\\nðŸ’¡ SOLUTIONS:\")\n",
        "    print(f\"   1. Runtime â†’ Restart Runtime, then re-run this cell\")\n",
        "    print(f\"   2. Check internet connection\")\n",
        "    print(f\"   3. Try installing packages individually\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… SYSTEMATIC SETUP COMPLETE!\")\n",
        "print(\"ðŸ”¬ Ready for research-grade prompt engineering experiments\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac65079",
      "metadata": {},
      "source": [
        "## 1. ðŸ”¬ Systematic Experimental Framework Setup\n",
        "\n",
        "**Research-Grade Methodology:** Before testing models, we establish systematic evaluation framework.\n",
        "\n",
        "### 1.1 Define Experimental Scope\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a132a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Your Systematic Experiment: Languages and Tasks\n",
        "# This systematic approach ensures reproducible, comparable results\n",
        "\n",
        "# Define your target languages (CUSTOMIZE THIS FOR YOUR RESEARCH)\n",
        "target_languages = [\n",
        "    {\n",
        "        \"code\": \"en\", \n",
        "        \"name\": \"English\", \n",
        "        \"family\": \"Germanic\",\n",
        "        \"speakers\": \"1.5B\",\n",
        "        \"resource_level\": \"high\",\n",
        "        \"writing_system\": \"Latin\"\n",
        "    },\n",
        "    {\n",
        "        \"code\": \"fr\", \n",
        "        \"name\": \"French\", \n",
        "        \"family\": \"Romance\",\n",
        "        \"speakers\": \"280M\", \n",
        "        \"resource_level\": \"high\",\n",
        "        \"writing_system\": \"Latin\"\n",
        "    },\n",
        "    {\n",
        "        \"code\": \"ar\", \n",
        "        \"name\": \"Arabic\", \n",
        "        \"family\": \"Semitic\",\n",
        "        \"speakers\": \"420M\",\n",
        "        \"resource_level\": \"medium\",\n",
        "        \"writing_system\": \"Arabic\"\n",
        "    },\n",
        "    # ðŸŽ¯ ADD YOUR LOW-RESOURCE LANGUAGE HERE:\n",
        "    # {\n",
        "    #     \"code\": \"your_code\", \n",
        "    #     \"name\": \"Your Language\", \n",
        "    #     \"family\": \"Language Family\",\n",
        "    #     \"speakers\": \"~XXXk\",\n",
        "    #     \"resource_level\": \"low\",\n",
        "    #     \"writing_system\": \"Script\"\n",
        "    # },\n",
        "]\n",
        "\n",
        "# Define systematic task framework for dialogue summarization and classification\n",
        "experimental_tasks = [\n",
        "    {\n",
        "        \"task_id\": \"dialogue_classification\",\n",
        "        \"description\": \"Classify dialogue type: meeting, social, support, transaction\",\n",
        "        \"evaluation_type\": \"categorical_accuracy\",\n",
        "        \"cultural_sensitivity\": \"medium\",\n",
        "        \"domain\": \"general\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"dialogue_summarization\", \n",
        "        \"description\": \"Generate concise summary of dialogue content\",\n",
        "        \"evaluation_type\": \"generative_quality\",\n",
        "        \"cultural_sensitivity\": \"high\",\n",
        "        \"domain\": \"general\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"intent_extraction\",\n",
        "        \"description\": \"Extract primary intent/action items from dialogue\",\n",
        "        \"evaluation_type\": \"information_extraction\", \n",
        "        \"cultural_sensitivity\": \"high\",\n",
        "        \"domain\": \"business\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to DataFrames for systematic analysis\n",
        "languages_df = pd.DataFrame(target_languages)\n",
        "tasks_df = pd.DataFrame(experimental_tasks)\n",
        "\n",
        "print(\"ðŸŒ SYSTEMATIC EXPERIMENTAL DESIGN\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\\\nðŸ“‹ Target Languages:\")\n",
        "display(languages_df[[\"name\", \"family\", \"resource_level\", \"speakers\", \"writing_system\"]])\n",
        "\n",
        "print(\"\\\\nðŸŽ¯ Experimental Tasks:\")\n",
        "display(tasks_df[[\"task_id\", \"description\", \"evaluation_type\", \"cultural_sensitivity\"]])\n",
        "\n",
        "print(f\"\\\\nðŸ“Š EXPERIMENTAL MATRIX:\")\n",
        "print(f\"   Languages: {len(languages_df)} \")\n",
        "print(f\"   Tasks: {len(tasks_df)}\")\n",
        "print(f\"   Total combinations: {len(languages_df) * len(tasks_df)}\")\n",
        "print(f\"   Systematic evaluation ensures comprehensive coverage!\")\n",
        "\n",
        "# Create systematic evaluation tracking framework\n",
        "evaluation_columns = [\n",
        "    # Experiment identifiers\n",
        "    \"experiment_id\", \"timestamp\", \"language_code\", \"language_name\", \"task_id\",\n",
        "    \n",
        "    # Model and prompt configuration\n",
        "    \"model_name\", \"model_type\", \"access_pattern\", \"prompt_strategy\", \"shots_used\",\n",
        "    \n",
        "    # Input/output data\n",
        "    \"input_text\", \"expected_output\", \"actual_output\", \"prompt_text\",\n",
        "    \n",
        "    # Quantitative metrics\n",
        "    \"correctness_score\", \"fluency_score\", \"cultural_appropriateness_score\",\n",
        "    \"response_time_ms\", \"token_count_input\", \"token_count_output\",\n",
        "    \n",
        "    # Qualitative assessment\n",
        "    \"quality_issues\", \"cultural_notes\", \"improvement_suggestions\", \n",
        "    \n",
        "    # Systematic metadata\n",
        "    \"experiment_conditions\", \"model_parameters\", \"success_flag\"\n",
        "]\n",
        "\n",
        "# Initialize systematic experiment tracking\n",
        "experiments_df = pd.DataFrame(columns=evaluation_columns)\n",
        "\n",
        "print(f\"\\\\nðŸ”¬ EVALUATION FRAMEWORK INITIALIZED:\")\n",
        "print(f\"   Tracking {len(evaluation_columns)} systematic metrics per experiment\")\n",
        "print(f\"   Ready for systematic data collection and analysis\")\n",
        "print(f\"   âœ… Research-grade methodology established!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03fbca1d",
      "metadata": {},
      "source": [
        "### 1.2 Systematic Test Data Framework\n",
        "\n",
        "**Structured Test Cases:** Ensure consistent, comparable evaluation across languages and tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c115c684",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Systematic Test Data Creation\n",
        "# Structured test cases for systematic comparison across languages and tasks\n",
        "\n",
        "def create_systematic_test_data():\n",
        "    \"\"\"\n",
        "    Create structured test data for systematic evaluation across languages and tasks.\n",
        "    This ensures consistent comparison and eliminates ad-hoc testing bias.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Systematic dialogue test cases (parallel across languages)\n",
        "    test_cases = []\n",
        "    \n",
        "    # Test Case 1: Business Meeting Scenario\n",
        "    test_cases.append({\n",
        "        \"case_id\": \"business_meeting_01\",\n",
        "        \"domain\": \"business\", \n",
        "        \"complexity\": \"medium\",\n",
        "        \"cultural_context\": \"professional\",\n",
        "        \"languages\": {\n",
        "            \"en\": {\n",
        "                \"dialogue\": \"A: We need to finalize the budget by Friday. B: I'll have the numbers ready by Thursday. A: Perfect, let's schedule a review meeting.\",\n",
        "                \"expected_classification\": \"meeting\",\n",
        "                \"expected_summary\": \"Team discusses budget deadline and schedules review meeting for Thursday numbers.\",\n",
        "                \"expected_intent\": \"Schedule budget review meeting\"\n",
        "            },\n",
        "            \"fr\": {\n",
        "                \"dialogue\": \"A: Nous devons finaliser le budget vendredi. B: J'aurai les chiffres prÃªts jeudi. A: Parfait, planifions une rÃ©union de rÃ©vision.\", \n",
        "                \"expected_classification\": \"meeting\",\n",
        "                \"expected_summary\": \"L'Ã©quipe discute de l'Ã©chÃ©ance budgÃ©taire et planifie une rÃ©union de rÃ©vision jeudi.\",\n",
        "                \"expected_intent\": \"Planifier une rÃ©union de rÃ©vision budgÃ©taire\"\n",
        "            },\n",
        "            \"ar\": {\n",
        "                \"dialogue\": \"Ø£: Ù†Ø­ØªØ§Ø¬ Ù„Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ© ÙŠÙˆÙ… Ø§Ù„Ø¬Ù…Ø¹Ø©. Ø¨: Ø³Ø£Ø¬Ù‡Ø² Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙŠÙˆÙ… Ø§Ù„Ø®Ù…ÙŠØ³. Ø£: Ù…Ù…ØªØ§Ø²ØŒ Ù„Ù†Ø­Ø¯Ø¯ Ù…ÙˆØ¹Ø¯ Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ø±Ø§Ø¬Ø¹Ø©.\",\n",
        "                \"expected_classification\": \"meeting\", \n",
        "                \"expected_summary\": \"ÙŠÙ†Ø§Ù‚Ø´ Ø§Ù„ÙØ±ÙŠÙ‚ Ù…ÙˆØ¹Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆÙŠØ­Ø¯Ø¯ Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ø±Ø§Ø¬Ø¹Ø© Ù„Ù„Ø£Ø±Ù‚Ø§Ù… ÙŠÙˆÙ… Ø§Ù„Ø®Ù…ÙŠØ³.\",\n",
        "                \"expected_intent\": \"Ø¬Ø¯ÙˆÙ„Ø© Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©\"\n",
        "            }\n",
        "            # ðŸŽ¯ ADD YOUR LANGUAGE HERE following the same structure\n",
        "        }\n",
        "    })\n",
        "    \n",
        "    # Test Case 2: Technical Support Scenario  \n",
        "    test_cases.append({\n",
        "        \"case_id\": \"tech_support_01\",\n",
        "        \"domain\": \"support\",\n",
        "        \"complexity\": \"low\", \n",
        "        \"cultural_context\": \"service\",\n",
        "        \"languages\": {\n",
        "            \"en\": {\n",
        "                \"dialogue\": \"A: My computer won't start this morning. B: Did you try unplugging it for 30 seconds? A: Yes, but still nothing. B: Let me schedule a technician visit.\",\n",
        "                \"expected_classification\": \"support\",\n",
        "                \"expected_summary\": \"Customer reports computer startup issue, basic troubleshooting attempted, technician visit scheduled.\",\n",
        "                \"expected_intent\": \"Schedule technician visit for computer repair\"\n",
        "            },\n",
        "            \"fr\": {\n",
        "                \"dialogue\": \"A: Mon ordinateur ne dÃ©marre pas ce matin. B: Avez-vous essayÃ© de le dÃ©brancher 30 secondes? A: Oui, mais toujours rien. B: Laissez-moi programmer une visite de technicien.\",\n",
        "                \"expected_classification\": \"support\", \n",
        "                \"expected_summary\": \"Le client signale un problÃ¨me de dÃ©marrage d'ordinateur, dÃ©pannage de base tentÃ©, visite de technicien programmÃ©e.\",\n",
        "                \"expected_intent\": \"Programmer une visite de technicien pour rÃ©paration d'ordinateur\"\n",
        "            },\n",
        "            \"ar\": {\n",
        "                \"dialogue\": \"Ø£: Ø¬Ù‡Ø§Ø² Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ù„Ø§ ÙŠØ¹Ù…Ù„ Ù‡Ø°Ø§ Ø§Ù„ØµØ¨Ø§Ø­. Ø¨: Ù‡Ù„ Ø¬Ø±Ø¨Øª ÙØµÙ„Ù‡ Ù„Ù…Ø¯Ø© 30 Ø«Ø§Ù†ÙŠØ©ØŸ Ø£: Ù†Ø¹Ù…ØŒ Ù„ÙƒÙ† Ù„Ø§ Ø´ÙŠØ¡. Ø¨: Ø¯Ø¹Ù†ÙŠ Ø£Ø­Ø¯Ø¯ Ø²ÙŠØ§Ø±Ø© ÙÙ†ÙŠ.\",\n",
        "                \"expected_classification\": \"support\",\n",
        "                \"expected_summary\": \"ÙŠØ¨Ù„Øº Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¹Ù† Ù…Ø´ÙƒÙ„Ø© Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ ØªÙ… Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©ØŒ ØªÙ… Ø¬Ø¯ÙˆÙ„Ø© Ø²ÙŠØ§Ø±Ø© ÙÙ†ÙŠ.\", \n",
        "                \"expected_intent\": \"Ø¬Ø¯ÙˆÙ„Ø© Ø²ÙŠØ§Ø±Ø© ÙÙ†ÙŠ Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±\"\n",
        "            }\n",
        "        }\n",
        "    })\n",
        "    \n",
        "    # Test Case 3: Social Conversation Scenario\n",
        "    test_cases.append({\n",
        "        \"case_id\": \"social_conversation_01\", \n",
        "        \"domain\": \"social\",\n",
        "        \"complexity\": \"low\",\n",
        "        \"cultural_context\": \"informal\",\n",
        "        \"languages\": {\n",
        "            \"en\": {\n",
        "                \"dialogue\": \"A: How was your weekend hiking trip? B: Amazing! The weather was perfect and the views were incredible. A: I'd love to join you next time!\",\n",
        "                \"expected_classification\": \"social\",\n",
        "                \"expected_summary\": \"Friends discuss successful weekend hiking trip with great weather and views, plan future trip together.\",\n",
        "                \"expected_intent\": \"Plan future hiking trip together\"\n",
        "            },\n",
        "            \"fr\": {\n",
        "                \"dialogue\": \"A: Comment s'est passÃ©e ta randonnÃ©e du week-end? B: Fantastique! Le temps Ã©tait parfait et les vues incroyables. A: J'aimerais vous accompagner la prochaine fois!\",\n",
        "                \"expected_classification\": \"social\",\n",
        "                \"expected_summary\": \"Les amis discutent d'une randonnÃ©e rÃ©ussie du week-end avec beau temps et vues, planifient un voyage futur ensemble.\",\n",
        "                \"expected_intent\": \"Planifier une future randonnÃ©e ensemble\"\n",
        "            },\n",
        "            \"ar\": {\n",
        "                \"dialogue\": \"Ø£: ÙƒÙŠÙ ÙƒØ§Ù†Øª Ø±Ø­Ù„Ø© Ø§Ù„Ù…Ø´ÙŠ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ØŸ Ø¨: Ø±Ø§Ø¦Ø¹Ø©! ÙƒØ§Ù† Ø§Ù„Ø·Ù‚Ø³ Ù…Ø«Ø§Ù„ÙŠØ§Ù‹ ÙˆØ§Ù„Ù…Ù†Ø§Ø¸Ø± Ù„Ø§ ØªØµØ¯Ù‚. Ø£: Ø£ÙˆØ¯ Ø§Ù„Ø§Ù†Ø¶Ù…Ø§Ù… Ø¥Ù„ÙŠÙƒÙ… Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©!\",\n",
        "                \"expected_classification\": \"social\",\n",
        "                \"expected_summary\": \"ÙŠÙ†Ø§Ù‚Ø´ Ø§Ù„Ø£ØµØ¯Ù‚Ø§Ø¡ Ø±Ø­Ù„Ø© Ø§Ù„Ù…Ø´ÙŠ Ø§Ù„Ù†Ø§Ø¬Ø­Ø© ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ Ù…Ø¹ Ø·Ù‚Ø³ Ø±Ø§Ø¦Ø¹ ÙˆÙ…Ù†Ø§Ø¸Ø±ØŒ ÙŠØ®Ø·Ø·ÙˆÙ† Ù„Ø±Ø­Ù„Ø© Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ù…Ø¹Ø§Ù‹.\",\n",
        "                \"expected_intent\": \"Ø§Ù„ØªØ®Ø·ÙŠØ· Ù„Ø±Ø­Ù„Ø© Ù…Ø´ÙŠ Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ù…Ø¹Ø§Ù‹\"\n",
        "            }\n",
        "        }\n",
        "    })\n",
        "    \n",
        "    return test_cases\n",
        "\n",
        "# Create systematic test data\n",
        "systematic_test_data = create_systematic_test_data()\n",
        "\n",
        "# Convert to structured format for systematic analysis\n",
        "test_items = []\n",
        "for case in systematic_test_data:\n",
        "    for lang_code, lang_data in case[\"languages\"].items():\n",
        "        # Create test items for each task type\n",
        "        for task in tasks_df[\"task_id\"].values:\n",
        "            expected_output = \"\"\n",
        "            if task == \"dialogue_classification\":\n",
        "                expected_output = lang_data[\"expected_classification\"]\n",
        "            elif task == \"dialogue_summarization\": \n",
        "                expected_output = lang_data[\"expected_summary\"]\n",
        "            elif task == \"intent_extraction\":\n",
        "                expected_output = lang_data[\"expected_intent\"]\n",
        "            \n",
        "            test_items.append({\n",
        "                \"item_id\": f\"{case['case_id']}_{lang_code}_{task}\",\n",
        "                \"case_id\": case[\"case_id\"],\n",
        "                \"language_code\": lang_code,\n",
        "                \"task_id\": task,\n",
        "                \"domain\": case[\"domain\"],\n",
        "                \"complexity\": case[\"complexity\"], \n",
        "                \"cultural_context\": case[\"cultural_context\"],\n",
        "                \"input_text\": lang_data[\"dialogue\"],\n",
        "                \"expected_output\": expected_output,\n",
        "                \"language_name\": languages_df[languages_df[\"code\"] == lang_code][\"name\"].iloc[0]\n",
        "            })\n",
        "\n",
        "# Convert to DataFrame for systematic analysis\n",
        "test_items_df = pd.DataFrame(test_items)\n",
        "\n",
        "print(\"ðŸ“Š SYSTEMATIC TEST DATA FRAMEWORK\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\\\nðŸŽ¯ Test Coverage:\")\n",
        "print(f\"   Test cases: {len(systematic_test_data)}\")\n",
        "print(f\"   Languages per case: {len(systematic_test_data[0]['languages'])}\")\n",
        "print(f\"   Tasks per language: {len(tasks_df)}\")\n",
        "print(f\"   Total test items: {len(test_items_df)}\")\n",
        "\n",
        "print(f\"\\\\nðŸ“‹ Test Distribution:\")\n",
        "test_summary = test_items_df.groupby([\"language_name\", \"task_id\"]).size().unstack(fill_value=0)\n",
        "display(test_summary)\n",
        "\n",
        "print(f\"\\\\nâœ… Systematic test framework ready!\")\n",
        "print(f\"   All languages have parallel test cases for fair comparison\")\n",
        "print(f\"   Multiple domains and complexity levels covered\")\n",
        "print(f\"   Cultural contexts systematically varied\")\n",
        "\n",
        "# Show sample test items\n",
        "print(f\"\\\\nðŸ” Sample Test Items (first 3):\")\n",
        "display(test_items_df[[\"item_id\", \"language_name\", \"task_id\", \"domain\", \"complexity\"]].head(6))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4800a6d2",
      "metadata": {},
      "source": [
        "## 2. Systematic Model Loading and Prompt Engineering Framework\n",
        "\n",
        "**Strategic Mission:** Systematically evaluate multiple model access patterns and prompt strategies across languages.\n",
        "\n",
        "### 2.1 Advanced Model Loading with Performance Tracking\n",
        "\n",
        "**Systematic Model Evaluation:** Compare local, API, and hosted approaches with quantitative metrics.\n",
        "\n",
        "| **Access Pattern** | **Pros** | **Cons** | **Cost Analysis** | **Performance Expectation** |\n",
        "|-------------------|----------|----------|-------------------|----------------------------|\n",
        "| **ðŸ”¥ Local mT5-Base** | Privacy, offline, fine-tunable | 2-4GB RAM, setup time | Hardware only (~$500-2000) | Good multilingual, customizable |\n",
        "| **â˜ï¸ GPT-4 API** | State-of-art, minimal setup | $0.01-0.03/1K tokens | ~$50-200/month typical use | Excellent English, good multilingual |\n",
        "| **ðŸŒ Hosted Free (Colab)** | Zero cost, easy access | 12hr limits, GPU competition | Free (with limits) | Variable quality, good for learning |\n",
        "| **ðŸ¢ Claude API** | Strong reasoning, safety | Limited multilingual support | Similar to GPT-4 | Excellent reasoning, English-focused |\n",
        "\n",
        "### 2.2 Systematic Task Framework\n",
        "\n",
        "**Our systematic evaluation covers:**\n",
        "1. **Dialogue Classification:** Business/social/support categorization with cultural context\n",
        "2. **Dialogue Summarization:** Concise, culturally-appropriate summaries\n",
        "3. **Intent Extraction:** Action items and next steps identification\n",
        "4. **Chain-of-Thought Reasoning:** Step-by-step multilingual reasoning evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0b01478",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ðŸ—ï¸ SECTION 1: Create the Model Manager Class Definition\n",
        "# ================================================================\n",
        "\n",
        "class SystematicModelManager:\n",
        "    \"\"\"\n",
        "    Advanced model management with systematic evaluation and performance tracking\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # --------------------------------------------------------\n",
        "        # 1.1 Initialize Storage Containers\n",
        "        # --------------------------------------------------------\n",
        "        self.models = {}                    # Store different AI models\n",
        "        self.performance_metrics = []       # Track performance data\n",
        "        self.current_model = None          # Keep track of active model\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "        # 1.2 Detect Available Hardware\n",
        "        # --------------------------------------------------------\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ================================================================\n",
        "# ðŸ“Š SECTION 2: Display Creation Results\n",
        "# ================================================================\n",
        "print(\"âœ… SystematicModelManager class created!\")\n",
        "print(f\"ðŸ–¥ï¸  Device detected: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
        "print(\"ðŸ“ Note: We'll add methods to this class in the next cells\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d654c3",
      "metadata": {},
      "source": [
        "### ðŸ“š What You Just Did:\n",
        "**You created the foundation of your model manager!** This class will help you:\n",
        "- Keep track of different AI models (local and online)\n",
        "- Measure how fast they work\n",
        "- Switch between models easily\n",
        "\n",
        "**ðŸŽ¯ What to Expect:** You should see no output (this just defines the class)  \n",
        "**âœ… Success Indicator:** No error messages  \n",
        "**âš ï¸ If you see errors:** Check that you ran the setup cell above first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a58ecb11",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ðŸš€ SECTION 1: Define Model Loading Function\n",
        "# ================================================================\n",
        "\n",
        "def load_local_model(self, model_name: str = \"google/mt5-small\"):\n",
        "    \"\"\"Load local multilingual model with systematic tracking\"\"\"\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 1.1 Display Loading Information\n",
        "    # --------------------------------------------------------\n",
        "    print(f\"ðŸ”„ Loading local model: {model_name}\")\n",
        "    print(f\"â±ï¸  This may take 2-3 minutes on first run...\")\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 1.2 Initialize Performance Tracking\n",
        "    # --------------------------------------------------------\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # --------------------------------------------------------\n",
        "        # 1.3 Download and Load Model Components\n",
        "        # --------------------------------------------------------\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)      # Text processor\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)  # AI model\n",
        "        model = model.to(self.device)                              # Move to GPU/CPU\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "        # 1.4 Calculate Model Statistics\n",
        "        # --------------------------------------------------------\n",
        "        load_time = time.time() - start_time\n",
        "        param_count = sum(p.numel() for p in model.parameters()) / 1e6  # Million parameters\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "        # 1.5 Store Model Configuration\n",
        "        # --------------------------------------------------------\n",
        "        model_config = {\n",
        "            \"name\": model_name,\n",
        "            \"type\": \"local_multilingual\", \n",
        "            \"tokenizer\": tokenizer,\n",
        "            \"model\": model,\n",
        "            \"device\": str(self.device),\n",
        "            \"is_encoder_decoder\": bool(getattr(model.config, \"is_encoder_decoder\", False)),\n",
        "            \"parameters_M\": param_count,\n",
        "            \"load_time_s\": load_time,\n",
        "            \"memory_gb\": torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
        "        }\n",
        "        \n",
        "        self.models[model_name] = model_config      # Save to storage\n",
        "        self.current_model = model_name             # Set as active model\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "        # 1.6 Display Success Results\n",
        "        # --------------------------------------------------------\n",
        "        print(f\"âœ… Model loaded successfully:\")\n",
        "        print(f\"   Device: {self.device}\")\n",
        "        print(f\"   Parameters: {param_count:.0f}M\")\n",
        "        print(f\"   Load time: {load_time:.1f}s\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"   GPU memory: {model_config['memory_gb']:.1f}GB\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        # --------------------------------------------------------\n",
        "        # 1.7 Handle Loading Errors\n",
        "        # --------------------------------------------------------\n",
        "        print(f\"âŒ Error loading {model_name}: {str(e)}\")\n",
        "        print(\"ðŸ’¡ Solutions:\")\n",
        "        print(\"   1. Try smaller model: google/mt5-small\")\n",
        "        print(\"   2. Restart runtime to free memory\")\n",
        "        print(\"   3. Use CPU-only mode\")\n",
        "        return False\n",
        "\n",
        "# ================================================================\n",
        "# ðŸ“ SECTION 2: Add Method to Class\n",
        "# ================================================================\n",
        "SystematicModelManager.load_local_model = load_local_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89612973",
      "metadata": {},
      "source": [
        "### What You Just Did:\n",
        "**You added a function to load AI models!** This is like installing an app on your phone.\n",
        "\n",
        "**What to Expect:** \n",
        "- First time: Nothing visible (just adds the function)\n",
        "- When you actually use it later: Downloads ~500MB model file\n",
        "\n",
        "**What the Numbers Mean:**\n",
        "- **Parameters**: How \"smart\" the model is (more = smarter but slower)\n",
        "- **Load time**: How long it took to start up\n",
        "- **GPU memory**: How much of your graphics card it's using\n",
        "\n",
        "**Common Issues:**\n",
        "- \"Out of memory\": The model is too big for your system\n",
        "- \"Connection error\": Internet issue during download\n",
        "- Takes forever: Normal for first download, fast after that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1522114e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ðŸŒ SECTION 1: Define REAL API Setup Functions\n",
        "# ================================================================\n",
        "\n",
        "def setup_huggingface_api(self, hf_token: str):\n",
        "    \"\"\"Setup Hugging Face Inference API (REAL - works with your token!)\"\"\"\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 1.1 Display Setup Information\n",
        "    # --------------------------------------------------------\n",
        "    print(f\"ðŸ¤— Setting up Hugging Face API with your token...\")\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 1.2 Create REAL API Configuration\n",
        "    # --------------------------------------------------------\n",
        "    import requests  # Import here to avoid dependency issues\n",
        "    \n",
        "    api_config = {\n",
        "        \"name\": \"huggingface_api\",\n",
        "        \"type\": \"api_access\",              # Keep types consistent for routing\n",
        "        \"provider\": \"huggingface\",           # Identify API provider\n",
        "        \"token\": hf_token,\n",
        "        \"api_url\": \"https://api-inference.huggingface.co/models/\",\n",
        "        \"models\": [\"microsoft/DialoGPT-medium\", \"google/flan-t5-base\"],\n",
        "        \"default_model\": \"google/flan-t5-base\",\n",
        "        \"cost\": \"Free (1000 requests/month)\",\n",
        "        \"requests_module\": requests  # Store for later use\n",
        "    }\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 1.3 Test API Connection\n",
        "    # --------------------------------------------------------\n",
        "    try:\n",
        "        # Test with a simple model\n",
        "        headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
        "        test_url = api_config[\"api_url\"] + \"microsoft/DialoGPT-medium\"\n",
        "        response = requests.post(test_url, headers=headers, \n",
        "                               json={\"inputs\": \"Hello\"}, timeout=10)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            print(\"âœ… Hugging Face API connection successful!\")\n",
        "            api_config[\"status\"] = \"active\"\n",
        "        else:\n",
        "            print(f\"âš ï¸  API responded with status {response.status_code}\")\n",
        "            api_config[\"status\"] = \"limited\"\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  API test failed: {str(e)[:50]}... (but API config saved)\")\n",
        "        api_config[\"status\"] = \"untested\"\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 1.4 Store Configuration\n",
        "    # --------------------------------------------------------\n",
        "    self.models[\"huggingface_api\"] = api_config\n",
        "    self.current_model = \"huggingface_api\"  # Set as active\n",
        "    \n",
        "    print(f\"   Model: Hugging Face Free API\")\n",
        "    print(f\"   Status: {api_config['status']}\")\n",
        "    print(f\"   Cost: {api_config['cost']}\")\n",
        "    return True\n",
        "\n",
        "def setup_placeholder_api(self, api_type: str = \"demo\"):\n",
        "    \"\"\"Setup placeholder API (for demo purposes only - doesn't actually work)\"\"\"\n",
        "    print(f\"ðŸ“ Setting up {api_type} placeholder (DEMO ONLY)\")\n",
        "    \n",
        "    api_config = {\n",
        "        \"name\": f\"{api_type}_placeholder\",\n",
        "        \"type\": \"placeholder\",\n",
        "        \"status\": \"demo_only\",\n",
        "        \"note\": \"This is just a demo - use setup_huggingface_api() for real API\"\n",
        "    }\n",
        "    \n",
        "    self.models[f\"{api_type}_placeholder\"] = api_config\n",
        "    print(\"âš ï¸  This is just a placeholder - use real APIs for actual work!\")\n",
        "    return True\n",
        "\n",
        "# ================================================================\n",
        "# ðŸ“ SECTION 2: Add Methods to Class\n",
        "# ================================================================\n",
        "SystematicModelManager.setup_huggingface_api = setup_huggingface_api\n",
        "SystematicModelManager.setup_placeholder_api = setup_placeholder_api"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ac64d74",
      "metadata": {},
      "source": [
        "### Simple API Option (Optional)\n",
        "If you want online models, use your HF token once and youâ€™re done:\n",
        "\n",
        "```python\n",
        "# Get your FREE token from https://huggingface.co/settings/tokens\n",
        "your_token = \"hf_xxxxxxxxxxxxxxxxxxxxx\"\n",
        "model_manager.setup_huggingface_api(your_token)\n",
        "```\n",
        "\n",
        "- If you skip this, the local model is enough for the hands-on.\n",
        "- Cost: Free (1000 requests/month)\n",
        "- Never share your token publicly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8802bf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ðŸŽ¯ SECTION 1: Define Main Text Generation Function  \n",
        "# ================================================================\n",
        "\n",
        "def generate_text_systematic(self, prompt: str, max_length: int = 100, \n",
        "                            temperature: float = 0.7, model_name: str = None) -> Dict:\n",
        "    \"\"\"Generate text with systematic performance tracking\"\"\"\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 1.1 Validate Model Selection\n",
        "    # --------------------------------------------------------\n",
        "    model_name = model_name or self.current_model       # Use current model if none specified\n",
        "    if not model_name or model_name not in self.models:\n",
        "        return {\"error\": \"No model loaded\", \"output\": \"\", \"metrics\": {}}\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 1.2 Get Model Configuration\n",
        "    # --------------------------------------------------------\n",
        "    model_config = self.models[model_name]\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 1.3 Route to Appropriate Generation Method\n",
        "    # --------------------------------------------------------\n",
        "    if model_config[\"type\"] == \"local_multilingual\":\n",
        "        return self._generate_local(prompt, model_config, max_length, temperature)\n",
        "    elif model_config[\"type\"] in [\"api_access\", \"huggingface_api\"]:\n",
        "        return self._generate_api(prompt, model_config, max_length, temperature)\n",
        "    else:\n",
        "        return {\"error\": \"Unknown model type\", \"output\": \"\", \"metrics\": {}}\n",
        "\n",
        "# ================================================================\n",
        "# ðŸ“ SECTION 2: Add Method to Class\n",
        "# ================================================================\n",
        "SystematicModelManager.generate_text_systematic = generate_text_systematic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b021f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ðŸ”§ SECTION 1: Define Local Model Generation Function\n",
        "# ================================================================\n",
        "\n",
        "def _generate_local(self, prompt: str, model_config: Dict, max_length: int, temperature: float) -> Dict:\n",
        "    \"\"\"Generate using local model with performance tracking\"\"\"\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 1.1 Initialize Performance Tracking\n",
        "    # --------------------------------------------------------\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # --------------------------------------------------------\n",
        "        # 1.2 Get Model Components\n",
        "        # --------------------------------------------------------\n",
        "        tokenizer = model_config[\"tokenizer\"]      # Text processor\n",
        "        model = model_config[\"model\"]              # AI model\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "        # 1.3 Process Input Text (Tokenization)\n",
        "        # --------------------------------------------------------\n",
        "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        input_tokens = inputs.shape[1]             # Count input tokens\n",
        "        inputs = inputs.to(self.device)            # Move to GPU/CPU\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "        # 1.4 Generate AI Response\n",
        "        # --------------------------------------------------------\n",
        "        with torch.no_grad():                      # Save memory during generation\n",
        "            outputs = model.generate(\n",
        "                inputs, \n",
        "                max_length=max_length, \n",
        "                temperature=temperature,\n",
        "                do_sample=True, \n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "        # 1.5 Process Output Text (Detokenization)\n",
        "        # --------------------------------------------------------\n",
        "        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "        \n",
        "        # For seq2seq models (like mT5), outputs are decoder-only tokens\n",
        "        if model_config.get(\"is_encoder_decoder\", False):\n",
        "            output_tokens = outputs.shape[1]\n",
        "        else:\n",
        "            output_tokens = max(outputs.shape[1] - input_tokens, 0)\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "        # 1.6 Calculate Performance Metrics\n",
        "        # --------------------------------------------------------\n",
        "        generation_time = time.time() - start_time\n",
        "        tokens_per_second = output_tokens / generation_time if generation_time > 0 else 0\n",
        "        \n",
        "        metrics = {\n",
        "            \"model_name\": model_config[\"name\"],\n",
        "            \"generation_time_ms\": generation_time * 1000,\n",
        "            \"input_tokens\": input_tokens,\n",
        "            \"output_tokens\": output_tokens,\n",
        "            \"tokens_per_second\": tokens_per_second,\n",
        "            \"success\": True\n",
        "        }\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "        # 1.7 Return Successful Result\n",
        "        # --------------------------------------------------------\n",
        "        return {\n",
        "            \"output\": output_text,\n",
        "            \"metrics\": metrics,\n",
        "            \"error\": None\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        # --------------------------------------------------------\n",
        "        # 1.8 Handle Generation Errors\n",
        "        # --------------------------------------------------------\n",
        "        return {\n",
        "            \"output\": \"\", \n",
        "            \"metrics\": {\"success\": False, \"error\": str(e)},\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# ================================================================\n",
        "# ðŸŒ SECTION 2: Define API Generation Function (Hugging Face)\n",
        "# ================================================================\n",
        "\n",
        "def _generate_api(self, prompt: str, model_config: Dict, max_length: int, temperature: float) -> Dict:\n",
        "    \"\"\"Generate using API providers (Hugging Face supported)\"\"\"\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 2.1 Initialize Performance Tracking\n",
        "    # --------------------------------------------------------\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 2.2 Route by Provider\n",
        "    # --------------------------------------------------------\n",
        "    provider = model_config.get(\"provider\")\n",
        "    if provider == \"huggingface\" or model_config.get(\"type\") == \"huggingface_api\":\n",
        "        try:\n",
        "            requests = model_config.get(\"requests_module\")\n",
        "            api_url = model_config[\"api_url\"]\n",
        "            model_name = model_config.get(\"default_model\", \"google/flan-t5-base\")\n",
        "            headers = {\"Authorization\": f\"Bearer {model_config['token']}\"}\n",
        "            \n",
        "            payload = {\n",
        "                \"inputs\": prompt,\n",
        "                \"parameters\": {\n",
        "                    \"max_new_tokens\": max_length,\n",
        "                    \"temperature\": temperature\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            response = requests.post(api_url + model_name, headers=headers, json=payload, timeout=30)\n",
        "            if response.status_code != 200:\n",
        "                return {\n",
        "                    \"output\": \"\",\n",
        "                    \"metrics\": {\n",
        "                        \"model_name\": model_name,\n",
        "                        \"provider\": \"huggingface\",\n",
        "                        \"success\": False,\n",
        "                        \"status_code\": response.status_code\n",
        "                    },\n",
        "                    \"error\": f\"Hugging Face API error: {response.status_code}\"\n",
        "                }\n",
        "            \n",
        "            data = response.json()\n",
        "            if isinstance(data, dict) and \"error\" in data:\n",
        "                return {\n",
        "                    \"output\": \"\",\n",
        "                    \"metrics\": {\n",
        "                        \"model_name\": model_name,\n",
        "                        \"provider\": \"huggingface\",\n",
        "                        \"success\": False\n",
        "                    },\n",
        "                    \"error\": data.get(\"error\")\n",
        "                }\n",
        "            \n",
        "            # HF text-generation often returns a list with generated_text\n",
        "            if isinstance(data, list) and len(data) > 0 and \"generated_text\" in data[0]:\n",
        "                output_text = data[0][\"generated_text\"]\n",
        "            elif isinstance(data, dict) and \"generated_text\" in data:\n",
        "                output_text = data[\"generated_text\"]\n",
        "            else:\n",
        "                output_text = str(data)\n",
        "            \n",
        "            generation_time = time.time() - start_time\n",
        "            return {\n",
        "                \"output\": output_text.strip(),\n",
        "                \"metrics\": {\n",
        "                    \"model_name\": model_name,\n",
        "                    \"provider\": \"huggingface\",\n",
        "                    \"generation_time_ms\": generation_time * 1000,\n",
        "                    \"input_tokens\": len(prompt.split()),\n",
        "                    \"success\": True\n",
        "                },\n",
        "                \"error\": None\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"output\": \"\",\n",
        "                \"metrics\": {\"success\": False, \"provider\": \"huggingface\"},\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 2.3 Fallback for Other API Providers\n",
        "    # --------------------------------------------------------\n",
        "    return {\n",
        "        \"output\": \"\",\n",
        "        \"metrics\": {\"success\": False},\n",
        "        \"error\": \"API provider not implemented in this notebook\"\n",
        "    }\n",
        "\n",
        "# ================================================================\n",
        "# ðŸ“ SECTION 3: Add Methods to Class\n",
        "# ================================================================\n",
        "SystematicModelManager._generate_local = _generate_local\n",
        "SystematicModelManager._generate_api = _generate_api"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40ea8218",
      "metadata": {},
      "source": [
        "### What You Just Did:\n",
        "**You added the \"engine\" that makes AI models work!** This is like the brain of your AI system.\n",
        "\n",
        "**What Each Function Does:**\n",
        "- **`generate_text_systematic`**: The main function you'll call to get AI responses\n",
        "- **`_generate_local`**: Handles local models (like mT5 on your computer)  \n",
        "- **`_generate_api`**: Handles online services (like ChatGPT APIs)\n",
        "\n",
        "**What the Performance Metrics Mean:**\n",
        "- **Generation time**: How long the AI took to respond\n",
        "- **Input/Output tokens**: How many \"words\" went in and came out\n",
        "- **Tokens per second**: How fast the AI is working\n",
        "\n",
        "**These are helper functions - you won't call them directly!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a26b0fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ðŸš€ SECTION 1: Initialize Model Manager\n",
        "# ================================================================\n",
        "\n",
        "print(\"ðŸš€ SYSTEMATIC MODEL MANAGER INITIALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 1.1 Create Model Manager Instance\n",
        "# --------------------------------------------------------\n",
        "model_manager = SystematicModelManager()\n",
        "print(\"âœ… Model manager created!\")\n",
        "print(f\"   Device available: {model_manager.device}\")\n",
        "\n",
        "# ================================================================\n",
        "# ðŸ“¥ SECTION 2: Load Model and Test System\n",
        "# ================================================================\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 2.1 Load Local Multilingual Model\n",
        "# --------------------------------------------------------\n",
        "print(\"\\nðŸ”¥ Loading local multilingual model...\")\n",
        "local_success = model_manager.load_local_model(\"google/mt5-small\")\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 2.2 OPTIONAL: Add Hugging Face Token (for API models)\n",
        "# --------------------------------------------------------\n",
        "# If you want to use Hugging Face API, uncomment and paste your token:\n",
        "# hf_token = \"hf_xxxxxxxxxxxxxxxxxxxxx\"\n",
        "# model_manager.setup_huggingface_api(hf_token)\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 2.3 Display Results\n",
        "# --------------------------------------------------------\n",
        "if local_success:\n",
        "    print(f\"\\nðŸ“Š MODEL MANAGER STATUS:\")\n",
        "    print(f\"   Available models: {len(model_manager.models)}\")\n",
        "    print(f\"   Current model: {model_manager.current_model}\")\n",
        "    print(f\"   Device: {model_manager.device}\")\n",
        "    print(f\"   âœ… Ready for experiments!\")\n",
        "else:\n",
        "    print(\"âš ï¸  Local model failed to load - we'll setup APIs next\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c03af92",
      "metadata": {},
      "source": [
        "### ðŸ“š What You Should See (Keep It Simple)\n",
        "- \"Model manager created!\" message\n",
        "- \"Device available: cuda\" (on Colab GPU) or \"cpu\" \n",
        "- Loading progress for the mT5 model (2-3 minutes first time)\n",
        "- \"Model loaded successfully\" with statistics\n",
        "- \"Ready for experiments!\" at the end\n",
        "\n",
        "**Optional:** If you want API models, paste your HF token in the optional block in the cell above.\n",
        "**â±ï¸ Expected Timeline:**\n",
        "- First run: 2-3 minutes (downloads ~500MB model)\n",
        "- Subsequent runs: 10-30 seconds (model already cached)\n",
        "\n",
        "**âŒ If You See Errors:**\n",
        "- \"CUDA out of memory\": Model too big, restart runtime\n",
        "- \"Connection error\": Internet issue, try again\n",
        "- \"Import error\": Run the setup cells above first\n",
        "\n",
        "**ðŸ“Š What the Statistics Mean:**\n",
        "- **Parameters**: 300M = small model, 3B = large model\n",
        "- **GPU Memory**: How much graphics card memory it's using\n",
        "- **Load time**: Normal range is 10-180 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4115737",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ðŸ§ª SECTION 1: Test AI Model Functionality\n",
        "# ================================================================\n",
        "\n",
        "if 'model_manager' in locals() and model_manager.current_model:\n",
        "    print(\"ðŸ§ª TESTING YOUR AI MODEL\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 1.1 Prepare Test Prompt (T5-style works better)\n",
        "    # --------------------------------------------------------\n",
        "    test_prompt = (\n",
        "        \"Classify the intent with ONE label: scheduling, confirmation. \"\n",
        "        \"Dialogue: A: Let's meet at 3pm. B: Perfect, see you then.\"\n",
        "    )\n",
        "    \n",
        "    print(f\"ðŸŽ¯ Test prompt: {test_prompt}\")\n",
        "    print(\"\\nâ±ï¸  Generating response...\")\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 1.2 Generate AI Response\n",
        "    # --------------------------------------------------------\n",
        "    result = model_manager.generate_text_systematic(test_prompt, max_length=50, temperature=0.1)\n",
        "    \n",
        "    # --------------------------------------------------------\n",
        "    # 1.3 Display Results\n",
        "    # --------------------------------------------------------\n",
        "    if result[\"error\"]:\n",
        "        print(f\"âŒ Test failed: {result['error']}\")\n",
        "        print(\"ðŸ’¡ Try restarting the runtime and running all cells again\")\n",
        "    else:\n",
        "        print(f\"âœ… SUCCESS! Your AI responded:\")\n",
        "        print(f\"ðŸ“ Output: {result['output']}\")\n",
        "        if result[\"output\"].strip() in {\"<extra_id_0>\", \"<extra_id_1>\"}:\n",
        "            print(\"ðŸ’¡ Tip: mT5 responds better with explicit labels and short instructions\")\n",
        "        print(f\"\\nðŸ“Š Performance:\")\n",
        "        print(f\"   Generation time: {result['metrics']['generation_time_ms']:.1f}ms\")\n",
        "        print(f\"   Tokens per second: {result['metrics']['tokens_per_second']:.1f}\")\n",
        "        print(f\"\\nðŸŽ‰ Your AI model is working perfectly!\")\n",
        "\n",
        "# ================================================================\n",
        "# âš ï¸ SECTION 2: Handle Missing Model\n",
        "# ================================================================        \n",
        "else:\n",
        "    print(\"âš ï¸  Model not loaded yet. Run the cell above first!\")\n",
        "    print(\"ðŸ”„ If it failed, try restarting runtime and running all cells\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23081187",
      "metadata": {},
      "source": [
        "### ðŸ“š What You Should See:\n",
        "**This cell tests that your AI is actually working!** \n",
        "\n",
        "**âœ… Success Indicators:**\n",
        "- \"SUCCESS! Your AI responded:\" message\n",
        "- Some text output (might be weird/incomplete - that's normal!)\n",
        "- Performance metrics showing generation time and speed\n",
        "- \"Your AI model is working perfectly!\" message\n",
        "\n",
        "**ðŸŽ¯ What the Output Means:**\n",
        "- **The AI response**: Might be random/incomplete (mT5 needs fine-tuning for good results)\n",
        "- **Generation time**: How fast your AI is (faster = better)  \n",
        "- **Tokens per second**: Processing speed (10-100 is normal range)\n",
        "\n",
        "**ðŸš€ What's Next:**\n",
        "- Don't worry if the response seems random\n",
        "- We'll improve it with proper prompting techniques\n",
        "- The important thing is that it's working without errors!\n",
        "\n",
        "**âŒ If It Doesn't Work:**\n",
        "- Check that all previous cells ran without errors\n",
        "- Try restarting runtime (Runtime â†’ Restart Runtime)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63f841ee",
      "metadata": {},
      "source": [
        "### ðŸ”„ Break Time! What's Coming Next\n",
        "\n",
        "**ðŸŽ‰ Great job so far!** You've successfully:\n",
        "- âœ… Set up your model manager\n",
        "- âœ… Loaded a multilingual AI model  \n",
        "- âœ… Tested that it's working\n",
        "\n",
        "**ðŸš€ What's Next:**\n",
        "Now we'll add **FREE online AI services** so you can compare different models and choose the best one for your project.\n",
        "\n",
        "**ðŸ“± Think of it like this:**\n",
        "- Local model (mT5) = App installed on your phone\n",
        "- API services = Apps you use through the internet (like Google Translate)\n",
        "\n",
        "**â±ï¸ This next section takes 2-3 minutes to read**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "291438cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸŽ¯ Part 3: Basic Prompt Engineering Techniques\n",
        "\n",
        "## 3.1 Zero-shot, Few-shot, and Chain-of-Thought Comparison\n",
        "\n",
        "# ðŸ”§ Prompt engineering toolkit\n",
        "def create_zero_shot_prompt(dialogue: str, task: str = \"classification\") -> str:\n",
        "    \"\"\"Zero-shot prompt - no examples provided\"\"\"\n",
        "    if task == \"classification\":\n",
        "        return f\"\"\"Classify this dialogue into one topic: meeting, social, support, transaction, other.\n",
        "\n",
        "Dialogue: {dialogue}\n",
        "\n",
        "Topic:\"\"\"\n",
        "    else:  # QA\n",
        "        return f\"\"\"Answer the question based on the context.\n",
        "\n",
        "Context: {dialogue}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "def create_few_shot_prompt(dialogue: str, examples: list, task: str = \"classification\") -> str:\n",
        "    \"\"\"Few-shot prompt - includes examples\"\"\"\n",
        "    if task == \"classification\":\n",
        "        prompt = \"Classify dialogues into topics: meeting, social, support, transaction, other.\\n\\nExamples:\\n\\n\"\n",
        "        for ex in examples[:2]:  # Use 2 examples to avoid length issues\n",
        "            prompt += f\"Dialogue: {ex['dialogue']}\\nTopic: {ex['topic']}\\n\\n\"\n",
        "        prompt += f\"Dialogue: {dialogue}\\nTopic:\"\n",
        "        return prompt\n",
        "    else:  # QA\n",
        "        return f\"\"\"Answer questions based on context.\n",
        "\n",
        "Context: {dialogue}\n",
        "\n",
        "Answer with specific information:\"\"\"\n",
        "\n",
        "def create_chain_of_thought_prompt(context: str, question: str) -> str:\n",
        "    \"\"\"Chain-of-Thought prompt for step-by-step reasoning\"\"\"\n",
        "    return f\"\"\"Answer the question step by step based on the context.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Let me think step by step:\n",
        "1. What is the question asking?\n",
        "2. What relevant information is in the context?\n",
        "3. What is the answer?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# ðŸ§ª Mini Test Dataset (kept simple for students)\n",
        "# --------------------------------------------------------\n",
        "# Keep just a few examples so the cell runs fast.\n",
        "\n",
        "test_data = {\n",
        "    \"English\": {\n",
        "        \"classification\": [\n",
        "            {\"dialogue\": \"A: Let's meet at 3pm. B: Perfect, see you then.\", \"topic\": \"meeting\"},\n",
        "            {\"dialogue\": \"A: I can't log in. B: Try resetting your password.\", \"topic\": \"support\"}\n",
        "        ],\n",
        "        \"qa\": {\n",
        "            \"context\": \"The meeting is on Friday at 3pm in Room 204.\",\n",
        "            \"questions\": [\"When is the meeting?\"],\n",
        "            \"answers\": [\"Friday at 3pm\"]\n",
        "        }\n",
        "    },\n",
        "    \"Swahili\": {\n",
        "        \"classification\": [\n",
        "            {\"dialogue\": \"A: Tutaonana saa tisa. B: Sawa, nitakuja.\", \"topic\": \"meeting\"}\n",
        "        ],\n",
        "        \"qa\": {\n",
        "            \"context\": \"Mkutano ni Ijumaa saa tisa katika Chumba 204.\",\n",
        "            \"questions\": [\"Mkutano ni lini?\"],\n",
        "            \"answers\": [\"Ijumaa saa tisa\"]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# ðŸ§ª Run comprehensive prompt testing\n",
        "# Simple helper to call the active model\n",
        "\n",
        "def generate_text(prompt, max_length=100, temperature=0.7):\n",
        "    result = model_manager.generate_text_systematic(prompt, max_length=max_length, temperature=temperature)\n",
        "    return result.get(\"output\", \"\")\n",
        "\n",
        "model_ready = \"model_manager\" in locals() and model_manager.current_model is not None\n",
        "\n",
        "def test_all_prompting_strategies():\n",
        "    \"\"\"Test zero-shot, few-shot, and Chain-of-Thought across languages\"\"\"\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    print(\"ðŸŽ¯ COMPREHENSIVE PROMPT ENGINEERING TEST\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for language, data in test_data.items():\n",
        "        print(f\"\\nðŸŒ TESTING: {language.upper()}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Test classification\n",
        "        if data[\"classification\"]:\n",
        "            test_dialogue = data[\"classification\"][0][\"dialogue\"]\n",
        "            true_topic = data[\"classification\"][0][\"topic\"]\n",
        "            \n",
        "            print(f\"ðŸ“ Classification task: {test_dialogue[:60]}...\")\n",
        "            print(f\"ðŸ“‹ Expected: {true_topic}\")\n",
        "            \n",
        "            # Zero-shot classification\n",
        "            zero_prompt = create_zero_shot_prompt(test_dialogue, \"classification\")\n",
        "            if model_ready:\n",
        "                zero_result = generate_text(zero_prompt, max_length=50, temperature=0.1)\n",
        "                print(f\"ðŸŽ¯ Zero-shot: {zero_result}\")\n",
        "                \n",
        "                # Few-shot classification (using English examples for transfer)\n",
        "                few_prompt = create_few_shot_prompt(test_dialogue, test_data[\"English\"][\"classification\"], \"classification\")\n",
        "                few_result = generate_text(few_prompt, max_length=50, temperature=0.1)\n",
        "                print(f\"ðŸ“š Few-shot: {few_result}\")\n",
        "                \n",
        "                results.append({\n",
        "                    \"language\": language, \"task\": \"classification\", \"method\": \"zero-shot\",\n",
        "                    \"input\": test_dialogue[:50] + \"...\", \"output\": zero_result, \"expected\": true_topic\n",
        "                })\n",
        "                results.append({\n",
        "                    \"language\": language, \"task\": \"classification\", \"method\": \"few-shot\", \n",
        "                    \"input\": test_dialogue[:50] + \"...\", \"output\": few_result, \"expected\": true_topic\n",
        "                })\n",
        "            \n",
        "        # Test QA with Chain-of-Thought\n",
        "        if data[\"qa\"][\"questions\"]:\n",
        "            context = data[\"qa\"][\"context\"]\n",
        "            question = data[\"qa\"][\"questions\"][0]\n",
        "            expected_answer = data[\"qa\"][\"answers\"][0]\n",
        "            \n",
        "            print(f\"\\\\nâ“ QA task: {question}\")\n",
        "            print(f\"ðŸ“‹ Expected: {expected_answer}\")\n",
        "            \n",
        "            # Chain-of-Thought QA\n",
        "            cot_prompt = create_chain_of_thought_prompt(context, question)\n",
        "            if model_ready:\n",
        "                cot_result = generate_text(cot_prompt, max_length=120, temperature=0.2)\n",
        "                print(f\"ðŸ§  Chain-of-Thought: {cot_result}\")\n",
        "                \n",
        "                results.append({\n",
        "                    \"language\": language, \"task\": \"qa\", \"method\": \"chain-of-thought\",\n",
        "                    \"input\": question, \"output\": cot_result, \"expected\": expected_answer\n",
        "                })\n",
        "        \n",
        "        print()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run the comprehensive test\n",
        "if model_ready:\n",
        "    test_results = test_all_prompting_strategies()\n",
        "    print(f\"âœ… Completed testing across {len(test_data)} languages\")\n",
        "else:\n",
        "    print(\"âš ï¸  Model not available - showing prompt structure only\")\n",
        "    # Show example prompts\n",
        "    example_dialogue = \"A: Can we meet at 3pm? B: Perfect!\"\n",
        "    print(\"\\\\nðŸ“ EXAMPLE PROMPTS:\")\n",
        "    print(\"\\\\nðŸŽ¯ Zero-shot:\")\n",
        "    print(create_zero_shot_prompt(example_dialogue))\n",
        "    print(\"\\\\nðŸ“š Few-shot structure:\")\n",
        "    print(create_few_shot_prompt(example_dialogue, [{\"dialogue\": \"Example\", \"topic\": \"meeting\"}])[:200] + \"...\")\n",
        "\n",
        "### 2.2 ðŸ“Š Evaluation Framework\n",
        "\n",
        "def create_evaluation_rubric():\n",
        "    \"\"\"Evaluation framework for model outputs\"\"\"\n",
        "    return {\n",
        "        \"correctness\": {\n",
        "            \"1\": \"Completely wrong\", \"2\": \"Partially wrong\", \"3\": \"Mostly right\", \n",
        "            \"4\": \"Right answer\", \"5\": \"Perfect with reasoning\"\n",
        "        },\n",
        "        \"fluency\": {\n",
        "            \"1\": \"Unnatural/errors\", \"2\": \"Awkward phrasing\", \"3\": \"Acceptable\", \n",
        "            \"4\": \"Good language\", \"5\": \"Native-like\"\n",
        "        },\n",
        "        \"cultural_appropriateness\": {\n",
        "            \"1\": \"Inappropriate\", \"2\": \"Questionable\", \"3\": \"Neutral\", \n",
        "            \"4\": \"Appropriate\", \"5\": \"Culturally aware\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "def evaluate_output(output: str, expected: str, language: str, task: str, method: str):\n",
        "    \"\"\"Template for manual evaluation\"\"\"\n",
        "    return {\n",
        "        \"output\": output,\n",
        "        \"expected\": expected,\n",
        "        \"language\": language,\n",
        "        \"task\": task,\n",
        "        \"method\": method,\n",
        "        \"correctness_score\": 0,  # Fill in 1-5\n",
        "        \"fluency_score\": 0,      # Fill in 1-5\n",
        "        \"cultural_score\": 0,     # Fill in 1-5\n",
        "        \"notes\": \"\",            # Your observations\n",
        "        \"improvement_suggestions\": \"\"\n",
        "    }\n",
        "\n",
        "print(\"\\\\nðŸ“‹ EVALUATION FRAMEWORK\")\n",
        "print(\"=\"*40)\n",
        "rubric = create_evaluation_rubric()\n",
        "for dimension, scale in rubric.items():\n",
        "    print(f\"\\\\n{dimension.upper()}:\")\n",
        "    for score, description in scale.items():\n",
        "        print(f\"  {score}: {description}\")\n",
        "\n",
        "print(f\"\\\\nðŸŽ¯ YOUR TURN: Evaluate the outputs above using this 1-5 scale\")\n",
        "print(\"ðŸ’¡ Focus on how well each method works for your target language\")\n",
        "\n",
        "### 2.3 ðŸ’¬ Discussion Questions and Key Takeaways\n",
        "\n",
        "discussion_guide = \"\"\"\n",
        "ðŸ¤” REFLECTION QUESTIONS:\n",
        "\n",
        "1. **Cross-language Performance:**\n",
        "   - Which prompting method worked best for your target language?\n",
        "   - How did performance differ between English and your language?\n",
        "\n",
        "2. **Method Comparison:** \n",
        "   - When did few-shot examples help vs. hurt?\n",
        "   - How effective was Chain-of-Thought reasoning in non-English?\n",
        "\n",
        "3. **Cultural Considerations:**\n",
        "   - What cultural assumptions did you notice in outputs?\n",
        "   - How would you adapt prompts for your cultural context?\n",
        "\n",
        "4. **Practical Applications:**\n",
        "   - Which approach would you use in production?\n",
        "   - What are the trade-offs between methods?\n",
        "\n",
        "ðŸ“ ACTION ITEMS:\n",
        "â–¡ Document 3 key insights about your target language\n",
        "â–¡ Identify best prompting strategies for your use case  \n",
        "â–¡ Note major challenges needing further research\n",
        "â–¡ Plan next steps for your project\n",
        "\n",
        "ðŸŽ¯ KEY TAKEAWAYS:\n",
        "â€¢ Prompt structure matters more than complexity\n",
        "â€¢ Cultural context significantly impacts performance  \n",
        "â€¢ Few-shot examples can bridge language gaps effectively\n",
        "â€¢ Chain-of-Thought helps with reasoning across languages\n",
        "â€¢ Evaluation must consider cultural appropriateness\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\\\n\" + discussion_guide)\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ CONGRATULATIONS!\")\n",
        "print(\"You've completed hands-on prompt engineering for low-resource languages!\")\n",
        "print(\"Use these techniques responsibly and keep experimenting! ðŸš€\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "903ae0af",
      "metadata": {},
      "source": [
        "## 4.1 ðŸ†“ FREE API Setup for Colab Students\n",
        "\n",
        "**Student-Friendly Free Options:**\n",
        "\n",
        "| **Service** | **Free Tier** | **Models Available** | **Setup Difficulty** | **Recommended For** |\n",
        "|-------------|---------------|---------------------|---------------------|-------------------|\n",
        "| **ðŸ¤— Hugging Face** | 1000 requests/month | Llama-2, CodeLlama, Mistral | Easy | Learning & experiments |\n",
        "| **ðŸŸ¢ Google Gemini** | 60 requests/minute | Gemini-1.5-flash, Gemini-pro | Easy | High-quality outputs |\n",
        "| **ðŸ”¥ Local mT5** | GPU memory only | mT5-small/base | Medium | Privacy & customization |\n",
        "\n",
        "**âœ… All options work perfectly on Google Colab GPU!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efc6dd6f",
      "metadata": {},
      "source": [
        "# ðŸ†“ Part 4: Free API Integration for Students\n",
        "\n",
        "**ðŸŽ¯ Goal:** Set up free online AI services to complement your local model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c78447dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ðŸ†“ SECTION 1: Import Libraries for Free APIs\n",
        "# ================================================================\n",
        "\n",
        "import requests     # For HTTP requests to APIs\n",
        "import os          # For environment variables  \n",
        "from typing import Dict, List  # For better code organization\n",
        "\n",
        "# ================================================================\n",
        "# ðŸ—ï¸ SECTION 2: Create Free API Manager Class\n",
        "# ================================================================\n",
        "\n",
        "class FreeAPIManager:\n",
        "    \"\"\"Manage multiple free API services for students\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # --------------------------------------------------------\n",
        "        # 2.1 Initialize Service Storage\n",
        "        # --------------------------------------------------------\n",
        "        self.available_services = {}  # Store configured APIs\n",
        "        \n",
        "    def setup_huggingface_free(self, hf_token: str = None):\n",
        "        \"\"\"Setup Hugging Face Inference API (1000 free requests/month)\"\"\"\n",
        "        if not hf_token:\n",
        "            print(\"ðŸ”‘ Get your FREE Hugging Face token:\")\n",
        "            print(\"   1. Visit: https://huggingface.co/settings/tokens\")\n",
        "            print(\"   2. Create a token with 'Read' permission\")\n",
        "            print(\"   3. Paste it when prompted\")\n",
        "            hf_token = input(\"Enter your HF token: \").strip()\n",
        "        \n",
        "        self.available_services[\"huggingface\"] = {\n",
        "            \"token\": hf_token,\n",
        "            \"api_url\": \"https://api-inference.huggingface.co/models/\",\n",
        "            \"models\": [\"microsoft/DialoGPT-medium\", \"google/flan-t5-base\", \"meta-llama/Llama-2-7b-chat-hf\"],\n",
        "            \"cost\": \"Free (1000 requests/month)\",\n",
        "            \"setup\": True\n",
        "        }\n",
        "        print(\"âœ… Hugging Face API configured!\")\n",
        "        return True\n",
        "    \n",
        "    def setup_gemini_free(self, api_key: str = None):\n",
        "        \"\"\"Setup Google Gemini API (60 free requests/minute)\"\"\"\n",
        "        if not api_key:\n",
        "            print(\"ðŸ”‘ Get your FREE Google AI Studio API key:\")\n",
        "            print(\"   1. Visit: https://makersuite.google.com/app/apikey\")\n",
        "            print(\"   2. Create a new API key\")\n",
        "            print(\"   3. Paste it when prompted\")\n",
        "            api_key = input(\"Enter your Gemini API key: \").strip()\n",
        "        \n",
        "        self.available_services[\"gemini\"] = {\n",
        "            \"api_key\": api_key,\n",
        "            \"models\": [\"gemini-1.5-flash\", \"gemini-1.5-pro\"],\n",
        "            \"cost\": \"Free (60 requests/minute)\",\n",
        "            \"setup\": True\n",
        "        }\n",
        "        print(\"âœ… Google Gemini API configured!\")\n",
        "        return True\n",
        "    \n",
        "    def query_huggingface(self, model_name: str, prompt: str, max_length: int = 100) -> Dict:\n",
        "        \"\"\"Query Hugging Face model with free API\"\"\"\n",
        "        if \"huggingface\" not in self.available_services:\n",
        "            return {\"error\": \"Hugging Face not setup. Run setup_huggingface_free() first\"}\n",
        "        \n",
        "        api_url = self.available_services[\"huggingface\"][\"api_url\"] + model_name\n",
        "        headers = {\"Authorization\": f\"Bearer {self.available_services['huggingface']['token']}\"}\n",
        "        \n",
        "        payload = {\n",
        "            \"inputs\": prompt,\n",
        "            \"parameters\": {\"max_length\": max_length, \"temperature\": 0.7}\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = requests.post(api_url, headers=headers, json=payload)\n",
        "            result = response.json()\n",
        "            \n",
        "            if isinstance(result, list) and len(result) > 0:\n",
        "                return {\n",
        "                    \"output\": result[0].get(\"generated_text\", \"\").replace(prompt, \"\").strip(),\n",
        "                    \"model\": model_name,\n",
        "                    \"service\": \"huggingface\",\n",
        "                    \"success\": True\n",
        "                }\n",
        "            else:\n",
        "                return {\"error\": f\"Unexpected response: {result}\", \"success\": False}\n",
        "                \n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"success\": False}\n",
        "    \n",
        "    def query_gemini(self, prompt: str, model: str = \"gemini-1.5-flash\") -> Dict:\n",
        "        \"\"\"Query Gemini with free API (placeholder - would need google-generativeai package)\"\"\"\n",
        "        if \"gemini\" not in self.available_services:\n",
        "            return {\"error\": \"Gemini not setup. Run setup_gemini_free() first\"}\n",
        "        \n",
        "        # This would require: pip install google-generativeai\n",
        "        # For now, showing the structure students would use\n",
        "        return {\n",
        "            \"output\": f\"[Gemini response - install google-generativeai package to use]\",\n",
        "            \"model\": model,\n",
        "            \"service\": \"gemini\", \n",
        "            \"success\": True,\n",
        "            \"note\": \"Install: pip install google-generativeai\"\n",
        "        }\n",
        "\n",
        "# Initialize free API manager\n",
        "free_api = FreeAPIManager()\n",
        "\n",
        "print(\"ðŸ†“ FREE API OPTIONS FOR STUDENTS\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Choose one of these FREE options:\")\n",
        "print()\n",
        "print(\"Option 1: ðŸ¤— Hugging Face (Recommended for beginners)\")\n",
        "print(\"   - 1000 free requests per month\")\n",
        "print(\"   - Multiple models available\")\n",
        "print(\"   - Easy to get started\")\n",
        "print()\n",
        "print(\"Option 2: ðŸŸ¢ Google Gemini\")  \n",
        "print(\"   - 60 requests per minute (very generous!)\")\n",
        "print(\"   - High-quality responses\")\n",
        "print(\"   - Excellent for production testing\")\n",
        "print()\n",
        "print(\"Option 3: ðŸ”¥ Local Model (Already loaded above)\")\n",
        "print(\"   - Completely free (uses Colab GPU)\")\n",
        "print(\"   - Works offline\")\n",
        "print(\"   - Perfect for learning\")\n",
        "\n",
        "print(\"\\nðŸ’¡ SETUP INSTRUCTIONS:\")\n",
        "print(\"   Uncomment ONE of these lines to setup your preferred API:\")\n",
        "print(\"   # free_api.setup_huggingface_free()  # For Hugging Face\")\n",
        "print(\"   # free_api.setup_gemini_free()       # For Google Gemini\")\n",
        "print(\"\\nâœ… All services work perfectly on Google Colab!\")\n",
        "\n",
        "# Test function for whichever API is setup\n",
        "def test_free_api(prompt: str = \"Hello! How are you today?\"):\n",
        "    \"\"\"Test whichever API service is configured\"\"\"\n",
        "    if \"huggingface\" in free_api.available_services:\n",
        "        print(\"Testing Hugging Face API...\")\n",
        "        result = free_api.query_huggingface(\"microsoft/DialoGPT-medium\", prompt)\n",
        "        print(f\"Result: {result}\")\n",
        "    elif \"gemini\" in free_api.available_services:\n",
        "        print(\"Testing Gemini API...\")\n",
        "        result = free_api.query_gemini(prompt)\n",
        "        print(f\"Result: {result}\")\n",
        "    else:\n",
        "        print(\"âš ï¸  No API configured yet. Run setup first!\")\n",
        "\n",
        "print(\"\\nðŸ§ª After setup, test with: test_free_api()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f23b8be9",
      "metadata": {},
      "source": [
        "# Part 5: Advanced Prompt Engineering Techniques\n",
        "\n",
        "## 5.1 Self-Consistency Prompting\n",
        "\n",
        "**Self-Consistency** is a powerful technique where we generate multiple responses to the same prompt and choose the most consistent answer. This significantly improves accuracy, especially for reasoning tasks.\n",
        "\n",
        "**How Self-Consistency Works:**\n",
        "1. Generate multiple responses (typically 3-5) to the same prompt\n",
        "2. Analyze the consistency across responses  \n",
        "3. Choose the most frequent/consistent answer\n",
        "4. Particularly effective for mathematical reasoning and factual questions\n",
        "\n",
        "**ðŸ“Š Research shows 10-20% accuracy improvement with self-consistency!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26576362",
      "metadata": {},
      "source": [
        "### ðŸ§  Ready for Advanced Techniques?\n",
        "\n",
        "**You've learned the basics!** Now we'll explore more advanced prompt engineering techniques.\n",
        "\n",
        "**What You'll Master Next:**\n",
        "1. **Self-Consistency**: Ask the AI the same question multiple times and pick the best answer\n",
        "2. **Hyperparameter Tuning**: Adjust AI \"settings\" to get better results  \n",
        "3. **Systematic Evaluation**: Measure which techniques work best\n",
        "\n",
        "**Why This Matters:**\n",
        "- Self-consistency can improve accuracy by 10-20%\n",
        "- Proper hyperparameters can make responses more reliable\n",
        "- Evaluation helps you choose the right technique for your task\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f001db8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ§  SELF-CONSISTENCY PROMPTING IMPLEMENTATION\n",
        "# Generate multiple responses and find the most consistent answer\n",
        "\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "class SelfConsistencyEngine:\n",
        "    \"\"\"\n",
        "    Implement self-consistency prompting for improved accuracy\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_manager):\n",
        "        self.model_manager = model_manager\n",
        "        \n",
        "    def generate_multiple_responses(self, prompt: str, num_responses: int = 5, \n",
        "                                  temperature: float = 0.8) -> List[Dict]:\n",
        "        \"\"\"Generate multiple responses for self-consistency evaluation\"\"\"\n",
        "        responses = []\n",
        "        \n",
        "        print(f\"ðŸ”„ Generating {num_responses} responses for self-consistency...\")\n",
        "        \n",
        "        for i in range(num_responses):\n",
        "            print(f\"   Response {i+1}/{num_responses}...\", end=\"\")\n",
        "            \n",
        "            result = self.model_manager.generate_text_systematic(\n",
        "                prompt, \n",
        "                max_length=100, \n",
        "                temperature=temperature\n",
        "            )\n",
        "            \n",
        "            if result[\"error\"]:\n",
        "                print(f\" âŒ Error\")\n",
        "                continue\n",
        "                \n",
        "            responses.append({\n",
        "                \"response_id\": i+1,\n",
        "                \"output\": result[\"output\"],\n",
        "                \"metrics\": result[\"metrics\"]\n",
        "            })\n",
        "            print(f\" âœ…\")\n",
        "            time.sleep(0.1)  # Small delay to avoid overwhelming APIs\n",
        "            \n",
        "        return responses\n",
        "    \n",
        "    def extract_answers(self, responses: List[Dict], task_type: str = \"classification\") -> List[str]:\n",
        "        \"\"\"Extract the core answers from responses for comparison\"\"\"\n",
        "        answers = []\n",
        "        \n",
        "        for response in responses:\n",
        "            output = response[\"output\"].strip()\n",
        "            \n",
        "            if task_type == \"classification\":\n",
        "                # Extract the classification result (first word or after \":\")\n",
        "                if \":\" in output:\n",
        "                    answer = output.split(\":\")[-1].strip().split()[0].lower()\n",
        "                else:\n",
        "                    answer = output.split()[0].lower()\n",
        "                    \n",
        "            elif task_type == \"number\":\n",
        "                # Extract numerical answers\n",
        "                import re\n",
        "                numbers = re.findall(r'\\d+', output)\n",
        "                answer = numbers[0] if numbers else \"no_number\"\n",
        "                \n",
        "            else:  # general text\n",
        "                # Use first significant word/phrase\n",
        "                answer = output.split('.')[0].strip()[:50]\n",
        "            \n",
        "            answers.append(answer)\n",
        "            \n",
        "        return answers\n",
        "    \n",
        "    def find_consensus(self, answers: List[str]) -> Dict:\n",
        "        \"\"\"Find the most consistent answer across multiple responses\"\"\"\n",
        "        if not answers:\n",
        "            return {\"consensus\": None, \"confidence\": 0, \"distribution\": {}}\n",
        "        \n",
        "        # Count frequency of each answer\n",
        "        answer_counts = Counter(answers)\n",
        "        most_common_answer, max_count = answer_counts.most_common(1)[0]\n",
        "        \n",
        "        # Calculate confidence as percentage of responses\n",
        "        confidence = max_count / len(answers)\n",
        "        \n",
        "        return {\n",
        "            \"consensus\": most_common_answer,\n",
        "            \"confidence\": confidence,\n",
        "            \"distribution\": dict(answer_counts),\n",
        "            \"total_responses\": len(answers)\n",
        "        }\n",
        "    \n",
        "    def self_consistent_query(self, prompt: str, task_type: str = \"classification\", \n",
        "                            num_responses: int = 5) -> Dict:\n",
        "        \"\"\"\n",
        "        Perform complete self-consistency evaluation\n",
        "        \"\"\"\n",
        "        print(f\"ðŸŽ¯ SELF-CONSISTENCY EVALUATION\")\n",
        "        print(f\"   Task: {task_type}\")\n",
        "        print(f\"   Responses: {num_responses}\")\n",
        "        print(f\"   Prompt: {prompt[:100]}...\")\n",
        "        print()\n",
        "        \n",
        "        # Generate multiple responses\n",
        "        responses = self.generate_multiple_responses(prompt, num_responses)\n",
        "        \n",
        "        if len(responses) < 2:\n",
        "            return {\"error\": \"Need at least 2 successful responses for consensus\"}\n",
        "        \n",
        "        # Extract answers for comparison\n",
        "        answers = self.extract_answers(responses, task_type)\n",
        "        \n",
        "        # Find consensus\n",
        "        consensus_result = self.find_consensus(answers)\n",
        "        \n",
        "        print(f\"\\\\nðŸ“Š SELF-CONSISTENCY RESULTS:\")\n",
        "        print(f\"   Consensus: {consensus_result['consensus']}\")\n",
        "        print(f\"   Confidence: {consensus_result['confidence']:.1%}\")\n",
        "        print(f\"   Distribution: {consensus_result['distribution']}\")\n",
        "        \n",
        "        return {\n",
        "            \"consensus\": consensus_result,\n",
        "            \"individual_responses\": responses,\n",
        "            \"extracted_answers\": answers,\n",
        "            \"success\": True\n",
        "        }\n",
        "\n",
        "# Initialize self-consistency engine (if model manager is available)\n",
        "if 'model_manager' in locals() and model_manager.current_model:\n",
        "    sc_engine = SelfConsistencyEngine(model_manager)\n",
        "    \n",
        "    print(\"ðŸ§  SELF-CONSISTENCY ENGINE READY!\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Demo with a classification task\n",
        "    demo_prompt = \"\"\"Classify this dialogue type: meeting, social, support, or transaction.\n",
        "\n",
        "Dialogue: A: I need help with my password reset. B: I can help you with that. Let me send you a reset link.\n",
        "\n",
        "Classification:\"\"\"\n",
        "    \n",
        "    print(\"ðŸ§ª DEMO: Self-Consistency vs Single Response\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Single response (traditional)\n",
        "    single_result = model_manager.generate_text_systematic(demo_prompt, temperature=0.1)\n",
        "    print(f\"ðŸ”¸ Single response: {single_result.get('output', 'Error')}\")\n",
        "    \n",
        "    # Self-consistency (multiple responses)\n",
        "    print(\"\\\\nðŸ”¸ Self-consistency evaluation:\")\n",
        "    sc_result = sc_engine.self_consistent_query(demo_prompt, \"classification\", 3)\n",
        "    \n",
        "    if sc_result[\"success\"]:\n",
        "        print(f\"\\\\nâœ… IMPROVEMENT WITH SELF-CONSISTENCY:\")\n",
        "        print(f\"   Single: {single_result.get('output', 'Error')}\")\n",
        "        print(f\"   Consensus: {sc_result['consensus']['consensus']}\")\n",
        "        print(f\"   Confidence: {sc_result['consensus']['confidence']:.1%}\")\n",
        "        \n",
        "        if sc_result['consensus']['confidence'] >= 0.6:\n",
        "            print(f\"   ðŸŽ¯ High confidence - reliable answer!\")\n",
        "        else:\n",
        "            print(f\"   âš ï¸  Low confidence - may need more responses or prompt tuning\")\n",
        "    \n",
        "else:\n",
        "    print(\"âš ï¸  Model manager not available - showing self-consistency concept only\")\n",
        "    print(\"\\\\nðŸ”¬ Self-Consistency Process:\")\n",
        "    print(\"1. Generate 3-5 responses with temperature > 0.5\")\n",
        "    print(\"2. Extract core answers from each response\") \n",
        "    print(\"3. Count frequency of each answer\")\n",
        "    print(\"4. Choose most frequent answer as consensus\")\n",
        "    print(\"5. Calculate confidence as agreement percentage\")\n",
        "\n",
        "print(\"\\\\nðŸ’¡ WHEN TO USE SELF-CONSISTENCY:\")\n",
        "print(\"   âœ… Mathematical reasoning tasks\")\n",
        "print(\"   âœ… Factual questions with definitive answers\")\n",
        "print(\"   âœ… Classification tasks\")\n",
        "print(\"   âœ… When accuracy is more important than speed\")\n",
        "print(\"   âŒ Creative writing tasks\")\n",
        "print(\"   âŒ Open-ended discussions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2edc2bc",
      "metadata": {},
      "source": [
        "## 5.2 LLM Hyperparameters Deep Dive\n",
        "\n",
        "**Understanding hyperparameters is crucial for effective prompt engineering!**\n",
        "\n",
        "| **Parameter** | **Range** | **Effect** | **Use When** | **Avoid When** |\n",
        "|---------------|-----------|------------|--------------|----------------|\n",
        "| **ðŸŒ¡ï¸ Temperature** | 0.0-2.0 | Controls randomness | Creative tasks (0.7-1.2) | Factual Q&A (0.0-0.3) |\n",
        "| **ðŸŽ¯ Top-p** | 0.1-1.0 | Nucleus sampling | Balanced control (0.8-0.95) | Extreme creativity or determinism |\n",
        "| **ðŸ”¢ Top-k** | 1-100 | Limits vocabulary | Focused domains (10-40) | Open conversations (high k) |\n",
        "| **ðŸ“ Max Length** | 1-4096+ | Output length limit | Specific formats | Open exploration |\n",
        "| **ðŸ” Repetition Penalty** | 0.8-1.5 | Reduces repetition | Avoiding loops (1.1-1.3) | Poetry/patterns (â‰¤1.0) |\n",
        "\n",
        "**ðŸŽ¯ Optimal Settings by Task:**\n",
        "- **Factual Q&A**: temp=0.1, top_p=0.9, max_length=100\n",
        "- **Creative Writing**: temp=0.8, top_p=0.95, max_length=500\n",
        "- **Code Generation**: temp=0.2, top_p=0.9, max_length=200\n",
        "- **Classification**: temp=0.0, top_p=0.8, max_length=10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048b62f8",
      "metadata": {},
      "source": [
        "### ðŸ“š Before Running Self-Consistency Code:\n",
        "\n",
        "**ðŸŽ¯ What This Cell Will Do:**\n",
        "1. Create a `SelfConsistencyEngine` class (like a smart assistant)\n",
        "2. Generate multiple responses to the same question\n",
        "3. Find the most common/consistent answer\n",
        "4. Show you confidence scores\n",
        "\n",
        "**â±ï¸ What to Expect:**\n",
        "- The code will run a bit slower (generates 3-5 responses instead of 1)\n",
        "- You'll see multiple attempts at the same question\n",
        "- Final result shows the \"consensus\" answer with confidence percentage\n",
        "\n",
        "**ðŸ“Š Example Output:**\n",
        "```\n",
        "ðŸ”„ Generating 5 responses for self-consistency...\n",
        "   Response 1/5... âœ…\n",
        "   Response 2/5... âœ…\n",
        "   ...\n",
        "ðŸ“Š SELF-CONSISTENCY RESULTS:\n",
        "   Consensus: support\n",
        "   Confidence: 80%\n",
        "```\n",
        "\n",
        "**âš ï¸ If It's Slow:** This is normal - we're generating multiple responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "654d1335",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ§ª HYPERPARAMETER EXPERIMENTATION FRAMEWORK\n",
        "# Systematic exploration of LLM hyperparameters\n",
        "\n",
        "class HyperparameterExplorer:\n",
        "    \"\"\"\n",
        "    Systematically test different hyperparameter combinations\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_manager):\n",
        "        self.model_manager = model_manager\n",
        "        self.results_df = pd.DataFrame()\n",
        "        \n",
        "    def create_hyperparameter_grid(self, task_type: str = \"classification\") -> List[Dict]:\n",
        "        \"\"\"Create systematic hyperparameter combinations for testing\"\"\"\n",
        "        \n",
        "        if task_type == \"classification\":\n",
        "            return [\n",
        "                {\"temperature\": 0.0, \"max_length\": 20, \"label\": \"Deterministic\"},\n",
        "                {\"temperature\": 0.3, \"max_length\": 20, \"label\": \"Low creativity\"},\n",
        "                {\"temperature\": 0.7, \"max_length\": 20, \"label\": \"Balanced\"},\n",
        "                {\"temperature\": 1.0, \"max_length\": 20, \"label\": \"Creative\"},\n",
        "            ]\n",
        "        elif task_type == \"generation\":\n",
        "            return [\n",
        "                {\"temperature\": 0.1, \"max_length\": 100, \"label\": \"Conservative\"}, \n",
        "                {\"temperature\": 0.5, \"max_length\": 100, \"label\": \"Moderate\"},\n",
        "                {\"temperature\": 0.8, \"max_length\": 100, \"label\": \"Creative\"},\n",
        "                {\"temperature\": 1.2, \"max_length\": 100, \"label\": \"Very creative\"},\n",
        "            ]\n",
        "        else:\n",
        "            return [\n",
        "                {\"temperature\": 0.2, \"max_length\": 50, \"label\": \"Default low\"},\n",
        "                {\"temperature\": 0.7, \"max_length\": 50, \"label\": \"Default balanced\"},\n",
        "            ]\n",
        "    \n",
        "    def test_hyperparameter_grid(self, prompt: str, task_type: str = \"classification\", \n",
        "                                runs_per_config: int = 3) -> pd.DataFrame:\n",
        "        \"\"\"Test systematic combinations of hyperparameters\"\"\"\n",
        "        \n",
        "        param_grid = self.create_hyperparameter_grid(task_type)\n",
        "        results = []\n",
        "        \n",
        "        print(f\"ðŸ§ª HYPERPARAMETER GRID SEARCH\")\n",
        "        print(f\"   Prompt: {prompt[:50]}...\")\n",
        "        print(f\"   Configurations: {len(param_grid)}\")\n",
        "        print(f\"   Runs per config: {runs_per_config}\")\n",
        "        print(f\"   Total experiments: {len(param_grid) * runs_per_config}\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        for i, config in enumerate(param_grid):\n",
        "            print(f\"\\\\nðŸŽ›ï¸  Config {i+1}/{len(param_grid)}: {config['label']}\")\n",
        "            print(f\"   Temperature: {config['temperature']}\")\n",
        "            print(f\"   Max length: {config['max_length']}\")\n",
        "            \n",
        "            config_results = []\n",
        "            \n",
        "            for run in range(runs_per_config):\n",
        "                print(f\"   Run {run+1}/{runs_per_config}...\", end=\"\")\n",
        "                \n",
        "                result = self.model_manager.generate_text_systematic(\n",
        "                    prompt,\n",
        "                    max_length=config[\"max_length\"],\n",
        "                    temperature=config[\"temperature\"]\n",
        "                )\n",
        "                \n",
        "                if result[\"error\"]:\n",
        "                    print(\" âŒ\")\n",
        "                    continue\n",
        "                \n",
        "                # Store systematic results\n",
        "                results.append({\n",
        "                    \"config_id\": i,\n",
        "                    \"config_label\": config[\"label\"],\n",
        "                    \"temperature\": config[\"temperature\"],\n",
        "                    \"max_length\": config[\"max_length\"],\n",
        "                    \"run_id\": run,\n",
        "                    \"output\": result[\"output\"],\n",
        "                    \"generation_time_ms\": result[\"metrics\"].get(\"generation_time_ms\", 0),\n",
        "                    \"output_tokens\": result[\"metrics\"].get(\"output_tokens\", 0),\n",
        "                    \"output_length\": len(result[\"output\"]),\n",
        "                    \"timestamp\": time.time()\n",
        "                })\n",
        "                \n",
        "                config_results.append(result[\"output\"])\n",
        "                print(\" âœ…\")\n",
        "            \n",
        "            # Show variety within this configuration\n",
        "            if config_results:\n",
        "                unique_outputs = len(set(config_results))\n",
        "                print(f\"   Unique outputs: {unique_outputs}/{len(config_results)}\")\n",
        "                if len(config_results) > 1:\n",
        "                    print(f\"   Sample outputs:\")\n",
        "                    for j, output in enumerate(config_results[:2]):\n",
        "                        print(f\"     {j+1}: {output[:60]}...\")\n",
        "        \n",
        "        # Convert to DataFrame for analysis\n",
        "        results_df = pd.DataFrame(results)\n",
        "        self.results_df = pd.concat([self.results_df, results_df], ignore_index=True)\n",
        "        \n",
        "        return results_df\n",
        "    \n",
        "    def analyze_hyperparameter_effects(self, results_df: pd.DataFrame):\n",
        "        \"\"\"Analyze the effects of different hyperparameter settings\"\"\"\n",
        "        \n",
        "        print(f\"\\\\nðŸ“Š HYPERPARAMETER ANALYSIS\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        # Group by configuration\n",
        "        config_analysis = results_df.groupby(['config_label', 'temperature']).agg({\n",
        "            'output_length': ['mean', 'std'],\n",
        "            'generation_time_ms': ['mean', 'std'],\n",
        "            'output': lambda x: len(set(x))  # Unique outputs (diversity)\n",
        "        }).round(2)\n",
        "        \n",
        "        config_analysis.columns = ['Avg_Length', 'Std_Length', 'Avg_Time_ms', 'Std_Time_ms', 'Diversity']\n",
        "        \n",
        "        print(\"\\\\nðŸŽ¯ Configuration Performance:\")\n",
        "        display(config_analysis)\n",
        "        \n",
        "        # Temperature effect analysis\n",
        "        print(f\"\\\\nðŸŒ¡ï¸ TEMPERATURE EFFECTS:\")\n",
        "        temp_effects = results_df.groupby('temperature').agg({\n",
        "            'output_length': 'mean',\n",
        "            'output': lambda x: len(set(x)) / len(x)  # Diversity ratio\n",
        "        }).round(3)\n",
        "        \n",
        "        for temp, row in temp_effects.iterrows():\n",
        "            diversity_pct = row['output'] * 100\n",
        "            print(f\"   Temperature {temp}: Avg length {row['output_length']:.0f}, Diversity {diversity_pct:.0f}%\")\n",
        "        \n",
        "        # Visualization if matplotlib is available\n",
        "        try:\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "            \n",
        "            # Length vs Temperature\n",
        "            results_df.groupby('temperature')['output_length'].mean().plot(kind='bar', ax=ax1)\n",
        "            ax1.set_title('Output Length vs Temperature')\n",
        "            ax1.set_ylabel('Average Output Length')\n",
        "            \n",
        "            # Generation time vs Temperature  \n",
        "            results_df.groupby('temperature')['generation_time_ms'].mean().plot(kind='bar', ax=ax2)\n",
        "            ax2.set_title('Generation Time vs Temperature')\n",
        "            ax2.set_ylabel('Average Time (ms)')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "        except:\n",
        "            print(\"   (Visualization requires matplotlib)\")\n",
        "        \n",
        "        return config_analysis\n",
        "\n",
        "# Initialize hyperparameter explorer (if model available)\n",
        "if 'model_manager' in locals() and model_manager.current_model:\n",
        "    hp_explorer = HyperparameterExplorer(model_manager)\n",
        "    \n",
        "    print(\"ðŸŽ›ï¸ HYPERPARAMETER EXPLORER READY!\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Demo hyperparameter experimentation\n",
        "    demo_prompt = \"Classify this conversation: A: Can you help me reset my password? B: Of course, I'll send you a link.\"\n",
        "    \n",
        "    print(\"\\\\nðŸ§ª DEMO: Hyperparameter Grid Search\")\n",
        "    print(\"Testing different temperature settings...\")\n",
        "    \n",
        "    # Run hyperparameter grid search\n",
        "    hp_results = hp_explorer.test_hyperparameter_grid(\n",
        "        demo_prompt, \n",
        "        task_type=\"classification\",\n",
        "        runs_per_config=2  # Use 2 for demo (normally 3-5)\n",
        "    )\n",
        "    \n",
        "    # Analyze results\n",
        "    analysis = hp_explorer.analyze_hyperparameter_effects(hp_results)\n",
        "    \n",
        "    print(\"\\\\nðŸ’¡ KEY INSIGHTS:\")\n",
        "    print(\"   ðŸŒ¡ï¸  Lower temperature = more consistent outputs\")\n",
        "    print(\"   ðŸŒ¡ï¸  Higher temperature = more diverse/creative outputs\") \n",
        "    print(\"   â±ï¸  Temperature has minimal effect on generation speed\")\n",
        "    print(\"   ðŸ“ Max length controls output verbosity\")\n",
        "    \n",
        "else:\n",
        "    print(\"âš ï¸  Model manager not available - showing hyperparameter concepts\")\n",
        "    \n",
        "print(\"\\\\nðŸŽ¯ HYPERPARAMETER BEST PRACTICES:\")\n",
        "print(\"   ðŸ“‹ Classification tasks: temperature = 0.0-0.3\")\n",
        "print(\"   âœï¸  Creative generation: temperature = 0.7-1.0\") \n",
        "print(\"   ðŸ” Factual Q&A: temperature = 0.0-0.2\")\n",
        "print(\"   ðŸ—£ï¸  Dialogue: temperature = 0.4-0.7\")\n",
        "print(\"   ðŸ§® Code generation: temperature = 0.1-0.4\")\n",
        "\n",
        "print(\"\\\\nâš¡ PERFORMANCE TIPS:\")\n",
        "print(\"   âœ… Start with temperature=0.0 for reproducible results\")\n",
        "print(\"   âœ… Increase temperature gradually for more creativity\")\n",
        "print(\"   âœ… Use max_length to prevent overly long responses\")\n",
        "print(\"   âœ… Test multiple runs to understand variability\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9821f188",
      "metadata": {},
      "source": [
        "### ðŸ“Š Understanding Self-Consistency Results:\n",
        "\n",
        "**ðŸŽ¯ How to Read the Output:**\n",
        "\n",
        "**âœ… Good Results (What You Want to See):**\n",
        "- **Confidence â‰¥ 60%**: The AI is fairly sure about the answer\n",
        "- **Clear consensus**: One answer appears much more often than others\n",
        "- **\"High confidence - reliable answer!\"** message\n",
        "\n",
        "**âš ï¸ Uncertain Results (Need More Work):**\n",
        "- **Confidence < 60%**: The AI is confused or the question is unclear\n",
        "- **Mixed results**: Multiple different answers with similar frequency\n",
        "- **\"Low confidence - may need more responses\"** message\n",
        "\n",
        "**ðŸ”§ What to Do with Low Confidence:**\n",
        "1. Try more responses (increase from 3 to 5 or 7)\n",
        "2. Make your prompt clearer and more specific\n",
        "3. Check if the question itself is ambiguous\n",
        "\n",
        "**ðŸ’¡ Real-World Tip:**\n",
        "In production systems, you might only trust results with >70% confidence!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f0f14e1",
      "metadata": {},
      "source": [
        "# ðŸŽ“ Part 6: Session Wrap-up\n",
        "\n",
        "## 6.1 SESSION COMPLETE: Advanced Prompt Engineering Mastery\n",
        "\n",
        "### Next Steps for Your Projects\n",
        "\n",
        "**Immediate Actions:**\n",
        "1. **Choose your API**: Set up Hugging Face or Gemini free API\n",
        "2. **Test with your language**: Apply techniques to your specific use case\n",
        "3. **Document insights**: Record what works best for your domain\n",
        "4. **Experiment systematically**: Use the evaluation frameworks provided\n",
        "\n",
        "**ðŸ”¬ Advanced Experiments:**\n",
        "- Compare self-consistency vs single-shot for your tasks\n",
        "- Optimize hyperparameters for your specific language/domain\n",
        "- Create custom few-shot examples for your cultural context\n",
        "- Build evaluation metrics for your specific requirements\n",
        "\n",
        "### ðŸ’¡ Key Production Insights\n",
        "\n",
        "**ðŸŽ¯ For Classification Tasks:**\n",
        "- Use temperature=0.0 with self-consistency (3-5 responses)\n",
        "- Few-shot examples improve cross-lingual transfer\n",
        "- Cultural context matters more than linguistic accuracy\n",
        "\n",
        "**âœï¸ For Text Generation:**\n",
        "- Start with temperature=0.7, adjust based on creativity needs\n",
        "- Chain-of-thought improves factual accuracy significantly  \n",
        "- Self-consistency reduces hallucination in factual tasks\n",
        "\n",
        "**âš¡ For Performance:**\n",
        "- Local mT5 models: Best for privacy and customization\n",
        "- Free APIs: Excellent for learning and small-scale projects\n",
        "- Systematic evaluation saves time in the long run\n",
        "\n",
        "### ðŸŒŸ Remember: Ethics and Responsible AI\n",
        "\n",
        "**Always consider:**\n",
        "- Cultural sensitivity in your prompts and evaluations\n",
        "- Privacy implications of your chosen API service\n",
        "- Bias evaluation across different demographic groups\n",
        "- Environmental impact of your model choices\n",
        "\n",
        "---\n",
        "\n",
        "**ðŸŽ‰ Congratulations!** You now have production-ready prompt engineering skills with systematic evaluation methodology. Keep experimenting and building responsibly! ðŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0474402c",
      "metadata": {},
      "source": [
        "### Understanding Hyperparameters (AI \"Settings\")\n",
        "\n",
        "**Think of hyperparameters like camera settings:**\n",
        "- **Temperature** = How \"creative\" vs \"predictable\" the AI is\n",
        "- **Max Length** = How long responses can be  \n",
        "- **Top-p** = How many word choices the AI considers\n",
        "\n",
        "**ðŸ“± Just Like Your Phone:**\n",
        "- Camera app: Brightness, contrast, focus settings\n",
        "- AI model: Temperature, length, creativity settings\n",
        "\n",
        "**What You'll Learn:**\n",
        "1. Which settings work best for different tasks\n",
        "2. How to test settings systematically  \n",
        "3. How to measure which settings give better results\n",
        "\n",
        "**âš ï¸ Important:** These settings can dramatically change your results!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daa762ae",
      "metadata": {},
      "source": [
        "### What the Hyperparameter Experiment Will Do:\n",
        "\n",
        "**This Cell Creates a Scientific Experiment:**\n",
        "1. Test the same question with different \"temperature\" settings\n",
        "2. Run each setting multiple times to see consistency\n",
        "3. Measure speed, length, and diversity of responses\n",
        "4. Show you which setting works best\n",
        "\n",
        "**What You'll See:**\n",
        "- Multiple configurations tested (Deterministic, Creative, etc.)\n",
        "- Timing measurements for each response\n",
        "- Summary table comparing all settings\n",
        "- Graphs showing the relationships (if matplotlib works)\n",
        "\n",
        "**Expected Runtime:**\n",
        "- With local model: 1-2 minutes total\n",
        "- Each setting tested 2-3 times\n",
        "- Progress shown for each test\n",
        "\n",
        "**Key Learning:**\n",
        "You'll discover that lower temperature = more consistent, higher temperature = more creative!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89bf217e",
      "metadata": {},
      "source": [
        "### ðŸ“Š How to Read Your Hyperparameter Results:\n",
        "\n",
        "**ðŸ” Understanding the Numbers:**\n",
        "\n",
        "**Average Length:**\n",
        "- Higher = More verbose responses\n",
        "- Lower = More concise responses\n",
        "- **Good for:** Classification tasks want low, creative writing wants higher\n",
        "\n",
        "**Generation Time:**\n",
        "- Lower = Faster responses  \n",
        "- Higher = Slower responses\n",
        "- **Note:** Temperature usually doesn't affect speed much\n",
        "\n",
        "**Diversity Score:**\n",
        "- Higher = More varied responses across runs\n",
        "- Lower = More consistent/predictable responses\n",
        "- **Good for:** Facts need low diversity, creativity needs high diversity\n",
        "\n",
        "**ðŸŽ¯ Choosing the Right Settings:**\n",
        "- **For factual Q&A:** Low temperature (0.0-0.2), low diversity\n",
        "- **For creative tasks:** Higher temperature (0.7-1.0), high diversity  \n",
        "- **For classification:** Very low temperature (0.0), maximum consistency\n",
        "\n",
        "**ðŸ’¡ Pro Tip:** Save the settings that work best for your specific use case!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cca0c03",
      "metadata": {},
      "source": [
        "### ðŸ’¼ Practical Next Steps for Your Projects\n",
        "\n",
        "**To Apply This to Your Own Work:**\n",
        "\n",
        "**1. Choose Your Setup (Pick One):**\n",
        "- **Local mT5**: Best for privacy, works offline, good for learning\n",
        "- **Hugging Face Free API**: Easy setup, 1000 free requests/month\n",
        "- **Google Gemini API**: High quality, 60 requests/minute free\n",
        "\n",
        "**2. Start Small:**\n",
        "- Pick ONE task (classification, summarization, or Q&A)\n",
        "- Test with 5-10 examples first  \n",
        "- Use temperature=0.0 for consistent results\n",
        "\n",
        "**3. Scale Up Systematically:**\n",
        "- Test zero-shot first (no examples)\n",
        "- Add few-shot examples if needed\n",
        "- Use self-consistency for critical decisions\n",
        "- Document what works best\n",
        "\n",
        "**4. For Low-Resource Languages:**\n",
        "- Start with English examples, then translate\n",
        "- Test cultural appropriateness carefully\n",
        "- Use few-shot examples from your target culture\n",
        "\n",
        "**ðŸ“ž Remember:**\n",
        "- All these techniques work on Google Colab\n",
        "- The free tiers are perfect for learning and small projects\n",
        "- Build systematically, don't try everything at once!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
