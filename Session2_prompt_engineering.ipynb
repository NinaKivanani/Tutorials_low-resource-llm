{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session 2: Pretrained Models and Prompt Engineering ðŸ¤–\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "**ðŸ“š Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/Session2_prompt_engineering.ipynb)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "Welcome to **systematic LLM-based prompt engineering** for dialogue summarization and cross-lingual tasks! This session combines rigorous methodology with practical applications, focusing on low-resource language challenges.\n",
        "\n",
        "**ðŸŽ¯ Focus:** Systematic prompt engineering, multilingual evaluation, dialogue summarization  \n",
        "**ðŸ’» Requirements:** GPU recommended for large models OR API access for best results  \n",
        "**ðŸ”¬ Methodology:** Research-grade systematic evaluation with pandas DataFrames\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "**ðŸ“‹ Recommended learning path:**\n",
        "1. **Session 0:** Setup and tokenization basics âœ…  \n",
        "2. **Session 1:** Systematic baseline techniques âœ…\n",
        "3. **This session (Session 2):** Systematic LLM prompt engineering â† You are here!\n",
        "\n",
        "## What You Will Master\n",
        "\n",
        "1. **ðŸ—ï¸ Model family comparison** and systematic access pattern evaluation\n",
        "2. **ðŸŽ¨ Prompt engineering vs. prompt design** with systematic methodology\n",
        "3. **ðŸŽ¯ Multi-strategy prompting** (zero-shot, few-shot, Chain-of-Thought) with quantitative comparison\n",
        "4. **ðŸŒ Cross-lingual prompt transfer** with systematic cultural adaptation\n",
        "5. **ðŸ“Š Comprehensive evaluation framework** (correctness, fluency, cultural appropriateness)\n",
        "6. **ðŸ’¼ Production-ready insights** with actionable recommendations\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this session, you will:\n",
        "- âœ… **Systematically compare** pretrained model families using structured evaluation\n",
        "- âœ… **Design culturally-aware prompts** for dialogue summarization and classification\n",
        "- âœ… **Implement systematic prompt engineering** with quantitative tracking\n",
        "- âœ… **Evaluate cross-lingual performance** using research-grade metrics\n",
        "- âœ… **Generate actionable insights** for production deployment decisions\n",
        "- âœ… **Export structured findings** for research and business applications\n",
        "\n",
        "## ðŸ”¬ Research Methodology\n",
        "\n",
        "**This session follows systematic research practices:**\n",
        "\n",
        "- **ðŸ“Š Structured Data Collection:** All experiments tracked in pandas DataFrames\n",
        "- **ðŸŽ¯ Controlled Comparisons:** Systematic A/B testing of prompt strategies  \n",
        "- **ðŸ“ˆ Quantitative Analysis:** Statistical evaluation with visualization\n",
        "- **ðŸŒ Cultural Sensitivity:** Multi-dimensional appropriateness assessment\n",
        "- **ðŸ’¾ Reproducible Results:** Exportable data for further analysis\n",
        "\n",
        "## How This Advanced Session Works\n",
        "\n",
        "- **ðŸŽ“ Theory + Systematic Practice:** Learn concepts â†’ Apply systematically â†’ Analyze quantitatively\n",
        "- **ðŸ”¬ Hypothesis-Driven Experiments:** Form hypotheses â†’ Test systematically â†’ Draw conclusions\n",
        "- **ðŸ“Š Data-First Analysis:** Every decision backed by quantitative evidence\n",
        "- **ðŸ’¬ Evidence-Based Discussions:** Group analysis using concrete experimental data\n",
        "- **ðŸŒ Cross-Cultural Focus:** Systematic evaluation across language/culture pairs\n",
        "- **ðŸ† Production Insights:** Actionable recommendations for real-world deployment\n"
      ],
      "id": "467d9f0f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. ðŸ”¬ Systematic Setup and Model Access Framework\n",
        "\n",
        "**Strategic Decision:** Choose your model access pattern based on systematic evaluation of your requirements.\n",
        "\n",
        "### 0.1 Access Pattern Decision Framework\n",
        "\n",
        "| **Access Pattern** | **Pros** | **Cons** | **Best For** | **Cost** |\n",
        "|-------------------|----------|----------|--------------|----------|\n",
        "| **ðŸ”¥ Local Models (mT5)** | Privacy, offline, customizable | GPU required, setup time | Research, sensitive data | Hardware only |\n",
        "| **â˜ï¸ API Access (GPT-4)** | State-of-art, no setup, scalable | Token costs, internet needed | Production, experiments | $0.01-0.03/1K tokens |\n",
        "| **ðŸŒ Hosted (Colab/HF)** | Free tiers, easy setup | Limited resources, usage caps | Learning, prototyping | Free-$10/month |\n",
        "\n",
        "**ðŸŽ¯ Recommendation Matrix:**\n",
        "- **Learning/Research:** Start with Local Models (below) + backup API for comparison\n",
        "- **Production Planning:** Test with APIs, validate costs, then decide on deployment\n",
        "- **Resource-Constrained:** Use hosted solutions with systematic evaluation\n"
      ],
      "id": "31db6028"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸ“¦ Systematic Setup for Session 2: Advanced Prompt Engineering\n",
        "# Install packages with systematic dependency management\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def install_packages_systematic(packages):\n",
        "    \"\"\"Install packages with better error handling and progress tracking\"\"\"\n",
        "    installed = []\n",
        "    failed = []\n",
        "    \n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "            installed.append(package.split(\">=\")[0].split(\"==\")[0])\n",
        "            print(f\"âœ… {package}\")\n",
        "        except Exception as e:\n",
        "            failed.append((package, str(e)[:50]))\n",
        "            print(f\"âŒ {package}: {str(e)[:50]}...\")\n",
        "    \n",
        "    return installed, failed\n",
        "\n",
        "print(\"ðŸš€ SYSTEMATIC SETUP: Installing advanced packages...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Core packages for systematic evaluation\n",
        "core_packages = [\n",
        "    \"pandas>=1.5.0\",\n",
        "    \"matplotlib>=3.5.0\", \n",
        "    \"seaborn>=0.11.0\",\n",
        "    \"numpy>=1.21.0\"\n",
        "]\n",
        "\n",
        "# LLM packages (grouped for better dependency management)\n",
        "llm_packages = [\n",
        "    \"transformers>=4.35.0\",\n",
        "    \"torch>=1.13.0\", \n",
        "    \"sentencepiece\",\n",
        "    \"accelerate\",\n",
        "    \"datasets\"\n",
        "]\n",
        "\n",
        "print(\"ðŸ“Š Installing data science packages...\")\n",
        "core_installed, core_failed = install_packages_systematic(core_packages)\n",
        "\n",
        "print(\"ðŸ¤– Installing LLM packages...\")\n",
        "llm_installed, llm_failed = install_packages_systematic(llm_packages)\n",
        "\n",
        "# Essential imports for systematic evaluation\n",
        "try:\n",
        "    import torch\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "    from typing import List, Dict, Optional, Tuple\n",
        "    import time\n",
        "    import json\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Set plotting style for professional visualizations\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "    \n",
        "    print(f\"\\nðŸŽ¯ SYSTEM CONFIGURATION:\")\n",
        "    print(f\"   Python: {sys.version.split()[0]}\")\n",
        "    print(f\"   PyTorch: {torch.__version__}\")\n",
        "    print(f\"   Pandas: {pd.__version__}\")\n",
        "    print(f\"   GPU Available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"   GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    \n",
        "    print(f\"\\nðŸ“Š EXPERIMENTAL FRAMEWORK READY:\")\n",
        "    print(f\"   âœ… Structured data collection with pandas\")\n",
        "    print(f\"   âœ… Statistical analysis and visualization\")\n",
        "    print(f\"   âœ… Export capabilities for research\")\n",
        "    print(f\"   âœ… Systematic model comparison framework\")\n",
        "    \n",
        "    setup_success = True\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"\\nâŒ IMPORT ERROR: {e}\")\n",
        "    print(\"ðŸ”„ Try restarting the runtime and running this cell again\")\n",
        "    setup_success = False\n",
        "\n",
        "# Verification and troubleshooting\n",
        "if core_failed or llm_failed:\n",
        "    print(f\"\\nâš ï¸  INSTALLATION ISSUES DETECTED:\")\n",
        "    for pkg, error in core_failed + llm_failed:\n",
        "        print(f\"   âŒ {pkg}: {error}\")\n",
        "    print(f\"\\nðŸ’¡ SOLUTIONS:\")\n",
        "    print(f\"   1. Runtime â†’ Restart Runtime, then re-run this cell\")\n",
        "    print(f\"   2. Check internet connection\")\n",
        "    print(f\"   3. Try installing packages individually\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… SYSTEMATIC SETUP COMPLETE!\")\n",
        "print(\"ðŸ”¬ Ready for research-grade prompt engineering experiments\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1efff16a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ðŸ”¬ Systematic Experimental Framework Setup\n",
        "\n",
        "**Research-Grade Methodology:** Before testing models, we establish systematic evaluation framework.\n",
        "\n",
        "### 1.1 ðŸ“Š Define Experimental Scope\n"
      ],
      "id": "eac65079"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸŒ Configure Your Systematic Experiment: Languages and Tasks\n",
        "# This systematic approach ensures reproducible, comparable results\n",
        "\n",
        "# Define your target languages (CUSTOMIZE THIS FOR YOUR RESEARCH)\n",
        "target_languages = [\n",
        "    {\n",
        "        \"code\": \"en\", \n",
        "        \"name\": \"English\", \n",
        "        \"family\": \"Germanic\",\n",
        "        \"speakers\": \"1.5B\",\n",
        "        \"resource_level\": \"high\",\n",
        "        \"writing_system\": \"Latin\"\n",
        "    },\n",
        "    {\n",
        "        \"code\": \"fr\", \n",
        "        \"name\": \"French\", \n",
        "        \"family\": \"Romance\",\n",
        "        \"speakers\": \"280M\", \n",
        "        \"resource_level\": \"high\",\n",
        "        \"writing_system\": \"Latin\"\n",
        "    },\n",
        "    {\n",
        "        \"code\": \"ar\", \n",
        "        \"name\": \"Arabic\", \n",
        "        \"family\": \"Semitic\",\n",
        "        \"speakers\": \"420M\",\n",
        "        \"resource_level\": \"medium\",\n",
        "        \"writing_system\": \"Arabic\"\n",
        "    },\n",
        "    # ðŸŽ¯ ADD YOUR LOW-RESOURCE LANGUAGE HERE:\n",
        "    # {\n",
        "    #     \"code\": \"your_code\", \n",
        "    #     \"name\": \"Your Language\", \n",
        "    #     \"family\": \"Language Family\",\n",
        "    #     \"speakers\": \"~XXXk\",\n",
        "    #     \"resource_level\": \"low\",\n",
        "    #     \"writing_system\": \"Script\"\n",
        "    # },\n",
        "]\n",
        "\n",
        "# Define systematic task framework for dialogue summarization and classification\n",
        "experimental_tasks = [\n",
        "    {\n",
        "        \"task_id\": \"dialogue_classification\",\n",
        "        \"description\": \"Classify dialogue type: meeting, social, support, transaction\",\n",
        "        \"evaluation_type\": \"categorical_accuracy\",\n",
        "        \"cultural_sensitivity\": \"medium\",\n",
        "        \"domain\": \"general\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"dialogue_summarization\", \n",
        "        \"description\": \"Generate concise summary of dialogue content\",\n",
        "        \"evaluation_type\": \"generative_quality\",\n",
        "        \"cultural_sensitivity\": \"high\",\n",
        "        \"domain\": \"general\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"intent_extraction\",\n",
        "        \"description\": \"Extract primary intent/action items from dialogue\",\n",
        "        \"evaluation_type\": \"information_extraction\", \n",
        "        \"cultural_sensitivity\": \"high\",\n",
        "        \"domain\": \"business\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to DataFrames for systematic analysis\n",
        "languages_df = pd.DataFrame(target_languages)\n",
        "tasks_df = pd.DataFrame(experimental_tasks)\n",
        "\n",
        "print(\"ðŸŒ SYSTEMATIC EXPERIMENTAL DESIGN\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\\\nðŸ“‹ Target Languages:\")\n",
        "display(languages_df[[\"name\", \"family\", \"resource_level\", \"speakers\", \"writing_system\"]])\n",
        "\n",
        "print(\"\\\\nðŸŽ¯ Experimental Tasks:\")\n",
        "display(tasks_df[[\"task_id\", \"description\", \"evaluation_type\", \"cultural_sensitivity\"]])\n",
        "\n",
        "print(f\"\\\\nðŸ“Š EXPERIMENTAL MATRIX:\")\n",
        "print(f\"   Languages: {len(languages_df)} \")\n",
        "print(f\"   Tasks: {len(tasks_df)}\")\n",
        "print(f\"   Total combinations: {len(languages_df) * len(tasks_df)}\")\n",
        "print(f\"   Systematic evaluation ensures comprehensive coverage!\")\n",
        "\n",
        "# Create systematic evaluation tracking framework\n",
        "evaluation_columns = [\n",
        "    # Experiment identifiers\n",
        "    \"experiment_id\", \"timestamp\", \"language_code\", \"language_name\", \"task_id\",\n",
        "    \n",
        "    # Model and prompt configuration\n",
        "    \"model_name\", \"model_type\", \"access_pattern\", \"prompt_strategy\", \"shots_used\",\n",
        "    \n",
        "    # Input/output data\n",
        "    \"input_text\", \"expected_output\", \"actual_output\", \"prompt_text\",\n",
        "    \n",
        "    # Quantitative metrics\n",
        "    \"correctness_score\", \"fluency_score\", \"cultural_appropriateness_score\",\n",
        "    \"response_time_ms\", \"token_count_input\", \"token_count_output\",\n",
        "    \n",
        "    # Qualitative assessment\n",
        "    \"quality_issues\", \"cultural_notes\", \"improvement_suggestions\", \n",
        "    \n",
        "    # Systematic metadata\n",
        "    \"experiment_conditions\", \"model_parameters\", \"success_flag\"\n",
        "]\n",
        "\n",
        "# Initialize systematic experiment tracking\n",
        "experiments_df = pd.DataFrame(columns=evaluation_columns)\n",
        "\n",
        "print(f\"\\\\nðŸ”¬ EVALUATION FRAMEWORK INITIALIZED:\")\n",
        "print(f\"   Tracking {len(evaluation_columns)} systematic metrics per experiment\")\n",
        "print(f\"   Ready for systematic data collection and analysis\")\n",
        "print(f\"   âœ… Research-grade methodology established!\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "95a132a0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 ðŸ“ Systematic Test Data Framework\n",
        "\n",
        "**Structured Test Cases:** Ensure consistent, comparable evaluation across languages and tasks.\n"
      ],
      "id": "03fbca1d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸ“Š Systematic Test Data Creation\n",
        "# Structured test cases for systematic comparison across languages and tasks\n",
        "\n",
        "def create_systematic_test_data():\n",
        "    \"\"\"\n",
        "    Create structured test data for systematic evaluation across languages and tasks.\n",
        "    This ensures consistent comparison and eliminates ad-hoc testing bias.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Systematic dialogue test cases (parallel across languages)\n",
        "    test_cases = []\n",
        "    \n",
        "    # Test Case 1: Business Meeting Scenario\n",
        "    test_cases.append({\n",
        "        \"case_id\": \"business_meeting_01\",\n",
        "        \"domain\": \"business\", \n",
        "        \"complexity\": \"medium\",\n",
        "        \"cultural_context\": \"professional\",\n",
        "        \"languages\": {\n",
        "            \"en\": {\n",
        "                \"dialogue\": \"A: We need to finalize the budget by Friday. B: I'll have the numbers ready by Thursday. A: Perfect, let's schedule a review meeting.\",\n",
        "                \"expected_classification\": \"meeting\",\n",
        "                \"expected_summary\": \"Team discusses budget deadline and schedules review meeting for Thursday numbers.\",\n",
        "                \"expected_intent\": \"Schedule budget review meeting\"\n",
        "            },\n",
        "            \"fr\": {\n",
        "                \"dialogue\": \"A: Nous devons finaliser le budget vendredi. B: J'aurai les chiffres prÃªts jeudi. A: Parfait, planifions une rÃ©union de rÃ©vision.\", \n",
        "                \"expected_classification\": \"meeting\",\n",
        "                \"expected_summary\": \"L'Ã©quipe discute de l'Ã©chÃ©ance budgÃ©taire et planifie une rÃ©union de rÃ©vision jeudi.\",\n",
        "                \"expected_intent\": \"Planifier une rÃ©union de rÃ©vision budgÃ©taire\"\n",
        "            },\n",
        "            \"ar\": {\n",
        "                \"dialogue\": \"Ø£: Ù†Ø­ØªØ§Ø¬ Ù„Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ© ÙŠÙˆÙ… Ø§Ù„Ø¬Ù…Ø¹Ø©. Ø¨: Ø³Ø£Ø¬Ù‡Ø² Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙŠÙˆÙ… Ø§Ù„Ø®Ù…ÙŠØ³. Ø£: Ù…Ù…ØªØ§Ø²ØŒ Ù„Ù†Ø­Ø¯Ø¯ Ù…ÙˆØ¹Ø¯ Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ø±Ø§Ø¬Ø¹Ø©.\",\n",
        "                \"expected_classification\": \"meeting\", \n",
        "                \"expected_summary\": \"ÙŠÙ†Ø§Ù‚Ø´ Ø§Ù„ÙØ±ÙŠÙ‚ Ù…ÙˆØ¹Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆÙŠØ­Ø¯Ø¯ Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ø±Ø§Ø¬Ø¹Ø© Ù„Ù„Ø£Ø±Ù‚Ø§Ù… ÙŠÙˆÙ… Ø§Ù„Ø®Ù…ÙŠØ³.\",\n",
        "                \"expected_intent\": \"Ø¬Ø¯ÙˆÙ„Ø© Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©\"\n",
        "            }\n",
        "            # ðŸŽ¯ ADD YOUR LANGUAGE HERE following the same structure\n",
        "        }\n",
        "    })\n",
        "    \n",
        "    # Test Case 2: Technical Support Scenario  \n",
        "    test_cases.append({\n",
        "        \"case_id\": \"tech_support_01\",\n",
        "        \"domain\": \"support\",\n",
        "        \"complexity\": \"low\", \n",
        "        \"cultural_context\": \"service\",\n",
        "        \"languages\": {\n",
        "            \"en\": {\n",
        "                \"dialogue\": \"A: My computer won't start this morning. B: Did you try unplugging it for 30 seconds? A: Yes, but still nothing. B: Let me schedule a technician visit.\",\n",
        "                \"expected_classification\": \"support\",\n",
        "                \"expected_summary\": \"Customer reports computer startup issue, basic troubleshooting attempted, technician visit scheduled.\",\n",
        "                \"expected_intent\": \"Schedule technician visit for computer repair\"\n",
        "            },\n",
        "            \"fr\": {\n",
        "                \"dialogue\": \"A: Mon ordinateur ne dÃ©marre pas ce matin. B: Avez-vous essayÃ© de le dÃ©brancher 30 secondes? A: Oui, mais toujours rien. B: Laissez-moi programmer une visite de technicien.\",\n",
        "                \"expected_classification\": \"support\", \n",
        "                \"expected_summary\": \"Le client signale un problÃ¨me de dÃ©marrage d'ordinateur, dÃ©pannage de base tentÃ©, visite de technicien programmÃ©e.\",\n",
        "                \"expected_intent\": \"Programmer une visite de technicien pour rÃ©paration d'ordinateur\"\n",
        "            },\n",
        "            \"ar\": {\n",
        "                \"dialogue\": \"Ø£: Ø¬Ù‡Ø§Ø² Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ù„Ø§ ÙŠØ¹Ù…Ù„ Ù‡Ø°Ø§ Ø§Ù„ØµØ¨Ø§Ø­. Ø¨: Ù‡Ù„ Ø¬Ø±Ø¨Øª ÙØµÙ„Ù‡ Ù„Ù…Ø¯Ø© 30 Ø«Ø§Ù†ÙŠØ©ØŸ Ø£: Ù†Ø¹Ù…ØŒ Ù„ÙƒÙ† Ù„Ø§ Ø´ÙŠØ¡. Ø¨: Ø¯Ø¹Ù†ÙŠ Ø£Ø­Ø¯Ø¯ Ø²ÙŠØ§Ø±Ø© ÙÙ†ÙŠ.\",\n",
        "                \"expected_classification\": \"support\",\n",
        "                \"expected_summary\": \"ÙŠØ¨Ù„Øº Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¹Ù† Ù…Ø´ÙƒÙ„Ø© Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ ØªÙ… Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©ØŒ ØªÙ… Ø¬Ø¯ÙˆÙ„Ø© Ø²ÙŠØ§Ø±Ø© ÙÙ†ÙŠ.\", \n",
        "                \"expected_intent\": \"Ø¬Ø¯ÙˆÙ„Ø© Ø²ÙŠØ§Ø±Ø© ÙÙ†ÙŠ Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±\"\n",
        "            }\n",
        "        }\n",
        "    })\n",
        "    \n",
        "    # Test Case 3: Social Conversation Scenario\n",
        "    test_cases.append({\n",
        "        \"case_id\": \"social_conversation_01\", \n",
        "        \"domain\": \"social\",\n",
        "        \"complexity\": \"low\",\n",
        "        \"cultural_context\": \"informal\",\n",
        "        \"languages\": {\n",
        "            \"en\": {\n",
        "                \"dialogue\": \"A: How was your weekend hiking trip? B: Amazing! The weather was perfect and the views were incredible. A: I'd love to join you next time!\",\n",
        "                \"expected_classification\": \"social\",\n",
        "                \"expected_summary\": \"Friends discuss successful weekend hiking trip with great weather and views, plan future trip together.\",\n",
        "                \"expected_intent\": \"Plan future hiking trip together\"\n",
        "            },\n",
        "            \"fr\": {\n",
        "                \"dialogue\": \"A: Comment s'est passÃ©e ta randonnÃ©e du week-end? B: Fantastique! Le temps Ã©tait parfait et les vues incroyables. A: J'aimerais vous accompagner la prochaine fois!\",\n",
        "                \"expected_classification\": \"social\",\n",
        "                \"expected_summary\": \"Les amis discutent d'une randonnÃ©e rÃ©ussie du week-end avec beau temps et vues, planifient un voyage futur ensemble.\",\n",
        "                \"expected_intent\": \"Planifier une future randonnÃ©e ensemble\"\n",
        "            },\n",
        "            \"ar\": {\n",
        "                \"dialogue\": \"Ø£: ÙƒÙŠÙ ÙƒØ§Ù†Øª Ø±Ø­Ù„Ø© Ø§Ù„Ù…Ø´ÙŠ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ØŸ Ø¨: Ø±Ø§Ø¦Ø¹Ø©! ÙƒØ§Ù† Ø§Ù„Ø·Ù‚Ø³ Ù…Ø«Ø§Ù„ÙŠØ§Ù‹ ÙˆØ§Ù„Ù…Ù†Ø§Ø¸Ø± Ù„Ø§ ØªØµØ¯Ù‚. Ø£: Ø£ÙˆØ¯ Ø§Ù„Ø§Ù†Ø¶Ù…Ø§Ù… Ø¥Ù„ÙŠÙƒÙ… Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©!\",\n",
        "                \"expected_classification\": \"social\",\n",
        "                \"expected_summary\": \"ÙŠÙ†Ø§Ù‚Ø´ Ø§Ù„Ø£ØµØ¯Ù‚Ø§Ø¡ Ø±Ø­Ù„Ø© Ø§Ù„Ù…Ø´ÙŠ Ø§Ù„Ù†Ø§Ø¬Ø­Ø© ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ Ù…Ø¹ Ø·Ù‚Ø³ Ø±Ø§Ø¦Ø¹ ÙˆÙ…Ù†Ø§Ø¸Ø±ØŒ ÙŠØ®Ø·Ø·ÙˆÙ† Ù„Ø±Ø­Ù„Ø© Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ù…Ø¹Ø§Ù‹.\",\n",
        "                \"expected_intent\": \"Ø§Ù„ØªØ®Ø·ÙŠØ· Ù„Ø±Ø­Ù„Ø© Ù…Ø´ÙŠ Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ù…Ø¹Ø§Ù‹\"\n",
        "            }\n",
        "        }\n",
        "    })\n",
        "    \n",
        "    return test_cases\n",
        "\n",
        "# Create systematic test data\n",
        "systematic_test_data = create_systematic_test_data()\n",
        "\n",
        "# Convert to structured format for systematic analysis\n",
        "test_items = []\n",
        "for case in systematic_test_data:\n",
        "    for lang_code, lang_data in case[\"languages\"].items():\n",
        "        # Create test items for each task type\n",
        "        for task in tasks_df[\"task_id\"].values:\n",
        "            expected_output = \"\"\n",
        "            if task == \"dialogue_classification\":\n",
        "                expected_output = lang_data[\"expected_classification\"]\n",
        "            elif task == \"dialogue_summarization\": \n",
        "                expected_output = lang_data[\"expected_summary\"]\n",
        "            elif task == \"intent_extraction\":\n",
        "                expected_output = lang_data[\"expected_intent\"]\n",
        "            \n",
        "            test_items.append({\n",
        "                \"item_id\": f\"{case['case_id']}_{lang_code}_{task}\",\n",
        "                \"case_id\": case[\"case_id\"],\n",
        "                \"language_code\": lang_code,\n",
        "                \"task_id\": task,\n",
        "                \"domain\": case[\"domain\"],\n",
        "                \"complexity\": case[\"complexity\"], \n",
        "                \"cultural_context\": case[\"cultural_context\"],\n",
        "                \"input_text\": lang_data[\"dialogue\"],\n",
        "                \"expected_output\": expected_output,\n",
        "                \"language_name\": languages_df[languages_df[\"code\"] == lang_code][\"name\"].iloc[0]\n",
        "            })\n",
        "\n",
        "# Convert to DataFrame for systematic analysis\n",
        "test_items_df = pd.DataFrame(test_items)\n",
        "\n",
        "print(\"ðŸ“Š SYSTEMATIC TEST DATA FRAMEWORK\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\\\nðŸŽ¯ Test Coverage:\")\n",
        "print(f\"   Test cases: {len(systematic_test_data)}\")\n",
        "print(f\"   Languages per case: {len(systematic_test_data[0]['languages'])}\")\n",
        "print(f\"   Tasks per language: {len(tasks_df)}\")\n",
        "print(f\"   Total test items: {len(test_items_df)}\")\n",
        "\n",
        "print(f\"\\\\nðŸ“‹ Test Distribution:\")\n",
        "test_summary = test_items_df.groupby([\"language_name\", \"task_id\"]).size().unstack(fill_value=0)\n",
        "display(test_summary)\n",
        "\n",
        "print(f\"\\\\nâœ… Systematic test framework ready!\")\n",
        "print(f\"   All languages have parallel test cases for fair comparison\")\n",
        "print(f\"   Multiple domains and complexity levels covered\")\n",
        "print(f\"   Cultural contexts systematically varied\")\n",
        "\n",
        "# Show sample test items\n",
        "print(f\"\\\\nðŸ” Sample Test Items (first 3):\")\n",
        "display(test_items_df[[\"item_id\", \"language_name\", \"task_id\", \"domain\", \"complexity\"]].head(6))\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c115c684"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7c81dfb5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ðŸ¤– Systematic Model Loading and Prompt Engineering Framework\n",
        "\n",
        "**Strategic Mission:** Systematically evaluate multiple model access patterns and prompt strategies across languages.\n",
        "\n",
        "### 2.1 ðŸ”¥ Advanced Model Loading with Performance Tracking\n",
        "\n",
        "**Systematic Model Evaluation:** Compare local, API, and hosted approaches with quantitative metrics.\n",
        "\n",
        "| **Access Pattern** | **Pros** | **Cons** | **Cost Analysis** | **Performance Expectation** |\n",
        "|-------------------|----------|----------|-------------------|----------------------------|\n",
        "| **ðŸ”¥ Local mT5-Base** | Privacy, offline, fine-tunable | 2-4GB RAM, setup time | Hardware only (~$500-2000) | Good multilingual, customizable |\n",
        "| **â˜ï¸ GPT-4 API** | State-of-art, minimal setup | $0.01-0.03/1K tokens | ~$50-200/month typical use | Excellent English, good multilingual |\n",
        "| **ðŸŒ Hosted Free (Colab)** | Zero cost, easy access | 12hr limits, GPU competition | Free (with limits) | Variable quality, good for learning |\n",
        "| **ðŸ¢ Claude API** | Strong reasoning, safety | Limited multilingual support | Similar to GPT-4 | Excellent reasoning, English-focused |\n",
        "\n",
        "### 2.2 ðŸ“Š Systematic Task Framework\n",
        "\n",
        "**Our systematic evaluation covers:**\n",
        "1. **ðŸ“Š Dialogue Classification:** Business/social/support categorization with cultural context\n",
        "2. **âœï¸ Dialogue Summarization:** Concise, culturally-appropriate summaries\n",
        "3. **ðŸŽ¯ Intent Extraction:** Action items and next steps identification\n",
        "4. **ðŸ§  Chain-of-Thought Reasoning:** Step-by-step multilingual reasoning evaluation\n"
      ],
      "id": "4800a6d2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸš€ STEP 1: Create the Model Manager Class\n",
        "# This creates a system to handle different types of AI models\n",
        "\n",
        "class SystematicModelManager:\n",
        "    \"\"\"\n",
        "    Advanced model management with systematic evaluation and performance tracking\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.performance_metrics = []\n",
        "        self.current_model = None\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \"\"\"Load local multilingual model with systematic tracking\"\"\"\n",
        "        print(f\"ðŸ”„ Loading local model: {model_name}\")\n",
        "        print(f\"â±ï¸  This may take 2-3 minutes on first run...\")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "            model = model.to(self.device)\n",
        "            \n",
        "            load_time = time.time() - start_time\n",
        "            param_count = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "            \n",
        "            # Store model configuration\n",
        "            model_config = {\n",
        "                \"name\": model_name,\n",
        "                \"type\": \"local_multilingual\", \n",
        "                \"tokenizer\": tokenizer,\n",
        "                \"model\": model,\n",
        "                \"device\": str(self.device),\n",
        "                \"parameters_M\": param_count,\n",
        "                \"load_time_s\": load_time,\n",
        "                \"memory_gb\": torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
        "            }\n",
        "            \n",
        "            self.models[model_name] = model_config\n",
        "            self.current_model = model_name\n",
        "            \n",
        "            print(f\"âœ… Model loaded successfully:\")\n",
        "            print(f\"   Device: {self.device}\")\n",
        "            print(f\"   Parameters: {param_count:.0f}M\")\n",
        "            print(f\"   Load time: {load_time:.1f}s\")\n",
        "            if torch.cuda.is_available():\n",
        "                print(f\"   GPU memory: {model_config['memory_gb']:.1f}GB\")\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading {model_name}: {str(e)}\")\n",
        "            print(\"ðŸ’¡ Solutions:\")\n",
        "            print(\"   1. Try smaller model: google/mt5-small\")\n",
        "            print(\"   2. Restart runtime to free memory\")\n",
        "            print(\"   3. Use CPU-only mode\")\n",
        "            return False\n",
        "    \n",
        "    def setup_api_access(self, api_type: str = \"placeholder\"):\n",
        "        \"\"\"Setup API access (placeholder for actual API integration)\"\"\"\n",
        "        print(f\"ðŸŒ Setting up {api_type} API access...\")\n",
        "        \n",
        "        # Placeholder for API setup - in real use, add API key configuration\n",
        "        api_config = {\n",
        "            \"name\": f\"{api_type}_api\",\n",
        "            \"type\": \"api_access\",\n",
        "            \"cost_per_1k_tokens\": 0.02 if api_type == \"gpt4\" else 0.01,\n",
        "            \"setup_time_s\": 0.1,\n",
        "            \"requires_internet\": True\n",
        "        }\n",
        "        \n",
        "        self.models[f\"{api_type}_api\"] = api_config\n",
        "        print(f\"âœ… {api_type} API configured (placeholder)\")\n",
        "        print(f\"   Estimated cost: ${api_config['cost_per_1k_tokens']}/1K tokens\")\n",
        "        return True\n",
        "    \n",
        "    def generate_text_systematic(self, prompt: str, max_length: int = 100, \n",
        "                                temperature: float = 0.7, model_name: str = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Generate text with systematic performance tracking\n",
        "        \"\"\"\n",
        "        model_name = model_name or self.current_model\n",
        "        if not model_name or model_name not in self.models:\n",
        "            return {\"error\": \"No model loaded\", \"output\": \"\", \"metrics\": {}}\n",
        "        \n",
        "        model_config = self.models[model_name]\n",
        "        \n",
        "        if model_config[\"type\"] == \"local_multilingual\":\n",
        "            return self._generate_local(prompt, model_config, max_length, temperature)\n",
        "        elif model_config[\"type\"] == \"api_access\":\n",
        "            return self._generate_api(prompt, model_config, max_length, temperature)\n",
        "        else:\n",
        "            return {\"error\": \"Unknown model type\", \"output\": \"\", \"metrics\": {}}\n",
        "    \n",
        "    def _generate_local(self, prompt: str, model_config: Dict, max_length: int, temperature: float) -> Dict:\n",
        "        \"\"\"Generate using local model with performance tracking\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            tokenizer = model_config[\"tokenizer\"]\n",
        "            model = model_config[\"model\"]\n",
        "            \n",
        "            # Tokenization with tracking\n",
        "            inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "            input_tokens = inputs.shape[1]\n",
        "            inputs = inputs.to(self.device)\n",
        "            \n",
        "            # Generation with tracking\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs, \n",
        "                    max_length=max_length, \n",
        "                    temperature=temperature,\n",
        "                    do_sample=True, \n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "            \n",
        "            # Decode output\n",
        "            output_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "            output_tokens = outputs.shape[1] - input_tokens\n",
        "            \n",
        "            generation_time = time.time() - start_time\n",
        "            \n",
        "            # Calculate performance metrics\n",
        "            metrics = {\n",
        "                \"model_name\": model_config[\"name\"],\n",
        "                \"generation_time_ms\": generation_time * 1000,\n",
        "                \"input_tokens\": input_tokens,\n",
        "                \"output_tokens\": output_tokens,\n",
        "                \"tokens_per_second\": output_tokens / generation_time if generation_time > 0 else 0,\n",
        "                \"success\": True\n",
        "            }\n",
        "            \n",
        "            return {\n",
        "                \"output\": output_text,\n",
        "                \"metrics\": metrics,\n",
        "                \"error\": None\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"output\": \"\", \n",
        "                \"metrics\": {\"success\": False, \"error\": str(e)},\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "    \n",
        "    def _generate_api(self, prompt: str, model_config: Dict, max_length: int, temperature: float) -> Dict:\n",
        "        \"\"\"Placeholder for API generation - replace with actual API calls\"\"\"\n",
        "        return {\n",
        "            \"output\": f\"[API placeholder - would call {model_config['name']} with prompt]\",\n",
        "            \"metrics\": {\n",
        "                \"model_name\": model_config[\"name\"],\n",
        "                \"estimated_cost\": len(prompt) * model_config[\"cost_per_1k_tokens\"] / 1000,\n",
        "                \"success\": True\n",
        "            },\n",
        "            \"error\": None\n",
        "        }\n",
        "\n",
        "# Initialize systematic model manager\n",
        "print(\"ðŸš€ SYSTEMATIC MODEL MANAGER INITIALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model_manager = SystematicModelManager()\n",
        "\n",
        "# Option 1: Load local model (recommended for learning)\n",
        "print(\"ðŸ”¥ Loading local multilingual model...\")\n",
        "local_success = model_manager.load_local_model(\"google/mt5-small\")\n",
        "\n",
        "# Option 2: Setup API access (for production comparison)\n",
        "print(\"\\\\nâ˜ï¸ Configuring API access...\")\n",
        "api_success = model_manager.setup_api_access(\"gpt4\")\n",
        "\n",
        "# Test the systematic framework\n",
        "if local_success:\n",
        "    print(\"\\\\nðŸ§ª TESTING SYSTEMATIC GENERATION:\")\n",
        "    test_prompt = \"Classify this dialogue: A: Let's schedule a meeting for 3pm. B: Perfect, I'll send the invite.\"\n",
        "    \n",
        "    result = model_manager.generate_text_systematic(test_prompt, max_length=50, temperature=0.1)\n",
        "    \n",
        "    if result[\"error\"]:\n",
        "        print(f\"âŒ Test failed: {result['error']}\")\n",
        "    else:\n",
        "        print(f\"âœ… Test successful:\")\n",
        "        print(f\"   Output: {result['output']}\")\n",
        "        print(f\"   Generation time: {result['metrics']['generation_time_ms']:.1f}ms\")\n",
        "        print(f\"   Tokens/second: {result['metrics']['tokens_per_second']:.1f}\")\n",
        "\n",
        "print(f\"\\\\nðŸ“Š MODEL MANAGER STATUS:\")\n",
        "print(f\"   Available models: {len(model_manager.models)}\")\n",
        "print(f\"   Current model: {model_manager.current_model}\")\n",
        "print(f\"   Device: {model_manager.device}\")\n",
        "print(f\"   âœ… Ready for systematic prompt engineering experiments!\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b0b01478"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ“š What You Just Did:\n",
        "**You created the foundation of your model manager!** This class will help you:\n",
        "- Keep track of different AI models (local and online)\n",
        "- Measure how fast they work\n",
        "- Switch between models easily\n",
        "\n",
        "**ðŸŽ¯ What to Expect:** You should see no output (this just defines the class)  \n",
        "**âœ… Success Indicator:** No error messages  \n",
        "**âš ï¸ If you see errors:** Check that you ran the setup cell above first"
      ],
      "id": "71d654c3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸš€ STEP 2: Add Model Loading Function\n",
        "# This function downloads and loads an AI model onto your computer/Colab\n",
        "\n",
        "def load_local_model(self, model_name: str = \"google/mt5-small\"):\n",
        "    \"\"\"Load local multilingual model with systematic tracking\"\"\"\n",
        "    print(f\"ðŸ”„ Loading local model: {model_name}\")\n",
        "    print(f\"â±ï¸  This may take 2-3 minutes on first run...\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "        model = model.to(self.device)\n",
        "        \n",
        "        load_time = time.time() - start_time\n",
        "        param_count = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "        \n",
        "        # Store model configuration\n",
        "        model_config = {\n",
        "            \"name\": model_name,\n",
        "            \"type\": \"local_multilingual\", \n",
        "            \"tokenizer\": tokenizer,\n",
        "            \"model\": model,\n",
        "            \"device\": str(self.device),\n",
        "            \"parameters_M\": param_count,\n",
        "            \"load_time_s\": load_time,\n",
        "            \"memory_gb\": torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
        "        }\n",
        "        \n",
        "        self.models[model_name] = model_config\n",
        "        self.current_model = model_name\n",
        "        \n",
        "        print(f\"âœ… Model loaded successfully:\")\n",
        "        print(f\"   Device: {self.device}\")\n",
        "        print(f\"   Parameters: {param_count:.0f}M\")\n",
        "        print(f\"   Load time: {load_time:.1f}s\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"   GPU memory: {model_config['memory_gb']:.1f}GB\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading {model_name}: {str(e)}\")\n",
        "        print(\"ðŸ’¡ Solutions:\")\n",
        "        print(\"   1. Try smaller model: google/mt5-small\")\n",
        "        print(\"   2. Restart runtime to free memory\")\n",
        "        print(\"   3. Use CPU-only mode\")\n",
        "        return False\n",
        "\n",
        "# Add this method to our SystematicModelManager class\n",
        "SystematicModelManager.load_local_model = load_local_model"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a58ecb11"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ“š What You Just Did:\n",
        "**You added a function to load AI models!** This is like installing an app on your phone.\n",
        "\n",
        "**ðŸŽ¯ What to Expect:** \n",
        "- First time: Nothing visible (just adds the function)\n",
        "- When you actually use it later: Downloads ~500MB model file\n",
        "\n",
        "**ðŸ“Š What the Numbers Mean:**\n",
        "- **Parameters**: How \"smart\" the model is (more = smarter but slower)\n",
        "- **Load time**: How long it took to start up\n",
        "- **GPU memory**: How much of your graphics card it's using\n",
        "\n",
        "**âš ï¸ Common Issues:**\n",
        "- \"Out of memory\": The model is too big for your system\n",
        "- \"Connection error\": Internet issue during download\n",
        "- Takes forever: Normal for first download, fast after that"
      ],
      "id": "89612973"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸš€ STEP 3: Add API Setup Function\n",
        "# This function sets up online AI services (like ChatGPT APIs)\n",
        "\n",
        "def setup_api_access(self, api_type: str = \"placeholder\"):\n",
        "    \"\"\"Setup API access (placeholder for actual API integration)\"\"\"\n",
        "    print(f\"ðŸŒ Setting up {api_type} API access...\")\n",
        "    \n",
        "    # Placeholder for API setup - in real use, add API key configuration\n",
        "    api_config = {\n",
        "        \"name\": f\"{api_type}_api\",\n",
        "        \"type\": \"api_access\",\n",
        "        \"cost_per_1k_tokens\": 0.02 if api_type == \"gpt4\" else 0.01,\n",
        "        \"setup_time_s\": 0.1,\n",
        "        \"requires_internet\": True\n",
        "    }\n",
        "    \n",
        "    self.models[f\"{api_type}_api\"] = api_config\n",
        "    print(f\"âœ… {api_type} API configured (placeholder)\")\n",
        "    print(f\"   Estimated cost: ${api_config['cost_per_1k_tokens']}/1K tokens\")\n",
        "    return True\n",
        "\n",
        "# Add this method to our SystematicModelManager class  \n",
        "SystematicModelManager.setup_api_access = setup_api_access"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1522114e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ“š What You Just Did:\n",
        "**You added a function to connect to online AI services!** This is like connecting to ChatGPT, but through code.\n",
        "\n",
        "**ðŸŽ¯ What to Expect:** \n",
        "- Shows \"API configured (placeholder)\" message\n",
        "- This is just a demo - real APIs need keys\n",
        "\n",
        "**ðŸ’° What the Cost Numbers Mean:**\n",
        "- **Cost per 1K tokens**: How much it costs to process ~750 words\n",
        "- Example: $0.02/1K tokens = 2 cents per ~750 words processed\n",
        "\n",
        "**ðŸ”„ Next:** We'll add the actual FREE APIs that work on Colab later"
      ],
      "id": "1ac64d74"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸš€ STEP 4: Add Text Generation Function (Main Function)\n",
        "# This is the function that actually generates text from your prompts\n",
        "\n",
        "def generate_text_systematic(self, prompt: str, max_length: int = 100, \n",
        "                            temperature: float = 0.7, model_name: str = None) -> Dict:\n",
        "    \"\"\"Generate text with systematic performance tracking\"\"\"\n",
        "    model_name = model_name or self.current_model\n",
        "    if not model_name or model_name not in self.models:\n",
        "        return {\"error\": \"No model loaded\", \"output\": \"\", \"metrics\": {}}\n",
        "    \n",
        "    model_config = self.models[model_name]\n",
        "    \n",
        "    if model_config[\"type\"] == \"local_multilingual\":\n",
        "        return self._generate_local(prompt, model_config, max_length, temperature)\n",
        "    elif model_config[\"type\"] == \"api_access\":\n",
        "        return self._generate_api(prompt, model_config, max_length, temperature)\n",
        "    else:\n",
        "        return {\"error\": \"Unknown model type\", \"output\": \"\", \"metrics\": {}}\n",
        "\n",
        "# Add this method to our SystematicModelManager class\n",
        "SystematicModelManager.generate_text_systematic = generate_text_systematic"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c8802bf9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸš€ STEP 5: Add Helper Functions for Text Generation\n",
        "# These functions handle the details of generating text with local models vs APIs\n",
        "\n",
        "def _generate_local(self, prompt: str, model_config: Dict, max_length: int, temperature: float) -> Dict:\n",
        "    \"\"\"Generate using local model with performance tracking\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        tokenizer = model_config[\"tokenizer\"]\n",
        "        model = model_config[\"model\"]\n",
        "        \n",
        "        # Tokenization with tracking\n",
        "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        input_tokens = inputs.shape[1]\n",
        "        inputs = inputs.to(self.device)\n",
        "        \n",
        "        # Generation with tracking\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs, \n",
        "                max_length=max_length, \n",
        "                temperature=temperature,\n",
        "                do_sample=True, \n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode output\n",
        "        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "        output_tokens = outputs.shape[1] - input_tokens\n",
        "        \n",
        "        generation_time = time.time() - start_time\n",
        "        \n",
        "        # Calculate performance metrics\n",
        "        metrics = {\n",
        "            \"model_name\": model_config[\"name\"],\n",
        "            \"generation_time_ms\": generation_time * 1000,\n",
        "            \"input_tokens\": input_tokens,\n",
        "            \"output_tokens\": output_tokens,\n",
        "            \"tokens_per_second\": output_tokens / generation_time if generation_time > 0 else 0,\n",
        "            \"success\": True\n",
        "        }\n",
        "        \n",
        "        return {\n",
        "            \"output\": output_text,\n",
        "            \"metrics\": metrics,\n",
        "            \"error\": None\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"output\": \"\", \n",
        "            \"metrics\": {\"success\": False, \"error\": str(e)},\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "def _generate_api(self, prompt: str, model_config: Dict, max_length: int, temperature: float) -> Dict:\n",
        "    \"\"\"Placeholder for API generation - replace with actual API calls\"\"\"\n",
        "    return {\n",
        "        \"output\": f\"[API placeholder - would call {model_config['name']} with prompt]\",\n",
        "        \"metrics\": {\n",
        "            \"model_name\": model_config[\"name\"],\n",
        "            \"estimated_cost\": len(prompt) * model_config[\"cost_per_1k_tokens\"] / 1000,\n",
        "            \"success\": True\n",
        "        },\n",
        "        \"error\": None\n",
        "    }\n",
        "\n",
        "# Add these methods to our SystematicModelManager class\n",
        "SystematicModelManager._generate_local = _generate_local\n",
        "SystematicModelManager._generate_api = _generate_api"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "46b021f1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ“š What You Just Did:\n",
        "**You added the \"engine\" that makes AI models work!** This is like the brain of your AI system.\n",
        "\n",
        "**ðŸŽ¯ What Each Function Does:**\n",
        "- **`generate_text_systematic`**: The main function you'll call to get AI responses\n",
        "- **`_generate_local`**: Handles local models (like mT5 on your computer)  \n",
        "- **`_generate_api`**: Handles online services (like ChatGPT APIs)\n",
        "\n",
        "**ðŸ“Š What the Performance Metrics Mean:**\n",
        "- **Generation time**: How long the AI took to respond\n",
        "- **Input/Output tokens**: How many \"words\" went in and came out\n",
        "- **Tokens per second**: How fast the AI is working\n",
        "\n",
        "**âš ï¸ These are helper functions - you won't call them directly!**"
      ],
      "id": "40ea8218"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸš€ STEP 6: Initialize and Test Your Model Manager\n",
        "# Now let's create your model manager and load a model!\n",
        "\n",
        "print(\"ðŸš€ SYSTEMATIC MODEL MANAGER INITIALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create your model manager\n",
        "model_manager = SystematicModelManager()\n",
        "print(\"âœ… Model manager created!\")\n",
        "print(f\"   Device available: {model_manager.device}\")\n",
        "\n",
        "# Try to load a local model\n",
        "print(\"\\nðŸ”¥ Loading local multilingual model...\")\n",
        "local_success = model_manager.load_local_model(\"google/mt5-small\")\n",
        "\n",
        "if local_success:\n",
        "    print(f\"\\nðŸ“Š MODEL MANAGER STATUS:\")\n",
        "    print(f\"   Available models: {len(model_manager.models)}\")\n",
        "    print(f\"   Current model: {model_manager.current_model}\")\n",
        "    print(f\"   Device: {model_manager.device}\")\n",
        "    print(f\"   âœ… Ready for experiments!\")\n",
        "else:\n",
        "    print(\"âš ï¸  Local model failed to load - we'll setup APIs next\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8a26b0fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ“š What You Should See:\n",
        "**This cell starts up your AI system!** Here's what to expect:\n",
        "\n",
        "**âœ… Success Indicators:**\n",
        "- \"Model manager created!\" message\n",
        "- \"Device available: cuda\" (on Colab GPU) or \"cpu\" \n",
        "- Loading progress for the mT5 model (2-3 minutes first time)\n",
        "- \"Model loaded successfully\" with statistics\n",
        "- \"Ready for experiments!\" at the end\n",
        "\n",
        "**â±ï¸ Expected Timeline:**\n",
        "- First run: 2-3 minutes (downloads ~500MB model)\n",
        "- Subsequent runs: 10-30 seconds (model already cached)\n",
        "\n",
        "**âŒ If You See Errors:**\n",
        "- \"CUDA out of memory\": Model too big, restart runtime\n",
        "- \"Connection error\": Internet issue, try again\n",
        "- \"Import error\": Run the setup cells above first\n",
        "\n",
        "**ðŸ“Š What the Statistics Mean:**\n",
        "- **Parameters**: 300M = small model, 3B = large model\n",
        "- **GPU Memory**: How much graphics card memory it's using\n",
        "- **Load time**: Normal range is 10-180 seconds"
      ],
      "id": "3c03af92"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸ§ª STEP 7: Test Your AI Model!\n",
        "# Let's see if your model can understand and respond to text\n",
        "\n",
        "if 'model_manager' in locals() and model_manager.current_model:\n",
        "    print(\"ðŸ§ª TESTING YOUR AI MODEL\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Simple test prompt\n",
        "    test_prompt = \"Hello! Can you help me classify this conversation: A: Let's meet at 3pm. B: Perfect, see you then.\"\n",
        "    \n",
        "    print(f\"ðŸŽ¯ Test prompt: {test_prompt}\")\n",
        "    print(\"\\nâ±ï¸  Generating response...\")\n",
        "    \n",
        "    # Generate response\n",
        "    result = model_manager.generate_text_systematic(test_prompt, max_length=50, temperature=0.1)\n",
        "    \n",
        "    if result[\"error\"]:\n",
        "        print(f\"âŒ Test failed: {result['error']}\")\n",
        "        print(\"ðŸ’¡ Try restarting the runtime and running all cells again\")\n",
        "    else:\n",
        "        print(f\"âœ… SUCCESS! Your AI responded:\")\n",
        "        print(f\"ðŸ“ Output: {result['output']}\")\n",
        "        print(f\"\\nðŸ“Š Performance:\")\n",
        "        print(f\"   Generation time: {result['metrics']['generation_time_ms']:.1f}ms\")\n",
        "        print(f\"   Tokens per second: {result['metrics']['tokens_per_second']:.1f}\")\n",
        "        print(f\"\\nðŸŽ‰ Your AI model is working perfectly!\")\n",
        "        \n",
        "else:\n",
        "    print(\"âš ï¸  Model not loaded yet. Run the cell above first!\")\n",
        "    print(\"ðŸ”„ If it failed, try restarting runtime and running all cells\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d4115737"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ“š What You Should See:\n",
        "**This cell tests that your AI is actually working!** \n",
        "\n",
        "**âœ… Success Indicators:**\n",
        "- \"SUCCESS! Your AI responded:\" message\n",
        "- Some text output (might be weird/incomplete - that's normal!)\n",
        "- Performance metrics showing generation time and speed\n",
        "- \"Your AI model is working perfectly!\" message\n",
        "\n",
        "**ðŸŽ¯ What the Output Means:**\n",
        "- **The AI response**: Might be random/incomplete (mT5 needs fine-tuning for good results)\n",
        "- **Generation time**: How fast your AI is (faster = better)  \n",
        "- **Tokens per second**: Processing speed (10-100 is normal range)\n",
        "\n",
        "**ðŸš€ What's Next:**\n",
        "- Don't worry if the response seems random\n",
        "- We'll improve it with proper prompting techniques\n",
        "- The important thing is that it's working without errors!\n",
        "\n",
        "**âŒ If It Doesn't Work:**\n",
        "- Check that all previous cells ran without errors\n",
        "- Try restarting runtime (Runtime â†’ Restart Runtime)"
      ],
      "id": "23081187"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "### 2.1 ðŸŽ¯ Zero-shot, Few-shot, and Chain-of-Thought Comparison\n",
        "\n",
        "# ðŸ”§ Prompt engineering toolkit\n",
        "def create_zero_shot_prompt(dialogue: str, task: str = \"classification\") -> str:\n",
        "    \"\"\"Zero-shot prompt - no examples provided\"\"\"\n",
        "    if task == \"classification\":\n",
        "        return f\"\"\"Classify this dialogue into one topic: meeting, social, support, transaction, other.\n",
        "\n",
        "Dialogue: {dialogue}\n",
        "\n",
        "Topic:\"\"\"\n",
        "    else:  # QA\n",
        "        return f\"\"\"Answer the question based on the context.\n",
        "\n",
        "Context: {dialogue}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "def create_few_shot_prompt(dialogue: str, examples: list, task: str = \"classification\") -> str:\n",
        "    \"\"\"Few-shot prompt - includes examples\"\"\"\n",
        "    if task == \"classification\":\n",
        "        prompt = \"Classify dialogues into topics: meeting, social, support, transaction, other.\\n\\nExamples:\\n\\n\"\n",
        "        for ex in examples[:2]:  # Use 2 examples to avoid length issues\n",
        "            prompt += f\"Dialogue: {ex['dialogue']}\\nTopic: {ex['topic']}\\n\\n\"\n",
        "        prompt += f\"Dialogue: {dialogue}\\nTopic:\"\n",
        "        return prompt\n",
        "    else:  # QA\n",
        "        return f\"\"\"Answer questions based on context.\n",
        "\n",
        "Context: {dialogue}\n",
        "\n",
        "Answer with specific information:\"\"\"\n",
        "\n",
        "def create_chain_of_thought_prompt(context: str, question: str) -> str:\n",
        "    \"\"\"Chain-of-Thought prompt for step-by-step reasoning\"\"\"\n",
        "    return f\"\"\"Answer the question step by step based on the context.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Let me think step by step:\n",
        "1. What is the question asking?\n",
        "2. What relevant information is in the context?\n",
        "3. What is the answer?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "# ðŸ§ª Run comprehensive prompt testing\n",
        "def test_all_prompting_strategies():\n",
        "    \"\"\"Test zero-shot, few-shot, and Chain-of-Thought across languages\"\"\"\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    print(\"ðŸŽ¯ COMPREHENSIVE PROMPT ENGINEERING TEST\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for language, data in test_data.items():\n",
        "        print(f\"\\nðŸŒ TESTING: {language.upper()}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Test classification\n",
        "        if data[\"classification\"]:\n",
        "            test_dialogue = data[\"classification\"][0][\"dialogue\"]\n",
        "            true_topic = data[\"classification\"][0][\"topic\"]\n",
        "            \n",
        "            print(f\"ðŸ“ Classification task: {test_dialogue[:60]}...\")\n",
        "            print(f\"ðŸ“‹ Expected: {true_topic}\")\n",
        "            \n",
        "            # Zero-shot classification\n",
        "            zero_prompt = create_zero_shot_prompt(test_dialogue, \"classification\")\n",
        "            if model:\n",
        "                zero_result = generate_text(zero_prompt, max_length=50, temperature=0.1)\n",
        "                print(f\"ðŸŽ¯ Zero-shot: {zero_result}\")\n",
        "                \n",
        "                # Few-shot classification (using English examples for transfer)\n",
        "                few_prompt = create_few_shot_prompt(test_dialogue, test_data[\"English\"][\"classification\"], \"classification\")\n",
        "                few_result = generate_text(few_prompt, max_length=50, temperature=0.1)\n",
        "                print(f\"ðŸ“š Few-shot: {few_result}\")\n",
        "                \n",
        "                results.append({\n",
        "                    \"language\": language, \"task\": \"classification\", \"method\": \"zero-shot\",\n",
        "                    \"input\": test_dialogue[:50] + \"...\", \"output\": zero_result, \"expected\": true_topic\n",
        "                })\n",
        "                results.append({\n",
        "                    \"language\": language, \"task\": \"classification\", \"method\": \"few-shot\", \n",
        "                    \"input\": test_dialogue[:50] + \"...\", \"output\": few_result, \"expected\": true_topic\n",
        "                })\n",
        "            \n",
        "        # Test QA with Chain-of-Thought\n",
        "        if data[\"qa\"][\"questions\"]:\n",
        "            context = data[\"qa\"][\"context\"]\n",
        "            question = data[\"qa\"][\"questions\"][0]\n",
        "            expected_answer = data[\"qa\"][\"answers\"][0]\n",
        "            \n",
        "            print(f\"\\\\nâ“ QA task: {question}\")\n",
        "            print(f\"ðŸ“‹ Expected: {expected_answer}\")\n",
        "            \n",
        "            # Chain-of-Thought QA\n",
        "            cot_prompt = create_chain_of_thought_prompt(context, question)\n",
        "            if model:\n",
        "                cot_result = generate_text(cot_prompt, max_length=120, temperature=0.2)\n",
        "                print(f\"ðŸ§  Chain-of-Thought: {cot_result}\")\n",
        "                \n",
        "                results.append({\n",
        "                    \"language\": language, \"task\": \"qa\", \"method\": \"chain-of-thought\",\n",
        "                    \"input\": question, \"output\": cot_result, \"expected\": expected_answer\n",
        "                })\n",
        "        \n",
        "        print()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run the comprehensive test\n",
        "if model:\n",
        "    test_results = test_all_prompting_strategies()\n",
        "    print(f\"âœ… Completed testing across {len(test_data)} languages\")\n",
        "else:\n",
        "    print(\"âš ï¸  Model not available - showing prompt structure only\")\n",
        "    # Show example prompts\n",
        "    example_dialogue = \"A: Can we meet at 3pm? B: Perfect!\"\n",
        "    print(\"\\\\nðŸ“ EXAMPLE PROMPTS:\")\n",
        "    print(\"\\\\nðŸŽ¯ Zero-shot:\")\n",
        "    print(create_zero_shot_prompt(example_dialogue))\n",
        "    print(\"\\\\nðŸ“š Few-shot structure:\")\n",
        "    print(create_few_shot_prompt(example_dialogue, [{\"dialogue\": \"Example\", \"topic\": \"meeting\"}])[:200] + \"...\")\n",
        "\n",
        "### 2.2 ðŸ“Š Evaluation Framework\n",
        "\n",
        "def create_evaluation_rubric():\n",
        "    \"\"\"Evaluation framework for model outputs\"\"\"\n",
        "    return {\n",
        "        \"correctness\": {\n",
        "            \"1\": \"Completely wrong\", \"2\": \"Partially wrong\", \"3\": \"Mostly right\", \n",
        "            \"4\": \"Right answer\", \"5\": \"Perfect with reasoning\"\n",
        "        },\n",
        "        \"fluency\": {\n",
        "            \"1\": \"Unnatural/errors\", \"2\": \"Awkward phrasing\", \"3\": \"Acceptable\", \n",
        "            \"4\": \"Good language\", \"5\": \"Native-like\"\n",
        "        },\n",
        "        \"cultural_appropriateness\": {\n",
        "            \"1\": \"Inappropriate\", \"2\": \"Questionable\", \"3\": \"Neutral\", \n",
        "            \"4\": \"Appropriate\", \"5\": \"Culturally aware\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "def evaluate_output(output: str, expected: str, language: str, task: str, method: str):\n",
        "    \"\"\"Template for manual evaluation\"\"\"\n",
        "    return {\n",
        "        \"output\": output,\n",
        "        \"expected\": expected,\n",
        "        \"language\": language,\n",
        "        \"task\": task,\n",
        "        \"method\": method,\n",
        "        \"correctness_score\": 0,  # Fill in 1-5\n",
        "        \"fluency_score\": 0,      # Fill in 1-5\n",
        "        \"cultural_score\": 0,     # Fill in 1-5\n",
        "        \"notes\": \"\",            # Your observations\n",
        "        \"improvement_suggestions\": \"\"\n",
        "    }\n",
        "\n",
        "print(\"\\\\nðŸ“‹ EVALUATION FRAMEWORK\")\n",
        "print(\"=\"*40)\n",
        "rubric = create_evaluation_rubric()\n",
        "for dimension, scale in rubric.items():\n",
        "    print(f\"\\\\n{dimension.upper()}:\")\n",
        "    for score, description in scale.items():\n",
        "        print(f\"  {score}: {description}\")\n",
        "\n",
        "print(f\"\\\\nðŸŽ¯ YOUR TURN: Evaluate the outputs above using this 1-5 scale\")\n",
        "print(\"ðŸ’¡ Focus on how well each method works for your target language\")\n",
        "\n",
        "### 2.3 ðŸ’¬ Discussion Questions and Key Takeaways\n",
        "\n",
        "discussion_guide = \"\"\"\n",
        "ðŸ¤” REFLECTION QUESTIONS:\n",
        "\n",
        "1. **Cross-language Performance:**\n",
        "   - Which prompting method worked best for your target language?\n",
        "   - How did performance differ between English and your language?\n",
        "\n",
        "2. **Method Comparison:** \n",
        "   - When did few-shot examples help vs. hurt?\n",
        "   - How effective was Chain-of-Thought reasoning in non-English?\n",
        "\n",
        "3. **Cultural Considerations:**\n",
        "   - What cultural assumptions did you notice in outputs?\n",
        "   - How would you adapt prompts for your cultural context?\n",
        "\n",
        "4. **Practical Applications:**\n",
        "   - Which approach would you use in production?\n",
        "   - What are the trade-offs between methods?\n",
        "\n",
        "ðŸ“ ACTION ITEMS:\n",
        "â–¡ Document 3 key insights about your target language\n",
        "â–¡ Identify best prompting strategies for your use case  \n",
        "â–¡ Note major challenges needing further research\n",
        "â–¡ Plan next steps for your project\n",
        "\n",
        "ðŸŽ¯ KEY TAKEAWAYS:\n",
        "â€¢ Prompt structure matters more than complexity\n",
        "â€¢ Cultural context significantly impacts performance  \n",
        "â€¢ Few-shot examples can bridge language gaps effectively\n",
        "â€¢ Chain-of-Thought helps with reasoning across languages\n",
        "â€¢ Evaluation must consider cultural appropriateness\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\\\n\" + discussion_guide)\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ CONGRATULATIONS!\")\n",
        "print(\"You've completed hands-on prompt engineering for low-resource languages!\")\n",
        "print(\"Use these techniques responsibly and keep experimenting! ðŸš€\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "291438cb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 ðŸ†“ FREE API Setup for Colab Students\n",
        "\n",
        "**ðŸŽ¯ Student-Friendly Free Options:**\n",
        "\n",
        "| **Service** | **Free Tier** | **Models Available** | **Setup Difficulty** | **Recommended For** |\n",
        "|-------------|---------------|---------------------|---------------------|-------------------|\n",
        "| **ðŸ¤— Hugging Face** | 1000 requests/month | Llama-2, CodeLlama, Mistral | Easy | Learning & experiments |\n",
        "| **ðŸŸ¢ Google Gemini** | 60 requests/minute | Gemini-1.5-flash, Gemini-pro | Easy | High-quality outputs |\n",
        "| **ðŸ”¥ Local mT5** | GPU memory only | mT5-small/base | Medium | Privacy & customization |\n",
        "\n",
        "**âœ… All options work perfectly on Google Colab GPU!**"
      ],
      "id": "903ae0af"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸ†“ FREE API SETUP FOR COLAB STUDENTS\n",
        "# Choose your preferred free option!\n",
        "\n",
        "import requests\n",
        "import os\n",
        "from typing import Dict, List\n",
        "\n",
        "class FreeAPIManager:\n",
        "    \"\"\"Manage multiple free API services for students\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.available_services = {}\n",
        "        \n",
        "    def setup_huggingface_free(self, hf_token: str = None):\n",
        "        \"\"\"Setup Hugging Face Inference API (1000 free requests/month)\"\"\"\n",
        "        if not hf_token:\n",
        "            print(\"ðŸ”‘ Get your FREE Hugging Face token:\")\n",
        "            print(\"   1. Visit: https://huggingface.co/settings/tokens\")\n",
        "            print(\"   2. Create a token with 'Read' permission\")\n",
        "            print(\"   3. Paste it when prompted\")\n",
        "            hf_token = input(\"Enter your HF token: \").strip()\n",
        "        \n",
        "        self.available_services[\"huggingface\"] = {\n",
        "            \"token\": hf_token,\n",
        "            \"api_url\": \"https://api-inference.huggingface.co/models/\",\n",
        "            \"models\": [\"microsoft/DialoGPT-medium\", \"google/flan-t5-base\", \"meta-llama/Llama-2-7b-chat-hf\"],\n",
        "            \"cost\": \"Free (1000 requests/month)\",\n",
        "            \"setup\": True\n",
        "        }\n",
        "        print(\"âœ… Hugging Face API configured!\")\n",
        "        return True\n",
        "    \n",
        "    def setup_gemini_free(self, api_key: str = None):\n",
        "        \"\"\"Setup Google Gemini API (60 free requests/minute)\"\"\"\n",
        "        if not api_key:\n",
        "            print(\"ðŸ”‘ Get your FREE Google AI Studio API key:\")\n",
        "            print(\"   1. Visit: https://makersuite.google.com/app/apikey\")\n",
        "            print(\"   2. Create a new API key\")\n",
        "            print(\"   3. Paste it when prompted\")\n",
        "            api_key = input(\"Enter your Gemini API key: \").strip()\n",
        "        \n",
        "        self.available_services[\"gemini\"] = {\n",
        "            \"api_key\": api_key,\n",
        "            \"models\": [\"gemini-1.5-flash\", \"gemini-1.5-pro\"],\n",
        "            \"cost\": \"Free (60 requests/minute)\",\n",
        "            \"setup\": True\n",
        "        }\n",
        "        print(\"âœ… Google Gemini API configured!\")\n",
        "        return True\n",
        "    \n",
        "    def query_huggingface(self, model_name: str, prompt: str, max_length: int = 100) -> Dict:\n",
        "        \"\"\"Query Hugging Face model with free API\"\"\"\n",
        "        if \"huggingface\" not in self.available_services:\n",
        "            return {\"error\": \"Hugging Face not setup. Run setup_huggingface_free() first\"}\n",
        "        \n",
        "        api_url = self.available_services[\"huggingface\"][\"api_url\"] + model_name\n",
        "        headers = {\"Authorization\": f\"Bearer {self.available_services['huggingface']['token']}\"}\n",
        "        \n",
        "        payload = {\n",
        "            \"inputs\": prompt,\n",
        "            \"parameters\": {\"max_length\": max_length, \"temperature\": 0.7}\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = requests.post(api_url, headers=headers, json=payload)\n",
        "            result = response.json()\n",
        "            \n",
        "            if isinstance(result, list) and len(result) > 0:\n",
        "                return {\n",
        "                    \"output\": result[0].get(\"generated_text\", \"\").replace(prompt, \"\").strip(),\n",
        "                    \"model\": model_name,\n",
        "                    \"service\": \"huggingface\",\n",
        "                    \"success\": True\n",
        "                }\n",
        "            else:\n",
        "                return {\"error\": f\"Unexpected response: {result}\", \"success\": False}\n",
        "                \n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"success\": False}\n",
        "    \n",
        "    def query_gemini(self, prompt: str, model: str = \"gemini-1.5-flash\") -> Dict:\n",
        "        \"\"\"Query Gemini with free API (placeholder - would need google-generativeai package)\"\"\"\n",
        "        if \"gemini\" not in self.available_services:\n",
        "            return {\"error\": \"Gemini not setup. Run setup_gemini_free() first\"}\n",
        "        \n",
        "        # This would require: pip install google-generativeai\n",
        "        # For now, showing the structure students would use\n",
        "        return {\n",
        "            \"output\": f\"[Gemini response - install google-generativeai package to use]\",\n",
        "            \"model\": model,\n",
        "            \"service\": \"gemini\", \n",
        "            \"success\": True,\n",
        "            \"note\": \"Install: pip install google-generativeai\"\n",
        "        }\n",
        "\n",
        "# Initialize free API manager\n",
        "free_api = FreeAPIManager()\n",
        "\n",
        "print(\"ðŸ†“ FREE API OPTIONS FOR STUDENTS\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Choose one of these FREE options:\")\n",
        "print()\n",
        "print(\"Option 1: ðŸ¤— Hugging Face (Recommended for beginners)\")\n",
        "print(\"   - 1000 free requests per month\")\n",
        "print(\"   - Multiple models available\")\n",
        "print(\"   - Easy to get started\")\n",
        "print()\n",
        "print(\"Option 2: ðŸŸ¢ Google Gemini\")  \n",
        "print(\"   - 60 requests per minute (very generous!)\")\n",
        "print(\"   - High-quality responses\")\n",
        "print(\"   - Excellent for production testing\")\n",
        "print()\n",
        "print(\"Option 3: ðŸ”¥ Local Model (Already loaded above)\")\n",
        "print(\"   - Completely free (uses Colab GPU)\")\n",
        "print(\"   - Works offline\")\n",
        "print(\"   - Perfect for learning\")\n",
        "\n",
        "print(\"\\nðŸ’¡ SETUP INSTRUCTIONS:\")\n",
        "print(\"   Uncomment ONE of these lines to setup your preferred API:\")\n",
        "print(\"   # free_api.setup_huggingface_free()  # For Hugging Face\")\n",
        "print(\"   # free_api.setup_gemini_free()       # For Google Gemini\")\n",
        "print(\"\\nâœ… All services work perfectly on Google Colab!\")\n",
        "\n",
        "# Test function for whichever API is setup\n",
        "def test_free_api(prompt: str = \"Hello! How are you today?\"):\n",
        "    \"\"\"Test whichever API service is configured\"\"\"\n",
        "    if \"huggingface\" in free_api.available_services:\n",
        "        print(\"Testing Hugging Face API...\")\n",
        "        result = free_api.query_huggingface(\"microsoft/DialoGPT-medium\", prompt)\n",
        "        print(f\"Result: {result}\")\n",
        "    elif \"gemini\" in free_api.available_services:\n",
        "        print(\"Testing Gemini API...\")\n",
        "        result = free_api.query_gemini(prompt)\n",
        "        print(f\"Result: {result}\")\n",
        "    else:\n",
        "        print(\"âš ï¸  No API configured yet. Run setup first!\")\n",
        "\n",
        "print(\"\\nðŸ§ª After setup, test with: test_free_api()\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c78447dc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 ðŸŽ¯ Self-Consistency Prompting (MISSING TECHNIQUE)\n",
        "\n",
        "**Self-Consistency** is a powerful technique where we generate multiple responses to the same prompt and choose the most consistent answer. This significantly improves accuracy, especially for reasoning tasks.\n",
        "\n",
        "**ðŸ”¬ How Self-Consistency Works:**\n",
        "1. Generate multiple responses (typically 3-5) to the same prompt\n",
        "2. Analyze the consistency across responses  \n",
        "3. Choose the most frequent/consistent answer\n",
        "4. Particularly effective for mathematical reasoning and factual questions\n",
        "\n",
        "**ðŸ“Š Research shows 10-20% accuracy improvement with self-consistency!**"
      ],
      "id": "f23b8be9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸ§  SELF-CONSISTENCY PROMPTING IMPLEMENTATION\n",
        "# Generate multiple responses and find the most consistent answer\n",
        "\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "class SelfConsistencyEngine:\n",
        "    \"\"\"\n",
        "    Implement self-consistency prompting for improved accuracy\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_manager):\n",
        "        self.model_manager = model_manager\n",
        "        \n",
        "    def generate_multiple_responses(self, prompt: str, num_responses: int = 5, \n",
        "                                  temperature: float = 0.8) -> List[Dict]:\n",
        "        \"\"\"Generate multiple responses for self-consistency evaluation\"\"\"\n",
        "        responses = []\n",
        "        \n",
        "        print(f\"ðŸ”„ Generating {num_responses} responses for self-consistency...\")\n",
        "        \n",
        "        for i in range(num_responses):\n",
        "            print(f\"   Response {i+1}/{num_responses}...\", end=\"\")\n",
        "            \n",
        "            result = self.model_manager.generate_text_systematic(\n",
        "                prompt, \n",
        "                max_length=100, \n",
        "                temperature=temperature\n",
        "            )\n",
        "            \n",
        "            if result[\"error\"]:\n",
        "                print(f\" âŒ Error\")\n",
        "                continue\n",
        "                \n",
        "            responses.append({\n",
        "                \"response_id\": i+1,\n",
        "                \"output\": result[\"output\"],\n",
        "                \"metrics\": result[\"metrics\"]\n",
        "            })\n",
        "            print(f\" âœ…\")\n",
        "            time.sleep(0.1)  # Small delay to avoid overwhelming APIs\n",
        "            \n",
        "        return responses\n",
        "    \n",
        "    def extract_answers(self, responses: List[Dict], task_type: str = \"classification\") -> List[str]:\n",
        "        \"\"\"Extract the core answers from responses for comparison\"\"\"\n",
        "        answers = []\n",
        "        \n",
        "        for response in responses:\n",
        "            output = response[\"output\"].strip()\n",
        "            \n",
        "            if task_type == \"classification\":\n",
        "                # Extract the classification result (first word or after \":\")\n",
        "                if \":\" in output:\n",
        "                    answer = output.split(\":\")[-1].strip().split()[0].lower()\n",
        "                else:\n",
        "                    answer = output.split()[0].lower()\n",
        "                    \n",
        "            elif task_type == \"number\":\n",
        "                # Extract numerical answers\n",
        "                import re\n",
        "                numbers = re.findall(r'\\d+', output)\n",
        "                answer = numbers[0] if numbers else \"no_number\"\n",
        "                \n",
        "            else:  # general text\n",
        "                # Use first significant word/phrase\n",
        "                answer = output.split('.')[0].strip()[:50]\n",
        "            \n",
        "            answers.append(answer)\n",
        "            \n",
        "        return answers\n",
        "    \n",
        "    def find_consensus(self, answers: List[str]) -> Dict:\n",
        "        \"\"\"Find the most consistent answer across multiple responses\"\"\"\n",
        "        if not answers:\n",
        "            return {\"consensus\": None, \"confidence\": 0, \"distribution\": {}}\n",
        "        \n",
        "        # Count frequency of each answer\n",
        "        answer_counts = Counter(answers)\n",
        "        most_common_answer, max_count = answer_counts.most_common(1)[0]\n",
        "        \n",
        "        # Calculate confidence as percentage of responses\n",
        "        confidence = max_count / len(answers)\n",
        "        \n",
        "        return {\n",
        "            \"consensus\": most_common_answer,\n",
        "            \"confidence\": confidence,\n",
        "            \"distribution\": dict(answer_counts),\n",
        "            \"total_responses\": len(answers)\n",
        "        }\n",
        "    \n",
        "    def self_consistent_query(self, prompt: str, task_type: str = \"classification\", \n",
        "                            num_responses: int = 5) -> Dict:\n",
        "        \"\"\"\n",
        "        Perform complete self-consistency evaluation\n",
        "        \"\"\"\n",
        "        print(f\"ðŸŽ¯ SELF-CONSISTENCY EVALUATION\")\n",
        "        print(f\"   Task: {task_type}\")\n",
        "        print(f\"   Responses: {num_responses}\")\n",
        "        print(f\"   Prompt: {prompt[:100]}...\")\n",
        "        print()\n",
        "        \n",
        "        # Generate multiple responses\n",
        "        responses = self.generate_multiple_responses(prompt, num_responses)\n",
        "        \n",
        "        if len(responses) < 2:\n",
        "            return {\"error\": \"Need at least 2 successful responses for consensus\"}\n",
        "        \n",
        "        # Extract answers for comparison\n",
        "        answers = self.extract_answers(responses, task_type)\n",
        "        \n",
        "        # Find consensus\n",
        "        consensus_result = self.find_consensus(answers)\n",
        "        \n",
        "        print(f\"\\\\nðŸ“Š SELF-CONSISTENCY RESULTS:\")\n",
        "        print(f\"   Consensus: {consensus_result['consensus']}\")\n",
        "        print(f\"   Confidence: {consensus_result['confidence']:.1%}\")\n",
        "        print(f\"   Distribution: {consensus_result['distribution']}\")\n",
        "        \n",
        "        return {\n",
        "            \"consensus\": consensus_result,\n",
        "            \"individual_responses\": responses,\n",
        "            \"extracted_answers\": answers,\n",
        "            \"success\": True\n",
        "        }\n",
        "\n",
        "# Initialize self-consistency engine (if model manager is available)\n",
        "if 'model_manager' in locals() and model_manager.current_model:\n",
        "    sc_engine = SelfConsistencyEngine(model_manager)\n",
        "    \n",
        "    print(\"ðŸ§  SELF-CONSISTENCY ENGINE READY!\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Demo with a classification task\n",
        "    demo_prompt = \"\"\"Classify this dialogue type: meeting, social, support, or transaction.\n",
        "\n",
        "Dialogue: A: I need help with my password reset. B: I can help you with that. Let me send you a reset link.\n",
        "\n",
        "Classification:\"\"\"\n",
        "    \n",
        "    print(\"ðŸ§ª DEMO: Self-Consistency vs Single Response\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Single response (traditional)\n",
        "    single_result = model_manager.generate_text_systematic(demo_prompt, temperature=0.1)\n",
        "    print(f\"ðŸ”¸ Single response: {single_result.get('output', 'Error')}\")\n",
        "    \n",
        "    # Self-consistency (multiple responses)\n",
        "    print(\"\\\\nðŸ”¸ Self-consistency evaluation:\")\n",
        "    sc_result = sc_engine.self_consistent_query(demo_prompt, \"classification\", 3)\n",
        "    \n",
        "    if sc_result[\"success\"]:\n",
        "        print(f\"\\\\nâœ… IMPROVEMENT WITH SELF-CONSISTENCY:\")\n",
        "        print(f\"   Single: {single_result.get('output', 'Error')}\")\n",
        "        print(f\"   Consensus: {sc_result['consensus']['consensus']}\")\n",
        "        print(f\"   Confidence: {sc_result['consensus']['confidence']:.1%}\")\n",
        "        \n",
        "        if sc_result['consensus']['confidence'] >= 0.6:\n",
        "            print(f\"   ðŸŽ¯ High confidence - reliable answer!\")\n",
        "        else:\n",
        "            print(f\"   âš ï¸  Low confidence - may need more responses or prompt tuning\")\n",
        "    \n",
        "else:\n",
        "    print(\"âš ï¸  Model manager not available - showing self-consistency concept only\")\n",
        "    print(\"\\\\nðŸ”¬ Self-Consistency Process:\")\n",
        "    print(\"1. Generate 3-5 responses with temperature > 0.5\")\n",
        "    print(\"2. Extract core answers from each response\") \n",
        "    print(\"3. Count frequency of each answer\")\n",
        "    print(\"4. Choose most frequent answer as consensus\")\n",
        "    print(\"5. Calculate confidence as agreement percentage\")\n",
        "\n",
        "print(\"\\\\nðŸ’¡ WHEN TO USE SELF-CONSISTENCY:\")\n",
        "print(\"   âœ… Mathematical reasoning tasks\")\n",
        "print(\"   âœ… Factual questions with definitive answers\")\n",
        "print(\"   âœ… Classification tasks\")\n",
        "print(\"   âœ… When accuracy is more important than speed\")\n",
        "print(\"   âŒ Creative writing tasks\")\n",
        "print(\"   âŒ Open-ended discussions\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9f001db8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 ðŸŽ›ï¸ LLM Hyperparameters Deep Dive (EXPANDED COVERAGE)\n",
        "\n",
        "**Understanding hyperparameters is crucial for effective prompt engineering!**\n",
        "\n",
        "| **Parameter** | **Range** | **Effect** | **Use When** | **Avoid When** |\n",
        "|---------------|-----------|------------|--------------|----------------|\n",
        "| **ðŸŒ¡ï¸ Temperature** | 0.0-2.0 | Controls randomness | Creative tasks (0.7-1.2) | Factual Q&A (0.0-0.3) |\n",
        "| **ðŸŽ¯ Top-p** | 0.1-1.0 | Nucleus sampling | Balanced control (0.8-0.95) | Extreme creativity or determinism |\n",
        "| **ðŸ”¢ Top-k** | 1-100 | Limits vocabulary | Focused domains (10-40) | Open conversations (high k) |\n",
        "| **ðŸ“ Max Length** | 1-4096+ | Output length limit | Specific formats | Open exploration |\n",
        "| **ðŸ” Repetition Penalty** | 0.8-1.5 | Reduces repetition | Avoiding loops (1.1-1.3) | Poetry/patterns (â‰¤1.0) |\n",
        "\n",
        "**ðŸŽ¯ Optimal Settings by Task:**\n",
        "- **Factual Q&A**: temp=0.1, top_p=0.9, max_length=100\n",
        "- **Creative Writing**: temp=0.8, top_p=0.95, max_length=500\n",
        "- **Code Generation**: temp=0.2, top_p=0.9, max_length=200\n",
        "- **Classification**: temp=0.0, top_p=0.8, max_length=10"
      ],
      "id": "e2edc2bc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸ§ª HYPERPARAMETER EXPERIMENTATION FRAMEWORK\n",
        "# Systematic exploration of LLM hyperparameters\n",
        "\n",
        "class HyperparameterExplorer:\n",
        "    \"\"\"\n",
        "    Systematically test different hyperparameter combinations\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_manager):\n",
        "        self.model_manager = model_manager\n",
        "        self.results_df = pd.DataFrame()\n",
        "        \n",
        "    def create_hyperparameter_grid(self, task_type: str = \"classification\") -> List[Dict]:\n",
        "        \"\"\"Create systematic hyperparameter combinations for testing\"\"\"\n",
        "        \n",
        "        if task_type == \"classification\":\n",
        "            return [\n",
        "                {\"temperature\": 0.0, \"max_length\": 20, \"label\": \"Deterministic\"},\n",
        "                {\"temperature\": 0.3, \"max_length\": 20, \"label\": \"Low creativity\"},\n",
        "                {\"temperature\": 0.7, \"max_length\": 20, \"label\": \"Balanced\"},\n",
        "                {\"temperature\": 1.0, \"max_length\": 20, \"label\": \"Creative\"},\n",
        "            ]\n",
        "        elif task_type == \"generation\":\n",
        "            return [\n",
        "                {\"temperature\": 0.1, \"max_length\": 100, \"label\": \"Conservative\"}, \n",
        "                {\"temperature\": 0.5, \"max_length\": 100, \"label\": \"Moderate\"},\n",
        "                {\"temperature\": 0.8, \"max_length\": 100, \"label\": \"Creative\"},\n",
        "                {\"temperature\": 1.2, \"max_length\": 100, \"label\": \"Very creative\"},\n",
        "            ]\n",
        "        else:\n",
        "            return [\n",
        "                {\"temperature\": 0.2, \"max_length\": 50, \"label\": \"Default low\"},\n",
        "                {\"temperature\": 0.7, \"max_length\": 50, \"label\": \"Default balanced\"},\n",
        "            ]\n",
        "    \n",
        "    def test_hyperparameter_grid(self, prompt: str, task_type: str = \"classification\", \n",
        "                                runs_per_config: int = 3) -> pd.DataFrame:\n",
        "        \"\"\"Test systematic combinations of hyperparameters\"\"\"\n",
        "        \n",
        "        param_grid = self.create_hyperparameter_grid(task_type)\n",
        "        results = []\n",
        "        \n",
        "        print(f\"ðŸ§ª HYPERPARAMETER GRID SEARCH\")\n",
        "        print(f\"   Prompt: {prompt[:50]}...\")\n",
        "        print(f\"   Configurations: {len(param_grid)}\")\n",
        "        print(f\"   Runs per config: {runs_per_config}\")\n",
        "        print(f\"   Total experiments: {len(param_grid) * runs_per_config}\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        for i, config in enumerate(param_grid):\n",
        "            print(f\"\\\\nðŸŽ›ï¸  Config {i+1}/{len(param_grid)}: {config['label']}\")\n",
        "            print(f\"   Temperature: {config['temperature']}\")\n",
        "            print(f\"   Max length: {config['max_length']}\")\n",
        "            \n",
        "            config_results = []\n",
        "            \n",
        "            for run in range(runs_per_config):\n",
        "                print(f\"   Run {run+1}/{runs_per_config}...\", end=\"\")\n",
        "                \n",
        "                result = self.model_manager.generate_text_systematic(\n",
        "                    prompt,\n",
        "                    max_length=config[\"max_length\"],\n",
        "                    temperature=config[\"temperature\"]\n",
        "                )\n",
        "                \n",
        "                if result[\"error\"]:\n",
        "                    print(\" âŒ\")\n",
        "                    continue\n",
        "                \n",
        "                # Store systematic results\n",
        "                results.append({\n",
        "                    \"config_id\": i,\n",
        "                    \"config_label\": config[\"label\"],\n",
        "                    \"temperature\": config[\"temperature\"],\n",
        "                    \"max_length\": config[\"max_length\"],\n",
        "                    \"run_id\": run,\n",
        "                    \"output\": result[\"output\"],\n",
        "                    \"generation_time_ms\": result[\"metrics\"].get(\"generation_time_ms\", 0),\n",
        "                    \"output_tokens\": result[\"metrics\"].get(\"output_tokens\", 0),\n",
        "                    \"output_length\": len(result[\"output\"]),\n",
        "                    \"timestamp\": time.time()\n",
        "                })\n",
        "                \n",
        "                config_results.append(result[\"output\"])\n",
        "                print(\" âœ…\")\n",
        "            \n",
        "            # Show variety within this configuration\n",
        "            if config_results:\n",
        "                unique_outputs = len(set(config_results))\n",
        "                print(f\"   Unique outputs: {unique_outputs}/{len(config_results)}\")\n",
        "                if len(config_results) > 1:\n",
        "                    print(f\"   Sample outputs:\")\n",
        "                    for j, output in enumerate(config_results[:2]):\n",
        "                        print(f\"     {j+1}: {output[:60]}...\")\n",
        "        \n",
        "        # Convert to DataFrame for analysis\n",
        "        results_df = pd.DataFrame(results)\n",
        "        self.results_df = pd.concat([self.results_df, results_df], ignore_index=True)\n",
        "        \n",
        "        return results_df\n",
        "    \n",
        "    def analyze_hyperparameter_effects(self, results_df: pd.DataFrame):\n",
        "        \"\"\"Analyze the effects of different hyperparameter settings\"\"\"\n",
        "        \n",
        "        print(f\"\\\\nðŸ“Š HYPERPARAMETER ANALYSIS\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        # Group by configuration\n",
        "        config_analysis = results_df.groupby(['config_label', 'temperature']).agg({\n",
        "            'output_length': ['mean', 'std'],\n",
        "            'generation_time_ms': ['mean', 'std'],\n",
        "            'output': lambda x: len(set(x))  # Unique outputs (diversity)\n",
        "        }).round(2)\n",
        "        \n",
        "        config_analysis.columns = ['Avg_Length', 'Std_Length', 'Avg_Time_ms', 'Std_Time_ms', 'Diversity']\n",
        "        \n",
        "        print(\"\\\\nðŸŽ¯ Configuration Performance:\")\n",
        "        display(config_analysis)\n",
        "        \n",
        "        # Temperature effect analysis\n",
        "        print(f\"\\\\nðŸŒ¡ï¸ TEMPERATURE EFFECTS:\")\n",
        "        temp_effects = results_df.groupby('temperature').agg({\n",
        "            'output_length': 'mean',\n",
        "            'output': lambda x: len(set(x)) / len(x)  # Diversity ratio\n",
        "        }).round(3)\n",
        "        \n",
        "        for temp, row in temp_effects.iterrows():\n",
        "            diversity_pct = row['output'] * 100\n",
        "            print(f\"   Temperature {temp}: Avg length {row['output_length']:.0f}, Diversity {diversity_pct:.0f}%\")\n",
        "        \n",
        "        # Visualization if matplotlib is available\n",
        "        try:\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "            \n",
        "            # Length vs Temperature\n",
        "            results_df.groupby('temperature')['output_length'].mean().plot(kind='bar', ax=ax1)\n",
        "            ax1.set_title('Output Length vs Temperature')\n",
        "            ax1.set_ylabel('Average Output Length')\n",
        "            \n",
        "            # Generation time vs Temperature  \n",
        "            results_df.groupby('temperature')['generation_time_ms'].mean().plot(kind='bar', ax=ax2)\n",
        "            ax2.set_title('Generation Time vs Temperature')\n",
        "            ax2.set_ylabel('Average Time (ms)')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "        except:\n",
        "            print(\"   (Visualization requires matplotlib)\")\n",
        "        \n",
        "        return config_analysis\n",
        "\n",
        "# Initialize hyperparameter explorer (if model available)\n",
        "if 'model_manager' in locals() and model_manager.current_model:\n",
        "    hp_explorer = HyperparameterExplorer(model_manager)\n",
        "    \n",
        "    print(\"ðŸŽ›ï¸ HYPERPARAMETER EXPLORER READY!\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Demo hyperparameter experimentation\n",
        "    demo_prompt = \"Classify this conversation: A: Can you help me reset my password? B: Of course, I'll send you a link.\"\n",
        "    \n",
        "    print(\"\\\\nðŸ§ª DEMO: Hyperparameter Grid Search\")\n",
        "    print(\"Testing different temperature settings...\")\n",
        "    \n",
        "    # Run hyperparameter grid search\n",
        "    hp_results = hp_explorer.test_hyperparameter_grid(\n",
        "        demo_prompt, \n",
        "        task_type=\"classification\",\n",
        "        runs_per_config=2  # Use 2 for demo (normally 3-5)\n",
        "    )\n",
        "    \n",
        "    # Analyze results\n",
        "    analysis = hp_explorer.analyze_hyperparameter_effects(hp_results)\n",
        "    \n",
        "    print(\"\\\\nðŸ’¡ KEY INSIGHTS:\")\n",
        "    print(\"   ðŸŒ¡ï¸  Lower temperature = more consistent outputs\")\n",
        "    print(\"   ðŸŒ¡ï¸  Higher temperature = more diverse/creative outputs\") \n",
        "    print(\"   â±ï¸  Temperature has minimal effect on generation speed\")\n",
        "    print(\"   ðŸ“ Max length controls output verbosity\")\n",
        "    \n",
        "else:\n",
        "    print(\"âš ï¸  Model manager not available - showing hyperparameter concepts\")\n",
        "    \n",
        "print(\"\\\\nðŸŽ¯ HYPERPARAMETER BEST PRACTICES:\")\n",
        "print(\"   ðŸ“‹ Classification tasks: temperature = 0.0-0.3\")\n",
        "print(\"   âœï¸  Creative generation: temperature = 0.7-1.0\") \n",
        "print(\"   ðŸ” Factual Q&A: temperature = 0.0-0.2\")\n",
        "print(\"   ðŸ—£ï¸  Dialogue: temperature = 0.4-0.7\")\n",
        "print(\"   ðŸ§® Code generation: temperature = 0.1-0.4\")\n",
        "\n",
        "print(\"\\\\nâš¡ PERFORMANCE TIPS:\")\n",
        "print(\"   âœ… Start with temperature=0.0 for reproducible results\")\n",
        "print(\"   âœ… Increase temperature gradually for more creativity\")\n",
        "print(\"   âœ… Use max_length to prevent overly long responses\")\n",
        "print(\"   âœ… Test multiple runs to understand variability\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "654d1335"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ“ SESSION COMPLETE: Advanced Prompt Engineering Mastery\n",
        "\n",
        "### âœ… What You've Mastered Today\n",
        "\n",
        "**ðŸ—ï¸ Pre-trained Models:**\n",
        "- âœ… Model family comparison (local vs API access)\n",
        "- âœ… Systematic evaluation framework with performance tracking\n",
        "- âœ… Free API integration for Colab (Hugging Face, Gemini)\n",
        "\n",
        "**ðŸŽ¨ Prompt Engineering & Design:**\n",
        "- âœ… Zero-shot prompting for immediate results\n",
        "- âœ… Few-shot prompting with cross-lingual examples\n",
        "- âœ… Chain-of-thought for step-by-step reasoning\n",
        "- âœ… **Self-consistency for improved accuracy** (NEW!)\n",
        "\n",
        "**ðŸŽ›ï¸ LLM Hyperparameters:**\n",
        "- âœ… Temperature, top-p, max_length optimization\n",
        "- âœ… Task-specific parameter tuning\n",
        "- âœ… Systematic hyperparameter grid search (NEW!)\n",
        "\n",
        "**ðŸŒ Low-Resource Languages:**\n",
        "- âœ… Cross-lingual prompt transfer strategies\n",
        "- âœ… Cultural adaptation and sensitivity evaluation\n",
        "- âœ… Few-shot learning for resource-constrained languages\n",
        "\n",
        "**ðŸ“Š Systematic Evaluation:**\n",
        "- âœ… Research-grade methodology with pandas tracking\n",
        "- âœ… Quantitative metrics and performance analysis\n",
        "- âœ… Structured comparison across techniques\n",
        "\n",
        "### ðŸš€ Next Steps for Your Projects\n",
        "\n",
        "**ðŸ“‹ Immediate Actions:**\n",
        "1. **Choose your API**: Set up Hugging Face or Gemini free API\n",
        "2. **Test with your language**: Apply techniques to your specific use case\n",
        "3. **Document insights**: Record what works best for your domain\n",
        "4. **Experiment systematically**: Use the evaluation frameworks provided\n",
        "\n",
        "**ðŸ”¬ Advanced Experiments:**\n",
        "- Compare self-consistency vs single-shot for your tasks\n",
        "- Optimize hyperparameters for your specific language/domain\n",
        "- Create custom few-shot examples for your cultural context\n",
        "- Build evaluation metrics for your specific requirements\n",
        "\n",
        "### ðŸ’¡ Key Production Insights\n",
        "\n",
        "**ðŸŽ¯ For Classification Tasks:**\n",
        "- Use temperature=0.0 with self-consistency (3-5 responses)\n",
        "- Few-shot examples improve cross-lingual transfer\n",
        "- Cultural context matters more than linguistic accuracy\n",
        "\n",
        "**âœï¸ For Text Generation:**\n",
        "- Start with temperature=0.7, adjust based on creativity needs\n",
        "- Chain-of-thought improves factual accuracy significantly  \n",
        "- Self-consistency reduces hallucination in factual tasks\n",
        "\n",
        "**âš¡ For Performance:**\n",
        "- Local mT5 models: Best for privacy and customization\n",
        "- Free APIs: Excellent for learning and small-scale projects\n",
        "- Systematic evaluation saves time in the long run\n",
        "\n",
        "### ðŸŒŸ Remember: Ethics and Responsible AI\n",
        "\n",
        "**Always consider:**\n",
        "- Cultural sensitivity in your prompts and evaluations\n",
        "- Privacy implications of your chosen API service\n",
        "- Bias evaluation across different demographic groups\n",
        "- Environmental impact of your model choices\n",
        "\n",
        "---\n",
        "\n",
        "**ðŸŽ‰ Congratulations!** You now have production-ready prompt engineering skills with systematic evaluation methodology. Keep experimenting and building responsibly! ðŸš€"
      ],
      "id": "0f0f14e1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}