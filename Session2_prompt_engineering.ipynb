{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "467d9f0f",
      "metadata": {},
      "source": [
        "# Session 2: Pretrained Models and Prompt Engineering ü§ñ\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "**üìö Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/Session2_prompt_engineering.ipynb)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "Welcome to **systematic LLM-based prompt engineering** for dialogue summarization and cross-lingual tasks! This session combines rigorous methodology with practical applications, focusing on low-resource language challenges.\n",
        "\n",
        "**üéØ Focus:** Systematic prompt engineering, multilingual evaluation, dialogue summarization  \n",
        "**üíª Requirements:** GPU recommended for large models OR API access for best results  \n",
        "**üî¨ Methodology:** Research-grade systematic evaluation with pandas DataFrames\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "**üìã Recommended learning path:**\n",
        "1. **Session 0:** Setup and tokenization basics ‚úÖ  \n",
        "2. **Session 1:** Systematic baseline techniques ‚úÖ\n",
        "3. **This session (Session 2):** Systematic LLM prompt engineering ‚Üê You are here!\n",
        "\n",
        "## What You Will Master\n",
        "\n",
        "1. **üèóÔ∏è Model family comparison** and systematic access pattern evaluation\n",
        "2. **üé® Prompt engineering vs. prompt design** with systematic methodology\n",
        "3. **üéØ Multi-strategy prompting** (zero-shot, few-shot, Chain-of-Thought) with quantitative comparison\n",
        "4. **üåç Cross-lingual prompt transfer** with systematic cultural adaptation\n",
        "5. **üìä Comprehensive evaluation framework** (correctness, fluency, cultural appropriateness)\n",
        "6. **üíº Production-ready insights** with actionable recommendations\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this session, you will:\n",
        "- ‚úÖ **Systematically compare** pretrained model families using structured evaluation\n",
        "- ‚úÖ **Design culturally-aware prompts** for dialogue summarization and classification\n",
        "- ‚úÖ **Implement systematic prompt engineering** with quantitative tracking\n",
        "- ‚úÖ **Evaluate cross-lingual performance** using research-grade metrics\n",
        "- ‚úÖ **Generate actionable insights** for production deployment decisions\n",
        "- ‚úÖ **Export structured findings** for research and business applications\n",
        "\n",
        "## üî¨ Research Methodology\n",
        "\n",
        "**This session follows systematic research practices:**\n",
        "\n",
        "- **üìä Structured Data Collection:** All experiments tracked in pandas DataFrames\n",
        "- **üéØ Controlled Comparisons:** Systematic A/B testing of prompt strategies  \n",
        "- **üìà Quantitative Analysis:** Statistical evaluation with visualization\n",
        "- **üåç Cultural Sensitivity:** Multi-dimensional appropriateness assessment\n",
        "- **üíæ Reproducible Results:** Exportable data for further analysis\n",
        "\n",
        "## How This Advanced Session Works\n",
        "\n",
        "- **üéì Theory + Systematic Practice:** Learn concepts ‚Üí Apply systematically ‚Üí Analyze quantitatively\n",
        "- **üî¨ Hypothesis-Driven Experiments:** Form hypotheses ‚Üí Test systematically ‚Üí Draw conclusions\n",
        "- **üìä Data-First Analysis:** Every decision backed by quantitative evidence\n",
        "- **üí¨ Evidence-Based Discussions:** Group analysis using concrete experimental data\n",
        "- **üåç Cross-Cultural Focus:** Systematic evaluation across language/culture pairs\n",
        "- **üèÜ Production Insights:** Actionable recommendations for real-world deployment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31db6028",
      "metadata": {},
      "source": [
        "## 0. üî¨ Systematic Setup and Model Access Framework\n",
        "\n",
        "**Strategic Decision:** Choose your model access pattern based on systematic evaluation of your requirements.\n",
        "\n",
        "### 0.1 Access Pattern Decision Framework\n",
        "\n",
        "| **Access Pattern** | **Pros** | **Cons** | **Best For** | **Cost** |\n",
        "|-------------------|----------|----------|--------------|----------|\n",
        "| **üî• Local Models (mT5)** | Privacy, offline, customizable | GPU required, setup time | Research, sensitive data | Hardware only |\n",
        "| **‚òÅÔ∏è API Access (GPT-4)** | State-of-art, no setup, scalable | Token costs, internet needed | Production, experiments | $0.01-0.03/1K tokens |\n",
        "| **üåê Hosted (Colab/HF)** | Free tiers, easy setup | Limited resources, usage caps | Learning, prototyping | Free-$10/month |\n",
        "\n",
        "**üéØ Recommendation Matrix:**\n",
        "- **Learning/Research:** Start with Local Models (below) + backup API for comparison\n",
        "- **Production Planning:** Test with APIs, validate costs, then decide on deployment\n",
        "- **Resource-Constrained:** Use hosted solutions with systematic evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1efff16a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ Systematic Setup for Session 2: Advanced Prompt Engineering\n",
        "# Install packages with systematic dependency management\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def install_packages_systematic(packages):\n",
        "    \"\"\"Install packages with better error handling and progress tracking\"\"\"\n",
        "    installed = []\n",
        "    failed = []\n",
        "    \n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "            installed.append(package.split(\">=\")[0].split(\"==\")[0])\n",
        "            print(f\"‚úÖ {package}\")\n",
        "        except Exception as e:\n",
        "            failed.append((package, str(e)[:50]))\n",
        "            print(f\"‚ùå {package}: {str(e)[:50]}...\")\n",
        "    \n",
        "    return installed, failed\n",
        "\n",
        "print(\"üöÄ SYSTEMATIC SETUP: Installing advanced packages...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Core packages for systematic evaluation\n",
        "core_packages = [\n",
        "    \"pandas>=1.5.0\",\n",
        "    \"matplotlib>=3.5.0\", \n",
        "    \"seaborn>=0.11.0\",\n",
        "    \"numpy>=1.21.0\"\n",
        "]\n",
        "\n",
        "# LLM packages (grouped for better dependency management)\n",
        "llm_packages = [\n",
        "    \"transformers>=4.35.0\",\n",
        "    \"torch>=1.13.0\", \n",
        "    \"sentencepiece\",\n",
        "    \"accelerate\",\n",
        "    \"datasets\"\n",
        "]\n",
        "\n",
        "print(\"üìä Installing data science packages...\")\n",
        "core_installed, core_failed = install_packages_systematic(core_packages)\n",
        "\n",
        "print(\"ü§ñ Installing LLM packages...\")\n",
        "llm_installed, llm_failed = install_packages_systematic(llm_packages)\n",
        "\n",
        "# Essential imports for systematic evaluation\n",
        "try:\n",
        "    import torch\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "    from typing import List, Dict, Optional, Tuple\n",
        "    import time\n",
        "    import json\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Set plotting style for professional visualizations\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "    \n",
        "    print(f\"\\nüéØ SYSTEM CONFIGURATION:\")\n",
        "    print(f\"   Python: {sys.version.split()[0]}\")\n",
        "    print(f\"   PyTorch: {torch.__version__}\")\n",
        "    print(f\"   Pandas: {pd.__version__}\")\n",
        "    print(f\"   GPU Available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"   GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    \n",
        "    print(f\"\\nüìä EXPERIMENTAL FRAMEWORK READY:\")\n",
        "    print(f\"   ‚úÖ Structured data collection with pandas\")\n",
        "    print(f\"   ‚úÖ Statistical analysis and visualization\")\n",
        "    print(f\"   ‚úÖ Export capabilities for research\")\n",
        "    print(f\"   ‚úÖ Systematic model comparison framework\")\n",
        "    \n",
        "    setup_success = True\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"\\n‚ùå IMPORT ERROR: {e}\")\n",
        "    print(\"üîÑ Try restarting the runtime and running this cell again\")\n",
        "    setup_success = False\n",
        "\n",
        "# Verification and troubleshooting\n",
        "if core_failed or llm_failed:\n",
        "    print(f\"\\n‚ö†Ô∏è  INSTALLATION ISSUES DETECTED:\")\n",
        "    for pkg, error in core_failed + llm_failed:\n",
        "        print(f\"   ‚ùå {pkg}: {error}\")\n",
        "    print(f\"\\nüí° SOLUTIONS:\")\n",
        "    print(f\"   1. Runtime ‚Üí Restart Runtime, then re-run this cell\")\n",
        "    print(f\"   2. Check internet connection\")\n",
        "    print(f\"   3. Try installing packages individually\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ SYSTEMATIC SETUP COMPLETE!\")\n",
        "print(\"üî¨ Ready for research-grade prompt engineering experiments\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac65079",
      "metadata": {},
      "source": [
        "## 1. üî¨ Systematic Experimental Framework Setup\n",
        "\n",
        "**Research-Grade Methodology:** Before testing models, we establish systematic evaluation framework.\n",
        "\n",
        "### 1.1 üìä Define Experimental Scope\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a132a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üåç Configure Your Systematic Experiment: Languages and Tasks\n",
        "# This systematic approach ensures reproducible, comparable results\n",
        "\n",
        "# Define your target languages (CUSTOMIZE THIS FOR YOUR RESEARCH)\n",
        "target_languages = [\n",
        "    {\n",
        "        \"code\": \"en\", \n",
        "        \"name\": \"English\", \n",
        "        \"family\": \"Germanic\",\n",
        "        \"speakers\": \"1.5B\",\n",
        "        \"resource_level\": \"high\",\n",
        "        \"writing_system\": \"Latin\"\n",
        "    },\n",
        "    {\n",
        "        \"code\": \"fr\", \n",
        "        \"name\": \"French\", \n",
        "        \"family\": \"Romance\",\n",
        "        \"speakers\": \"280M\", \n",
        "        \"resource_level\": \"high\",\n",
        "        \"writing_system\": \"Latin\"\n",
        "    },\n",
        "    {\n",
        "        \"code\": \"ar\", \n",
        "        \"name\": \"Arabic\", \n",
        "        \"family\": \"Semitic\",\n",
        "        \"speakers\": \"420M\",\n",
        "        \"resource_level\": \"medium\",\n",
        "        \"writing_system\": \"Arabic\"\n",
        "    },\n",
        "    # üéØ ADD YOUR LOW-RESOURCE LANGUAGE HERE:\n",
        "    # {\n",
        "    #     \"code\": \"your_code\", \n",
        "    #     \"name\": \"Your Language\", \n",
        "    #     \"family\": \"Language Family\",\n",
        "    #     \"speakers\": \"~XXXk\",\n",
        "    #     \"resource_level\": \"low\",\n",
        "    #     \"writing_system\": \"Script\"\n",
        "    # },\n",
        "]\n",
        "\n",
        "# Define systematic task framework for dialogue summarization and classification\n",
        "experimental_tasks = [\n",
        "    {\n",
        "        \"task_id\": \"dialogue_classification\",\n",
        "        \"description\": \"Classify dialogue type: meeting, social, support, transaction\",\n",
        "        \"evaluation_type\": \"categorical_accuracy\",\n",
        "        \"cultural_sensitivity\": \"medium\",\n",
        "        \"domain\": \"general\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"dialogue_summarization\", \n",
        "        \"description\": \"Generate concise summary of dialogue content\",\n",
        "        \"evaluation_type\": \"generative_quality\",\n",
        "        \"cultural_sensitivity\": \"high\",\n",
        "        \"domain\": \"general\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"intent_extraction\",\n",
        "        \"description\": \"Extract primary intent/action items from dialogue\",\n",
        "        \"evaluation_type\": \"information_extraction\", \n",
        "        \"cultural_sensitivity\": \"high\",\n",
        "        \"domain\": \"business\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to DataFrames for systematic analysis\n",
        "languages_df = pd.DataFrame(target_languages)\n",
        "tasks_df = pd.DataFrame(experimental_tasks)\n",
        "\n",
        "print(\"üåç SYSTEMATIC EXPERIMENTAL DESIGN\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\\\nüìã Target Languages:\")\n",
        "display(languages_df[[\"name\", \"family\", \"resource_level\", \"speakers\", \"writing_system\"]])\n",
        "\n",
        "print(\"\\\\nüéØ Experimental Tasks:\")\n",
        "display(tasks_df[[\"task_id\", \"description\", \"evaluation_type\", \"cultural_sensitivity\"]])\n",
        "\n",
        "print(f\"\\\\nüìä EXPERIMENTAL MATRIX:\")\n",
        "print(f\"   Languages: {len(languages_df)} \")\n",
        "print(f\"   Tasks: {len(tasks_df)}\")\n",
        "print(f\"   Total combinations: {len(languages_df) * len(tasks_df)}\")\n",
        "print(f\"   Systematic evaluation ensures comprehensive coverage!\")\n",
        "\n",
        "# Create systematic evaluation tracking framework\n",
        "evaluation_columns = [\n",
        "    # Experiment identifiers\n",
        "    \"experiment_id\", \"timestamp\", \"language_code\", \"language_name\", \"task_id\",\n",
        "    \n",
        "    # Model and prompt configuration\n",
        "    \"model_name\", \"model_type\", \"access_pattern\", \"prompt_strategy\", \"shots_used\",\n",
        "    \n",
        "    # Input/output data\n",
        "    \"input_text\", \"expected_output\", \"actual_output\", \"prompt_text\",\n",
        "    \n",
        "    # Quantitative metrics\n",
        "    \"correctness_score\", \"fluency_score\", \"cultural_appropriateness_score\",\n",
        "    \"response_time_ms\", \"token_count_input\", \"token_count_output\",\n",
        "    \n",
        "    # Qualitative assessment\n",
        "    \"quality_issues\", \"cultural_notes\", \"improvement_suggestions\", \n",
        "    \n",
        "    # Systematic metadata\n",
        "    \"experiment_conditions\", \"model_parameters\", \"success_flag\"\n",
        "]\n",
        "\n",
        "# Initialize systematic experiment tracking\n",
        "experiments_df = pd.DataFrame(columns=evaluation_columns)\n",
        "\n",
        "print(f\"\\\\nüî¨ EVALUATION FRAMEWORK INITIALIZED:\")\n",
        "print(f\"   Tracking {len(evaluation_columns)} systematic metrics per experiment\")\n",
        "print(f\"   Ready for systematic data collection and analysis\")\n",
        "print(f\"   ‚úÖ Research-grade methodology established!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03fbca1d",
      "metadata": {},
      "source": [
        "### 1.2 üìù Systematic Test Data Framework\n",
        "\n",
        "**Structured Test Cases:** Ensure consistent, comparable evaluation across languages and tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c115c684",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Systematic Test Data Creation\n",
        "# Structured test cases for systematic comparison across languages and tasks\n",
        "\n",
        "def create_systematic_test_data():\n",
        "    \"\"\"\n",
        "    Create structured test data for systematic evaluation across languages and tasks.\n",
        "    This ensures consistent comparison and eliminates ad-hoc testing bias.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Systematic dialogue test cases (parallel across languages)\n",
        "    test_cases = []\n",
        "    \n",
        "    # Test Case 1: Business Meeting Scenario\n",
        "    test_cases.append({\n",
        "        \"case_id\": \"business_meeting_01\",\n",
        "        \"domain\": \"business\", \n",
        "        \"complexity\": \"medium\",\n",
        "        \"cultural_context\": \"professional\",\n",
        "        \"languages\": {\n",
        "            \"en\": {\n",
        "                \"dialogue\": \"A: We need to finalize the budget by Friday. B: I'll have the numbers ready by Thursday. A: Perfect, let's schedule a review meeting.\",\n",
        "                \"expected_classification\": \"meeting\",\n",
        "                \"expected_summary\": \"Team discusses budget deadline and schedules review meeting for Thursday numbers.\",\n",
        "                \"expected_intent\": \"Schedule budget review meeting\"\n",
        "            },\n",
        "            \"fr\": {\n",
        "                \"dialogue\": \"A: Nous devons finaliser le budget vendredi. B: J'aurai les chiffres pr√™ts jeudi. A: Parfait, planifions une r√©union de r√©vision.\", \n",
        "                \"expected_classification\": \"meeting\",\n",
        "                \"expected_summary\": \"L'√©quipe discute de l'√©ch√©ance budg√©taire et planifie une r√©union de r√©vision jeudi.\",\n",
        "                \"expected_intent\": \"Planifier une r√©union de r√©vision budg√©taire\"\n",
        "            },\n",
        "            \"ar\": {\n",
        "                \"dialogue\": \"ÿ£: ŸÜÿ≠ÿ™ÿßÿ¨ ŸÑÿ•ŸÜŸáÿßÿ° ÿßŸÑŸÖŸäÿ≤ÿßŸÜŸäÿ© ŸäŸàŸÖ ÿßŸÑÿ¨ŸÖÿπÿ©. ÿ®: ÿ≥ÿ£ÿ¨Ÿáÿ≤ ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ ŸäŸàŸÖ ÿßŸÑÿÆŸÖŸäÿ≥. ÿ£: ŸÖŸÖÿ™ÿßÿ≤ÿå ŸÑŸÜÿ≠ÿØÿØ ŸÖŸàÿπÿØ ÿßÿ¨ÿ™ŸÖÿßÿπ ŸÖÿ±ÿßÿ¨ÿπÿ©.\",\n",
        "                \"expected_classification\": \"meeting\", \n",
        "                \"expected_summary\": \"ŸäŸÜÿßŸÇÿ¥ ÿßŸÑŸÅÿ±ŸäŸÇ ŸÖŸàÿπÿØ ÿßŸÑŸÖŸäÿ≤ÿßŸÜŸäÿ© ÿßŸÑŸÜŸáÿßÿ¶Ÿä ŸàŸäÿ≠ÿØÿØ ÿßÿ¨ÿ™ŸÖÿßÿπ ŸÖÿ±ÿßÿ¨ÿπÿ© ŸÑŸÑÿ£ÿ±ŸÇÿßŸÖ ŸäŸàŸÖ ÿßŸÑÿÆŸÖŸäÿ≥.\",\n",
        "                \"expected_intent\": \"ÿ¨ÿØŸàŸÑÿ© ÿßÿ¨ÿ™ŸÖÿßÿπ ŸÖÿ±ÿßÿ¨ÿπÿ© ÿßŸÑŸÖŸäÿ≤ÿßŸÜŸäÿ©\"\n",
        "            }\n",
        "            # üéØ ADD YOUR LANGUAGE HERE following the same structure\n",
        "        }\n",
        "    })\n",
        "    \n",
        "    # Test Case 2: Technical Support Scenario  \n",
        "    test_cases.append({\n",
        "        \"case_id\": \"tech_support_01\",\n",
        "        \"domain\": \"support\",\n",
        "        \"complexity\": \"low\", \n",
        "        \"cultural_context\": \"service\",\n",
        "        \"languages\": {\n",
        "            \"en\": {\n",
        "                \"dialogue\": \"A: My computer won't start this morning. B: Did you try unplugging it for 30 seconds? A: Yes, but still nothing. B: Let me schedule a technician visit.\",\n",
        "                \"expected_classification\": \"support\",\n",
        "                \"expected_summary\": \"Customer reports computer startup issue, basic troubleshooting attempted, technician visit scheduled.\",\n",
        "                \"expected_intent\": \"Schedule technician visit for computer repair\"\n",
        "            },\n",
        "            \"fr\": {\n",
        "                \"dialogue\": \"A: Mon ordinateur ne d√©marre pas ce matin. B: Avez-vous essay√© de le d√©brancher 30 secondes? A: Oui, mais toujours rien. B: Laissez-moi programmer une visite de technicien.\",\n",
        "                \"expected_classification\": \"support\", \n",
        "                \"expected_summary\": \"Le client signale un probl√®me de d√©marrage d'ordinateur, d√©pannage de base tent√©, visite de technicien programm√©e.\",\n",
        "                \"expected_intent\": \"Programmer une visite de technicien pour r√©paration d'ordinateur\"\n",
        "            },\n",
        "            \"ar\": {\n",
        "                \"dialogue\": \"ÿ£: ÿ¨Ÿáÿßÿ≤ ÿßŸÑŸÉŸÖÿ®ŸäŸàÿ™ÿ± ŸÑÿß ŸäÿπŸÖŸÑ Ÿáÿ∞ÿß ÿßŸÑÿµÿ®ÿßÿ≠. ÿ®: ŸáŸÑ ÿ¨ÿ±ÿ®ÿ™ ŸÅÿµŸÑŸá ŸÑŸÖÿØÿ© 30 ÿ´ÿßŸÜŸäÿ©ÿü ÿ£: ŸÜÿπŸÖÿå ŸÑŸÉŸÜ ŸÑÿß ÿ¥Ÿäÿ°. ÿ®: ÿØÿπŸÜŸä ÿ£ÿ≠ÿØÿØ ÿ≤Ÿäÿßÿ±ÿ© ŸÅŸÜŸä.\",\n",
        "                \"expected_classification\": \"support\",\n",
        "                \"expected_summary\": \"Ÿäÿ®ŸÑÿ∫ ÿßŸÑÿπŸÖŸäŸÑ ÿπŸÜ ŸÖÿ¥ŸÉŸÑÿ© ÿ®ÿØÿ° ÿ™ÿ¥ÿ∫ŸäŸÑ ÿßŸÑŸÉŸÖÿ®ŸäŸàÿ™ÿ±ÿå ÿ™ŸÖ ŸÖÿ≠ÿßŸàŸÑÿ© ÿßÿ≥ÿ™ŸÉÿ¥ÿßŸÅ ÿßŸÑÿ£ÿÆÿ∑ÿßÿ° ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©ÿå ÿ™ŸÖ ÿ¨ÿØŸàŸÑÿ© ÿ≤Ÿäÿßÿ±ÿ© ŸÅŸÜŸä.\", \n",
        "                \"expected_intent\": \"ÿ¨ÿØŸàŸÑÿ© ÿ≤Ÿäÿßÿ±ÿ© ŸÅŸÜŸä ŸÑÿ•ÿµŸÑÿßÿ≠ ÿßŸÑŸÉŸÖÿ®ŸäŸàÿ™ÿ±\"\n",
        "            }\n",
        "        }\n",
        "    })\n",
        "    \n",
        "    # Test Case 3: Social Conversation Scenario\n",
        "    test_cases.append({\n",
        "        \"case_id\": \"social_conversation_01\", \n",
        "        \"domain\": \"social\",\n",
        "        \"complexity\": \"low\",\n",
        "        \"cultural_context\": \"informal\",\n",
        "        \"languages\": {\n",
        "            \"en\": {\n",
        "                \"dialogue\": \"A: How was your weekend hiking trip? B: Amazing! The weather was perfect and the views were incredible. A: I'd love to join you next time!\",\n",
        "                \"expected_classification\": \"social\",\n",
        "                \"expected_summary\": \"Friends discuss successful weekend hiking trip with great weather and views, plan future trip together.\",\n",
        "                \"expected_intent\": \"Plan future hiking trip together\"\n",
        "            },\n",
        "            \"fr\": {\n",
        "                \"dialogue\": \"A: Comment s'est pass√©e ta randonn√©e du week-end? B: Fantastique! Le temps √©tait parfait et les vues incroyables. A: J'aimerais vous accompagner la prochaine fois!\",\n",
        "                \"expected_classification\": \"social\",\n",
        "                \"expected_summary\": \"Les amis discutent d'une randonn√©e r√©ussie du week-end avec beau temps et vues, planifient un voyage futur ensemble.\",\n",
        "                \"expected_intent\": \"Planifier une future randonn√©e ensemble\"\n",
        "            },\n",
        "            \"ar\": {\n",
        "                \"dialogue\": \"ÿ£: ŸÉŸäŸÅ ŸÉÿßŸÜÿ™ ÿ±ÿ≠ŸÑÿ© ÿßŸÑŸÖÿ¥Ÿä ŸÅŸä ŸÜŸáÿßŸäÿ© ÿßŸÑÿ£ÿ≥ÿ®Ÿàÿπÿü ÿ®: ÿ±ÿßÿ¶ÿπÿ©! ŸÉÿßŸÜ ÿßŸÑÿ∑ŸÇÿ≥ ŸÖÿ´ÿßŸÑŸäÿßŸã ŸàÿßŸÑŸÖŸÜÿßÿ∏ÿ± ŸÑÿß ÿ™ÿµÿØŸÇ. ÿ£: ÿ£ŸàÿØ ÿßŸÑÿßŸÜÿ∂ŸÖÿßŸÖ ÿ•ŸÑŸäŸÉŸÖ ÿßŸÑŸÖÿ±ÿ© ÿßŸÑŸÇÿßÿØŸÖÿ©!\",\n",
        "                \"expected_classification\": \"social\",\n",
        "                \"expected_summary\": \"ŸäŸÜÿßŸÇÿ¥ ÿßŸÑÿ£ÿµÿØŸÇÿßÿ° ÿ±ÿ≠ŸÑÿ© ÿßŸÑŸÖÿ¥Ÿä ÿßŸÑŸÜÿßÿ¨ÿ≠ÿ© ŸÅŸä ŸÜŸáÿßŸäÿ© ÿßŸÑÿ£ÿ≥ÿ®Ÿàÿπ ŸÖÿπ ÿ∑ŸÇÿ≥ ÿ±ÿßÿ¶ÿπ ŸàŸÖŸÜÿßÿ∏ÿ±ÿå ŸäÿÆÿ∑ÿ∑ŸàŸÜ ŸÑÿ±ÿ≠ŸÑÿ© ŸÖÿ≥ÿ™ŸÇÿ®ŸÑŸäÿ© ŸÖÿπÿßŸã.\",\n",
        "                \"expected_intent\": \"ÿßŸÑÿ™ÿÆÿ∑Ÿäÿ∑ ŸÑÿ±ÿ≠ŸÑÿ© ŸÖÿ¥Ÿä ŸÖÿ≥ÿ™ŸÇÿ®ŸÑŸäÿ© ŸÖÿπÿßŸã\"\n",
        "            }\n",
        "        }\n",
        "    })\n",
        "    \n",
        "    return test_cases\n",
        "\n",
        "# Create systematic test data\n",
        "systematic_test_data = create_systematic_test_data()\n",
        "\n",
        "# Convert to structured format for systematic analysis\n",
        "test_items = []\n",
        "for case in systematic_test_data:\n",
        "    for lang_code, lang_data in case[\"languages\"].items():\n",
        "        # Create test items for each task type\n",
        "        for task in tasks_df[\"task_id\"].values:\n",
        "            expected_output = \"\"\n",
        "            if task == \"dialogue_classification\":\n",
        "                expected_output = lang_data[\"expected_classification\"]\n",
        "            elif task == \"dialogue_summarization\": \n",
        "                expected_output = lang_data[\"expected_summary\"]\n",
        "            elif task == \"intent_extraction\":\n",
        "                expected_output = lang_data[\"expected_intent\"]\n",
        "            \n",
        "            test_items.append({\n",
        "                \"item_id\": f\"{case['case_id']}_{lang_code}_{task}\",\n",
        "                \"case_id\": case[\"case_id\"],\n",
        "                \"language_code\": lang_code,\n",
        "                \"task_id\": task,\n",
        "                \"domain\": case[\"domain\"],\n",
        "                \"complexity\": case[\"complexity\"], \n",
        "                \"cultural_context\": case[\"cultural_context\"],\n",
        "                \"input_text\": lang_data[\"dialogue\"],\n",
        "                \"expected_output\": expected_output,\n",
        "                \"language_name\": languages_df[languages_df[\"code\"] == lang_code][\"name\"].iloc[0]\n",
        "            })\n",
        "\n",
        "# Convert to DataFrame for systematic analysis\n",
        "test_items_df = pd.DataFrame(test_items)\n",
        "\n",
        "print(\"üìä SYSTEMATIC TEST DATA FRAMEWORK\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\\\nüéØ Test Coverage:\")\n",
        "print(f\"   Test cases: {len(systematic_test_data)}\")\n",
        "print(f\"   Languages per case: {len(systematic_test_data[0]['languages'])}\")\n",
        "print(f\"   Tasks per language: {len(tasks_df)}\")\n",
        "print(f\"   Total test items: {len(test_items_df)}\")\n",
        "\n",
        "print(f\"\\\\nüìã Test Distribution:\")\n",
        "test_summary = test_items_df.groupby([\"language_name\", \"task_id\"]).size().unstack(fill_value=0)\n",
        "display(test_summary)\n",
        "\n",
        "print(f\"\\\\n‚úÖ Systematic test framework ready!\")\n",
        "print(f\"   All languages have parallel test cases for fair comparison\")\n",
        "print(f\"   Multiple domains and complexity levels covered\")\n",
        "print(f\"   Cultural contexts systematically varied\")\n",
        "\n",
        "# Show sample test items\n",
        "print(f\"\\\\nüîç Sample Test Items (first 3):\")\n",
        "display(test_items_df[[\"item_id\", \"language_name\", \"task_id\", \"domain\", \"complexity\"]].head(6))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c81dfb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4800a6d2",
      "metadata": {},
      "source": [
        "## 2. ü§ñ Systematic Model Loading and Prompt Engineering Framework\n",
        "\n",
        "**Strategic Mission:** Systematically evaluate multiple model access patterns and prompt strategies across languages.\n",
        "\n",
        "### 2.1 üî• Advanced Model Loading with Performance Tracking\n",
        "\n",
        "**Systematic Model Evaluation:** Compare local, API, and hosted approaches with quantitative metrics.\n",
        "\n",
        "| **Access Pattern** | **Pros** | **Cons** | **Cost Analysis** | **Performance Expectation** |\n",
        "|-------------------|----------|----------|-------------------|----------------------------|\n",
        "| **üî• Local mT5-Base** | Privacy, offline, fine-tunable | 2-4GB RAM, setup time | Hardware only (~$500-2000) | Good multilingual, customizable |\n",
        "| **‚òÅÔ∏è GPT-4 API** | State-of-art, minimal setup | $0.01-0.03/1K tokens | ~$50-200/month typical use | Excellent English, good multilingual |\n",
        "| **üåê Hosted Free (Colab)** | Zero cost, easy access | 12hr limits, GPU competition | Free (with limits) | Variable quality, good for learning |\n",
        "| **üè¢ Claude API** | Strong reasoning, safety | Limited multilingual support | Similar to GPT-4 | Excellent reasoning, English-focused |\n",
        "\n",
        "### 2.2 üìä Systematic Task Framework\n",
        "\n",
        "**Our systematic evaluation covers:**\n",
        "1. **üìä Dialogue Classification:** Business/social/support categorization with cultural context\n",
        "2. **‚úçÔ∏è Dialogue Summarization:** Concise, culturally-appropriate summaries\n",
        "3. **üéØ Intent Extraction:** Action items and next steps identification\n",
        "4. **üß† Chain-of-Thought Reasoning:** Step-by-step multilingual reasoning evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0b01478",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Systematic Model Loading and Configuration Framework\n",
        "# Advanced setup with multiple model support and performance tracking\n",
        "\n",
        "class SystematicModelManager:\n",
        "    \"\"\"\n",
        "    Advanced model management with systematic evaluation and performance tracking\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.performance_metrics = []\n",
        "        self.current_model = None\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "    def load_local_model(self, model_name: str = \"google/mt5-small\"):\n",
        "        \"\"\"Load local multilingual model with systematic tracking\"\"\"\n",
        "        print(f\"üîÑ Loading local model: {model_name}\")\n",
        "        print(f\"‚è±Ô∏è  This may take 2-3 minutes on first run...\")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "            model = model.to(self.device)\n",
        "            \n",
        "            load_time = time.time() - start_time\n",
        "            param_count = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "            \n",
        "            # Store model configuration\n",
        "            model_config = {\n",
        "                \"name\": model_name,\n",
        "                \"type\": \"local_multilingual\", \n",
        "                \"tokenizer\": tokenizer,\n",
        "                \"model\": model,\n",
        "                \"device\": str(self.device),\n",
        "                \"parameters_M\": param_count,\n",
        "                \"load_time_s\": load_time,\n",
        "                \"memory_gb\": torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
        "            }\n",
        "            \n",
        "            self.models[model_name] = model_config\n",
        "            self.current_model = model_name\n",
        "            \n",
        "            print(f\"‚úÖ Model loaded successfully:\")\n",
        "            print(f\"   Device: {self.device}\")\n",
        "            print(f\"   Parameters: {param_count:.0f}M\")\n",
        "            print(f\"   Load time: {load_time:.1f}s\")\n",
        "            if torch.cuda.is_available():\n",
        "                print(f\"   GPU memory: {model_config['memory_gb']:.1f}GB\")\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading {model_name}: {str(e)}\")\n",
        "            print(\"üí° Solutions:\")\n",
        "            print(\"   1. Try smaller model: google/mt5-small\")\n",
        "            print(\"   2. Restart runtime to free memory\")\n",
        "            print(\"   3. Use CPU-only mode\")\n",
        "            return False\n",
        "    \n",
        "    def setup_api_access(self, api_type: str = \"placeholder\"):\n",
        "        \"\"\"Setup API access (placeholder for actual API integration)\"\"\"\n",
        "        print(f\"üåê Setting up {api_type} API access...\")\n",
        "        \n",
        "        # Placeholder for API setup - in real use, add API key configuration\n",
        "        api_config = {\n",
        "            \"name\": f\"{api_type}_api\",\n",
        "            \"type\": \"api_access\",\n",
        "            \"cost_per_1k_tokens\": 0.02 if api_type == \"gpt4\" else 0.01,\n",
        "            \"setup_time_s\": 0.1,\n",
        "            \"requires_internet\": True\n",
        "        }\n",
        "        \n",
        "        self.models[f\"{api_type}_api\"] = api_config\n",
        "        print(f\"‚úÖ {api_type} API configured (placeholder)\")\n",
        "        print(f\"   Estimated cost: ${api_config['cost_per_1k_tokens']}/1K tokens\")\n",
        "        return True\n",
        "    \n",
        "    def generate_text_systematic(self, prompt: str, max_length: int = 100, \n",
        "                                temperature: float = 0.7, model_name: str = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Generate text with systematic performance tracking\n",
        "        \"\"\"\n",
        "        model_name = model_name or self.current_model\n",
        "        if not model_name or model_name not in self.models:\n",
        "            return {\"error\": \"No model loaded\", \"output\": \"\", \"metrics\": {}}\n",
        "        \n",
        "        model_config = self.models[model_name]\n",
        "        \n",
        "        if model_config[\"type\"] == \"local_multilingual\":\n",
        "            return self._generate_local(prompt, model_config, max_length, temperature)\n",
        "        elif model_config[\"type\"] == \"api_access\":\n",
        "            return self._generate_api(prompt, model_config, max_length, temperature)\n",
        "        else:\n",
        "            return {\"error\": \"Unknown model type\", \"output\": \"\", \"metrics\": {}}\n",
        "    \n",
        "    def _generate_local(self, prompt: str, model_config: Dict, max_length: int, temperature: float) -> Dict:\n",
        "        \"\"\"Generate using local model with performance tracking\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            tokenizer = model_config[\"tokenizer\"]\n",
        "            model = model_config[\"model\"]\n",
        "            \n",
        "            # Tokenization with tracking\n",
        "            inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "            input_tokens = inputs.shape[1]\n",
        "            inputs = inputs.to(self.device)\n",
        "            \n",
        "            # Generation with tracking\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs, \n",
        "                    max_length=max_length, \n",
        "                    temperature=temperature,\n",
        "                    do_sample=True, \n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "            \n",
        "            # Decode output\n",
        "            output_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "            output_tokens = outputs.shape[1] - input_tokens\n",
        "            \n",
        "            generation_time = time.time() - start_time\n",
        "            \n",
        "            # Calculate performance metrics\n",
        "            metrics = {\n",
        "                \"model_name\": model_config[\"name\"],\n",
        "                \"generation_time_ms\": generation_time * 1000,\n",
        "                \"input_tokens\": input_tokens,\n",
        "                \"output_tokens\": output_tokens,\n",
        "                \"tokens_per_second\": output_tokens / generation_time if generation_time > 0 else 0,\n",
        "                \"success\": True\n",
        "            }\n",
        "            \n",
        "            return {\n",
        "                \"output\": output_text,\n",
        "                \"metrics\": metrics,\n",
        "                \"error\": None\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"output\": \"\", \n",
        "                \"metrics\": {\"success\": False, \"error\": str(e)},\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "    \n",
        "    def _generate_api(self, prompt: str, model_config: Dict, max_length: int, temperature: float) -> Dict:\n",
        "        \"\"\"Placeholder for API generation - replace with actual API calls\"\"\"\n",
        "        return {\n",
        "            \"output\": f\"[API placeholder - would call {model_config['name']} with prompt]\",\n",
        "            \"metrics\": {\n",
        "                \"model_name\": model_config[\"name\"],\n",
        "                \"estimated_cost\": len(prompt) * model_config[\"cost_per_1k_tokens\"] / 1000,\n",
        "                \"success\": True\n",
        "            },\n",
        "            \"error\": None\n",
        "        }\n",
        "\n",
        "# Initialize systematic model manager\n",
        "print(\"üöÄ SYSTEMATIC MODEL MANAGER INITIALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model_manager = SystematicModelManager()\n",
        "\n",
        "# Option 1: Load local model (recommended for learning)\n",
        "print(\"üî• Loading local multilingual model...\")\n",
        "local_success = model_manager.load_local_model(\"google/mt5-small\")\n",
        "\n",
        "# Option 2: Setup API access (for production comparison)\n",
        "print(\"\\\\n‚òÅÔ∏è Configuring API access...\")\n",
        "api_success = model_manager.setup_api_access(\"gpt4\")\n",
        "\n",
        "# Test the systematic framework\n",
        "if local_success:\n",
        "    print(\"\\\\nüß™ TESTING SYSTEMATIC GENERATION:\")\n",
        "    test_prompt = \"Classify this dialogue: A: Let's schedule a meeting for 3pm. B: Perfect, I'll send the invite.\"\n",
        "    \n",
        "    result = model_manager.generate_text_systematic(test_prompt, max_length=50, temperature=0.1)\n",
        "    \n",
        "    if result[\"error\"]:\n",
        "        print(f\"‚ùå Test failed: {result['error']}\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Test successful:\")\n",
        "        print(f\"   Output: {result['output']}\")\n",
        "        print(f\"   Generation time: {result['metrics']['generation_time_ms']:.1f}ms\")\n",
        "        print(f\"   Tokens/second: {result['metrics']['tokens_per_second']:.1f}\")\n",
        "\n",
        "print(f\"\\\\nüìä MODEL MANAGER STATUS:\")\n",
        "print(f\"   Available models: {len(model_manager.models)}\")\n",
        "print(f\"   Current model: {model_manager.current_model}\")\n",
        "print(f\"   Device: {model_manager.device}\")\n",
        "print(f\"   ‚úÖ Ready for systematic prompt engineering experiments!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "291438cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "### 2.1 üéØ Zero-shot, Few-shot, and Chain-of-Thought Comparison\n",
        "\n",
        "# üîß Prompt engineering toolkit\n",
        "def create_zero_shot_prompt(dialogue: str, task: str = \"classification\") -> str:\n",
        "    \"\"\"Zero-shot prompt - no examples provided\"\"\"\n",
        "    if task == \"classification\":\n",
        "        return f\"\"\"Classify this dialogue into one topic: meeting, social, support, transaction, other.\n",
        "\n",
        "Dialogue: {dialogue}\n",
        "\n",
        "Topic:\"\"\"\n",
        "    else:  # QA\n",
        "        return f\"\"\"Answer the question based on the context.\n",
        "\n",
        "Context: {dialogue}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "def create_few_shot_prompt(dialogue: str, examples: list, task: str = \"classification\") -> str:\n",
        "    \"\"\"Few-shot prompt - includes examples\"\"\"\n",
        "    if task == \"classification\":\n",
        "        prompt = \"Classify dialogues into topics: meeting, social, support, transaction, other.\\n\\nExamples:\\n\\n\"\n",
        "        for ex in examples[:2]:  # Use 2 examples to avoid length issues\n",
        "            prompt += f\"Dialogue: {ex['dialogue']}\\nTopic: {ex['topic']}\\n\\n\"\n",
        "        prompt += f\"Dialogue: {dialogue}\\nTopic:\"\n",
        "        return prompt\n",
        "    else:  # QA\n",
        "        return f\"\"\"Answer questions based on context.\n",
        "\n",
        "Context: {dialogue}\n",
        "\n",
        "Answer with specific information:\"\"\"\n",
        "\n",
        "def create_chain_of_thought_prompt(context: str, question: str) -> str:\n",
        "    \"\"\"Chain-of-Thought prompt for step-by-step reasoning\"\"\"\n",
        "    return f\"\"\"Answer the question step by step based on the context.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Let me think step by step:\n",
        "1. What is the question asking?\n",
        "2. What relevant information is in the context?\n",
        "3. What is the answer?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "# üß™ Run comprehensive prompt testing\n",
        "def test_all_prompting_strategies():\n",
        "    \"\"\"Test zero-shot, few-shot, and Chain-of-Thought across languages\"\"\"\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    print(\"üéØ COMPREHENSIVE PROMPT ENGINEERING TEST\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for language, data in test_data.items():\n",
        "        print(f\"\\nüåç TESTING: {language.upper()}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Test classification\n",
        "        if data[\"classification\"]:\n",
        "            test_dialogue = data[\"classification\"][0][\"dialogue\"]\n",
        "            true_topic = data[\"classification\"][0][\"topic\"]\n",
        "            \n",
        "            print(f\"üìù Classification task: {test_dialogue[:60]}...\")\n",
        "            print(f\"üìã Expected: {true_topic}\")\n",
        "            \n",
        "            # Zero-shot classification\n",
        "            zero_prompt = create_zero_shot_prompt(test_dialogue, \"classification\")\n",
        "            if model:\n",
        "                zero_result = generate_text(zero_prompt, max_length=50, temperature=0.1)\n",
        "                print(f\"üéØ Zero-shot: {zero_result}\")\n",
        "                \n",
        "                # Few-shot classification (using English examples for transfer)\n",
        "                few_prompt = create_few_shot_prompt(test_dialogue, test_data[\"English\"][\"classification\"], \"classification\")\n",
        "                few_result = generate_text(few_prompt, max_length=50, temperature=0.1)\n",
        "                print(f\"üìö Few-shot: {few_result}\")\n",
        "                \n",
        "                results.append({\n",
        "                    \"language\": language, \"task\": \"classification\", \"method\": \"zero-shot\",\n",
        "                    \"input\": test_dialogue[:50] + \"...\", \"output\": zero_result, \"expected\": true_topic\n",
        "                })\n",
        "                results.append({\n",
        "                    \"language\": language, \"task\": \"classification\", \"method\": \"few-shot\", \n",
        "                    \"input\": test_dialogue[:50] + \"...\", \"output\": few_result, \"expected\": true_topic\n",
        "                })\n",
        "            \n",
        "        # Test QA with Chain-of-Thought\n",
        "        if data[\"qa\"][\"questions\"]:\n",
        "            context = data[\"qa\"][\"context\"]\n",
        "            question = data[\"qa\"][\"questions\"][0]\n",
        "            expected_answer = data[\"qa\"][\"answers\"][0]\n",
        "            \n",
        "            print(f\"\\\\n‚ùì QA task: {question}\")\n",
        "            print(f\"üìã Expected: {expected_answer}\")\n",
        "            \n",
        "            # Chain-of-Thought QA\n",
        "            cot_prompt = create_chain_of_thought_prompt(context, question)\n",
        "            if model:\n",
        "                cot_result = generate_text(cot_prompt, max_length=120, temperature=0.2)\n",
        "                print(f\"üß† Chain-of-Thought: {cot_result}\")\n",
        "                \n",
        "                results.append({\n",
        "                    \"language\": language, \"task\": \"qa\", \"method\": \"chain-of-thought\",\n",
        "                    \"input\": question, \"output\": cot_result, \"expected\": expected_answer\n",
        "                })\n",
        "        \n",
        "        print()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run the comprehensive test\n",
        "if model:\n",
        "    test_results = test_all_prompting_strategies()\n",
        "    print(f\"‚úÖ Completed testing across {len(test_data)} languages\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Model not available - showing prompt structure only\")\n",
        "    # Show example prompts\n",
        "    example_dialogue = \"A: Can we meet at 3pm? B: Perfect!\"\n",
        "    print(\"\\\\nüìù EXAMPLE PROMPTS:\")\n",
        "    print(\"\\\\nüéØ Zero-shot:\")\n",
        "    print(create_zero_shot_prompt(example_dialogue))\n",
        "    print(\"\\\\nüìö Few-shot structure:\")\n",
        "    print(create_few_shot_prompt(example_dialogue, [{\"dialogue\": \"Example\", \"topic\": \"meeting\"}])[:200] + \"...\")\n",
        "\n",
        "### 2.2 üìä Evaluation Framework\n",
        "\n",
        "def create_evaluation_rubric():\n",
        "    \"\"\"Evaluation framework for model outputs\"\"\"\n",
        "    return {\n",
        "        \"correctness\": {\n",
        "            \"1\": \"Completely wrong\", \"2\": \"Partially wrong\", \"3\": \"Mostly right\", \n",
        "            \"4\": \"Right answer\", \"5\": \"Perfect with reasoning\"\n",
        "        },\n",
        "        \"fluency\": {\n",
        "            \"1\": \"Unnatural/errors\", \"2\": \"Awkward phrasing\", \"3\": \"Acceptable\", \n",
        "            \"4\": \"Good language\", \"5\": \"Native-like\"\n",
        "        },\n",
        "        \"cultural_appropriateness\": {\n",
        "            \"1\": \"Inappropriate\", \"2\": \"Questionable\", \"3\": \"Neutral\", \n",
        "            \"4\": \"Appropriate\", \"5\": \"Culturally aware\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "def evaluate_output(output: str, expected: str, language: str, task: str, method: str):\n",
        "    \"\"\"Template for manual evaluation\"\"\"\n",
        "    return {\n",
        "        \"output\": output,\n",
        "        \"expected\": expected,\n",
        "        \"language\": language,\n",
        "        \"task\": task,\n",
        "        \"method\": method,\n",
        "        \"correctness_score\": 0,  # Fill in 1-5\n",
        "        \"fluency_score\": 0,      # Fill in 1-5\n",
        "        \"cultural_score\": 0,     # Fill in 1-5\n",
        "        \"notes\": \"\",            # Your observations\n",
        "        \"improvement_suggestions\": \"\"\n",
        "    }\n",
        "\n",
        "print(\"\\\\nüìã EVALUATION FRAMEWORK\")\n",
        "print(\"=\"*40)\n",
        "rubric = create_evaluation_rubric()\n",
        "for dimension, scale in rubric.items():\n",
        "    print(f\"\\\\n{dimension.upper()}:\")\n",
        "    for score, description in scale.items():\n",
        "        print(f\"  {score}: {description}\")\n",
        "\n",
        "print(f\"\\\\nüéØ YOUR TURN: Evaluate the outputs above using this 1-5 scale\")\n",
        "print(\"üí° Focus on how well each method works for your target language\")\n",
        "\n",
        "### 2.3 üí¨ Discussion Questions and Key Takeaways\n",
        "\n",
        "discussion_guide = \"\"\"\n",
        "ü§î REFLECTION QUESTIONS:\n",
        "\n",
        "1. **Cross-language Performance:**\n",
        "   - Which prompting method worked best for your target language?\n",
        "   - How did performance differ between English and your language?\n",
        "\n",
        "2. **Method Comparison:** \n",
        "   - When did few-shot examples help vs. hurt?\n",
        "   - How effective was Chain-of-Thought reasoning in non-English?\n",
        "\n",
        "3. **Cultural Considerations:**\n",
        "   - What cultural assumptions did you notice in outputs?\n",
        "   - How would you adapt prompts for your cultural context?\n",
        "\n",
        "4. **Practical Applications:**\n",
        "   - Which approach would you use in production?\n",
        "   - What are the trade-offs between methods?\n",
        "\n",
        "üìù ACTION ITEMS:\n",
        "‚ñ° Document 3 key insights about your target language\n",
        "‚ñ° Identify best prompting strategies for your use case  \n",
        "‚ñ° Note major challenges needing further research\n",
        "‚ñ° Plan next steps for your project\n",
        "\n",
        "üéØ KEY TAKEAWAYS:\n",
        "‚Ä¢ Prompt structure matters more than complexity\n",
        "‚Ä¢ Cultural context significantly impacts performance  \n",
        "‚Ä¢ Few-shot examples can bridge language gaps effectively\n",
        "‚Ä¢ Chain-of-Thought helps with reasoning across languages\n",
        "‚Ä¢ Evaluation must consider cultural appropriateness\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\\\n\" + discussion_guide)\n",
        "\n",
        "print(\"\\\\nüéâ CONGRATULATIONS!\")\n",
        "print(\"You've completed hands-on prompt engineering for low-resource languages!\")\n",
        "print(\"Use these techniques responsibly and keep experimenting! üöÄ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "903ae0af",
      "metadata": {},
      "source": [
        "### 2.2 üÜì FREE API Setup for Colab Students\n",
        "\n",
        "**üéØ Student-Friendly Free Options:**\n",
        "\n",
        "| **Service** | **Free Tier** | **Models Available** | **Setup Difficulty** | **Recommended For** |\n",
        "|-------------|---------------|---------------------|---------------------|-------------------|\n",
        "| **ü§ó Hugging Face** | 1000 requests/month | Llama-2, CodeLlama, Mistral | Easy | Learning & experiments |\n",
        "| **üü¢ Google Gemini** | 60 requests/minute | Gemini-1.5-flash, Gemini-pro | Easy | High-quality outputs |\n",
        "| **üî• Local mT5** | GPU memory only | mT5-small/base | Medium | Privacy & customization |\n",
        "\n",
        "**‚úÖ All options work perfectly on Google Colab GPU!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c78447dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üÜì FREE API SETUP FOR COLAB STUDENTS\n",
        "# Choose your preferred free option!\n",
        "\n",
        "import requests\n",
        "import os\n",
        "from typing import Dict, List\n",
        "\n",
        "class FreeAPIManager:\n",
        "    \"\"\"Manage multiple free API services for students\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.available_services = {}\n",
        "        \n",
        "    def setup_huggingface_free(self, hf_token: str = None):\n",
        "        \"\"\"Setup Hugging Face Inference API (1000 free requests/month)\"\"\"\n",
        "        if not hf_token:\n",
        "            print(\"üîë Get your FREE Hugging Face token:\")\n",
        "            print(\"   1. Visit: https://huggingface.co/settings/tokens\")\n",
        "            print(\"   2. Create a token with 'Read' permission\")\n",
        "            print(\"   3. Paste it when prompted\")\n",
        "            hf_token = input(\"Enter your HF token: \").strip()\n",
        "        \n",
        "        self.available_services[\"huggingface\"] = {\n",
        "            \"token\": hf_token,\n",
        "            \"api_url\": \"https://api-inference.huggingface.co/models/\",\n",
        "            \"models\": [\"microsoft/DialoGPT-medium\", \"google/flan-t5-base\", \"meta-llama/Llama-2-7b-chat-hf\"],\n",
        "            \"cost\": \"Free (1000 requests/month)\",\n",
        "            \"setup\": True\n",
        "        }\n",
        "        print(\"‚úÖ Hugging Face API configured!\")\n",
        "        return True\n",
        "    \n",
        "    def setup_gemini_free(self, api_key: str = None):\n",
        "        \"\"\"Setup Google Gemini API (60 free requests/minute)\"\"\"\n",
        "        if not api_key:\n",
        "            print(\"üîë Get your FREE Google AI Studio API key:\")\n",
        "            print(\"   1. Visit: https://makersuite.google.com/app/apikey\")\n",
        "            print(\"   2. Create a new API key\")\n",
        "            print(\"   3. Paste it when prompted\")\n",
        "            api_key = input(\"Enter your Gemini API key: \").strip()\n",
        "        \n",
        "        self.available_services[\"gemini\"] = {\n",
        "            \"api_key\": api_key,\n",
        "            \"models\": [\"gemini-1.5-flash\", \"gemini-1.5-pro\"],\n",
        "            \"cost\": \"Free (60 requests/minute)\",\n",
        "            \"setup\": True\n",
        "        }\n",
        "        print(\"‚úÖ Google Gemini API configured!\")\n",
        "        return True\n",
        "    \n",
        "    def query_huggingface(self, model_name: str, prompt: str, max_length: int = 100) -> Dict:\n",
        "        \"\"\"Query Hugging Face model with free API\"\"\"\n",
        "        if \"huggingface\" not in self.available_services:\n",
        "            return {\"error\": \"Hugging Face not setup. Run setup_huggingface_free() first\"}\n",
        "        \n",
        "        api_url = self.available_services[\"huggingface\"][\"api_url\"] + model_name\n",
        "        headers = {\"Authorization\": f\"Bearer {self.available_services['huggingface']['token']}\"}\n",
        "        \n",
        "        payload = {\n",
        "            \"inputs\": prompt,\n",
        "            \"parameters\": {\"max_length\": max_length, \"temperature\": 0.7}\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = requests.post(api_url, headers=headers, json=payload)\n",
        "            result = response.json()\n",
        "            \n",
        "            if isinstance(result, list) and len(result) > 0:\n",
        "                return {\n",
        "                    \"output\": result[0].get(\"generated_text\", \"\").replace(prompt, \"\").strip(),\n",
        "                    \"model\": model_name,\n",
        "                    \"service\": \"huggingface\",\n",
        "                    \"success\": True\n",
        "                }\n",
        "            else:\n",
        "                return {\"error\": f\"Unexpected response: {result}\", \"success\": False}\n",
        "                \n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"success\": False}\n",
        "    \n",
        "    def query_gemini(self, prompt: str, model: str = \"gemini-1.5-flash\") -> Dict:\n",
        "        \"\"\"Query Gemini with free API (placeholder - would need google-generativeai package)\"\"\"\n",
        "        if \"gemini\" not in self.available_services:\n",
        "            return {\"error\": \"Gemini not setup. Run setup_gemini_free() first\"}\n",
        "        \n",
        "        # This would require: pip install google-generativeai\n",
        "        # For now, showing the structure students would use\n",
        "        return {\n",
        "            \"output\": f\"[Gemini response - install google-generativeai package to use]\",\n",
        "            \"model\": model,\n",
        "            \"service\": \"gemini\", \n",
        "            \"success\": True,\n",
        "            \"note\": \"Install: pip install google-generativeai\"\n",
        "        }\n",
        "\n",
        "# Initialize free API manager\n",
        "free_api = FreeAPIManager()\n",
        "\n",
        "print(\"üÜì FREE API OPTIONS FOR STUDENTS\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Choose one of these FREE options:\")\n",
        "print()\n",
        "print(\"Option 1: ü§ó Hugging Face (Recommended for beginners)\")\n",
        "print(\"   - 1000 free requests per month\")\n",
        "print(\"   - Multiple models available\")\n",
        "print(\"   - Easy to get started\")\n",
        "print()\n",
        "print(\"Option 2: üü¢ Google Gemini\")  \n",
        "print(\"   - 60 requests per minute (very generous!)\")\n",
        "print(\"   - High-quality responses\")\n",
        "print(\"   - Excellent for production testing\")\n",
        "print()\n",
        "print(\"Option 3: üî• Local Model (Already loaded above)\")\n",
        "print(\"   - Completely free (uses Colab GPU)\")\n",
        "print(\"   - Works offline\")\n",
        "print(\"   - Perfect for learning\")\n",
        "\n",
        "print(\"\\nüí° SETUP INSTRUCTIONS:\")\n",
        "print(\"   Uncomment ONE of these lines to setup your preferred API:\")\n",
        "print(\"   # free_api.setup_huggingface_free()  # For Hugging Face\")\n",
        "print(\"   # free_api.setup_gemini_free()       # For Google Gemini\")\n",
        "print(\"\\n‚úÖ All services work perfectly on Google Colab!\")\n",
        "\n",
        "# Test function for whichever API is setup\n",
        "def test_free_api(prompt: str = \"Hello! How are you today?\"):\n",
        "    \"\"\"Test whichever API service is configured\"\"\"\n",
        "    if \"huggingface\" in free_api.available_services:\n",
        "        print(\"Testing Hugging Face API...\")\n",
        "        result = free_api.query_huggingface(\"microsoft/DialoGPT-medium\", prompt)\n",
        "        print(f\"Result: {result}\")\n",
        "    elif \"gemini\" in free_api.available_services:\n",
        "        print(\"Testing Gemini API...\")\n",
        "        result = free_api.query_gemini(prompt)\n",
        "        print(f\"Result: {result}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No API configured yet. Run setup first!\")\n",
        "\n",
        "print(\"\\nüß™ After setup, test with: test_free_api()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f23b8be9",
      "metadata": {},
      "source": [
        "### 2.3 üéØ Self-Consistency Prompting (MISSING TECHNIQUE)\n",
        "\n",
        "**Self-Consistency** is a powerful technique where we generate multiple responses to the same prompt and choose the most consistent answer. This significantly improves accuracy, especially for reasoning tasks.\n",
        "\n",
        "**üî¨ How Self-Consistency Works:**\n",
        "1. Generate multiple responses (typically 3-5) to the same prompt\n",
        "2. Analyze the consistency across responses  \n",
        "3. Choose the most frequent/consistent answer\n",
        "4. Particularly effective for mathematical reasoning and factual questions\n",
        "\n",
        "**üìä Research shows 10-20% accuracy improvement with self-consistency!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f001db8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß† SELF-CONSISTENCY PROMPTING IMPLEMENTATION\n",
        "# Generate multiple responses and find the most consistent answer\n",
        "\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "class SelfConsistencyEngine:\n",
        "    \"\"\"\n",
        "    Implement self-consistency prompting for improved accuracy\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_manager):\n",
        "        self.model_manager = model_manager\n",
        "        \n",
        "    def generate_multiple_responses(self, prompt: str, num_responses: int = 5, \n",
        "                                  temperature: float = 0.8) -> List[Dict]:\n",
        "        \"\"\"Generate multiple responses for self-consistency evaluation\"\"\"\n",
        "        responses = []\n",
        "        \n",
        "        print(f\"üîÑ Generating {num_responses} responses for self-consistency...\")\n",
        "        \n",
        "        for i in range(num_responses):\n",
        "            print(f\"   Response {i+1}/{num_responses}...\", end=\"\")\n",
        "            \n",
        "            result = self.model_manager.generate_text_systematic(\n",
        "                prompt, \n",
        "                max_length=100, \n",
        "                temperature=temperature\n",
        "            )\n",
        "            \n",
        "            if result[\"error\"]:\n",
        "                print(f\" ‚ùå Error\")\n",
        "                continue\n",
        "                \n",
        "            responses.append({\n",
        "                \"response_id\": i+1,\n",
        "                \"output\": result[\"output\"],\n",
        "                \"metrics\": result[\"metrics\"]\n",
        "            })\n",
        "            print(f\" ‚úÖ\")\n",
        "            time.sleep(0.1)  # Small delay to avoid overwhelming APIs\n",
        "            \n",
        "        return responses\n",
        "    \n",
        "    def extract_answers(self, responses: List[Dict], task_type: str = \"classification\") -> List[str]:\n",
        "        \"\"\"Extract the core answers from responses for comparison\"\"\"\n",
        "        answers = []\n",
        "        \n",
        "        for response in responses:\n",
        "            output = response[\"output\"].strip()\n",
        "            \n",
        "            if task_type == \"classification\":\n",
        "                # Extract the classification result (first word or after \":\")\n",
        "                if \":\" in output:\n",
        "                    answer = output.split(\":\")[-1].strip().split()[0].lower()\n",
        "                else:\n",
        "                    answer = output.split()[0].lower()\n",
        "                    \n",
        "            elif task_type == \"number\":\n",
        "                # Extract numerical answers\n",
        "                import re\n",
        "                numbers = re.findall(r'\\d+', output)\n",
        "                answer = numbers[0] if numbers else \"no_number\"\n",
        "                \n",
        "            else:  # general text\n",
        "                # Use first significant word/phrase\n",
        "                answer = output.split('.')[0].strip()[:50]\n",
        "            \n",
        "            answers.append(answer)\n",
        "            \n",
        "        return answers\n",
        "    \n",
        "    def find_consensus(self, answers: List[str]) -> Dict:\n",
        "        \"\"\"Find the most consistent answer across multiple responses\"\"\"\n",
        "        if not answers:\n",
        "            return {\"consensus\": None, \"confidence\": 0, \"distribution\": {}}\n",
        "        \n",
        "        # Count frequency of each answer\n",
        "        answer_counts = Counter(answers)\n",
        "        most_common_answer, max_count = answer_counts.most_common(1)[0]\n",
        "        \n",
        "        # Calculate confidence as percentage of responses\n",
        "        confidence = max_count / len(answers)\n",
        "        \n",
        "        return {\n",
        "            \"consensus\": most_common_answer,\n",
        "            \"confidence\": confidence,\n",
        "            \"distribution\": dict(answer_counts),\n",
        "            \"total_responses\": len(answers)\n",
        "        }\n",
        "    \n",
        "    def self_consistent_query(self, prompt: str, task_type: str = \"classification\", \n",
        "                            num_responses: int = 5) -> Dict:\n",
        "        \"\"\"\n",
        "        Perform complete self-consistency evaluation\n",
        "        \"\"\"\n",
        "        print(f\"üéØ SELF-CONSISTENCY EVALUATION\")\n",
        "        print(f\"   Task: {task_type}\")\n",
        "        print(f\"   Responses: {num_responses}\")\n",
        "        print(f\"   Prompt: {prompt[:100]}...\")\n",
        "        print()\n",
        "        \n",
        "        # Generate multiple responses\n",
        "        responses = self.generate_multiple_responses(prompt, num_responses)\n",
        "        \n",
        "        if len(responses) < 2:\n",
        "            return {\"error\": \"Need at least 2 successful responses for consensus\"}\n",
        "        \n",
        "        # Extract answers for comparison\n",
        "        answers = self.extract_answers(responses, task_type)\n",
        "        \n",
        "        # Find consensus\n",
        "        consensus_result = self.find_consensus(answers)\n",
        "        \n",
        "        print(f\"\\\\nüìä SELF-CONSISTENCY RESULTS:\")\n",
        "        print(f\"   Consensus: {consensus_result['consensus']}\")\n",
        "        print(f\"   Confidence: {consensus_result['confidence']:.1%}\")\n",
        "        print(f\"   Distribution: {consensus_result['distribution']}\")\n",
        "        \n",
        "        return {\n",
        "            \"consensus\": consensus_result,\n",
        "            \"individual_responses\": responses,\n",
        "            \"extracted_answers\": answers,\n",
        "            \"success\": True\n",
        "        }\n",
        "\n",
        "# Initialize self-consistency engine (if model manager is available)\n",
        "if 'model_manager' in locals() and model_manager.current_model:\n",
        "    sc_engine = SelfConsistencyEngine(model_manager)\n",
        "    \n",
        "    print(\"üß† SELF-CONSISTENCY ENGINE READY!\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Demo with a classification task\n",
        "    demo_prompt = \"\"\"Classify this dialogue type: meeting, social, support, or transaction.\n",
        "\n",
        "Dialogue: A: I need help with my password reset. B: I can help you with that. Let me send you a reset link.\n",
        "\n",
        "Classification:\"\"\"\n",
        "    \n",
        "    print(\"üß™ DEMO: Self-Consistency vs Single Response\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Single response (traditional)\n",
        "    single_result = model_manager.generate_text_systematic(demo_prompt, temperature=0.1)\n",
        "    print(f\"üî∏ Single response: {single_result.get('output', 'Error')}\")\n",
        "    \n",
        "    # Self-consistency (multiple responses)\n",
        "    print(\"\\\\nüî∏ Self-consistency evaluation:\")\n",
        "    sc_result = sc_engine.self_consistent_query(demo_prompt, \"classification\", 3)\n",
        "    \n",
        "    if sc_result[\"success\"]:\n",
        "        print(f\"\\\\n‚úÖ IMPROVEMENT WITH SELF-CONSISTENCY:\")\n",
        "        print(f\"   Single: {single_result.get('output', 'Error')}\")\n",
        "        print(f\"   Consensus: {sc_result['consensus']['consensus']}\")\n",
        "        print(f\"   Confidence: {sc_result['consensus']['confidence']:.1%}\")\n",
        "        \n",
        "        if sc_result['consensus']['confidence'] >= 0.6:\n",
        "            print(f\"   üéØ High confidence - reliable answer!\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Low confidence - may need more responses or prompt tuning\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Model manager not available - showing self-consistency concept only\")\n",
        "    print(\"\\\\nüî¨ Self-Consistency Process:\")\n",
        "    print(\"1. Generate 3-5 responses with temperature > 0.5\")\n",
        "    print(\"2. Extract core answers from each response\") \n",
        "    print(\"3. Count frequency of each answer\")\n",
        "    print(\"4. Choose most frequent answer as consensus\")\n",
        "    print(\"5. Calculate confidence as agreement percentage\")\n",
        "\n",
        "print(\"\\\\nüí° WHEN TO USE SELF-CONSISTENCY:\")\n",
        "print(\"   ‚úÖ Mathematical reasoning tasks\")\n",
        "print(\"   ‚úÖ Factual questions with definitive answers\")\n",
        "print(\"   ‚úÖ Classification tasks\")\n",
        "print(\"   ‚úÖ When accuracy is more important than speed\")\n",
        "print(\"   ‚ùå Creative writing tasks\")\n",
        "print(\"   ‚ùå Open-ended discussions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2edc2bc",
      "metadata": {},
      "source": [
        "### 2.4 üéõÔ∏è LLM Hyperparameters Deep Dive (EXPANDED COVERAGE)\n",
        "\n",
        "**Understanding hyperparameters is crucial for effective prompt engineering!**\n",
        "\n",
        "| **Parameter** | **Range** | **Effect** | **Use When** | **Avoid When** |\n",
        "|---------------|-----------|------------|--------------|----------------|\n",
        "| **üå°Ô∏è Temperature** | 0.0-2.0 | Controls randomness | Creative tasks (0.7-1.2) | Factual Q&A (0.0-0.3) |\n",
        "| **üéØ Top-p** | 0.1-1.0 | Nucleus sampling | Balanced control (0.8-0.95) | Extreme creativity or determinism |\n",
        "| **üî¢ Top-k** | 1-100 | Limits vocabulary | Focused domains (10-40) | Open conversations (high k) |\n",
        "| **üìè Max Length** | 1-4096+ | Output length limit | Specific formats | Open exploration |\n",
        "| **üîÅ Repetition Penalty** | 0.8-1.5 | Reduces repetition | Avoiding loops (1.1-1.3) | Poetry/patterns (‚â§1.0) |\n",
        "\n",
        "**üéØ Optimal Settings by Task:**\n",
        "- **Factual Q&A**: temp=0.1, top_p=0.9, max_length=100\n",
        "- **Creative Writing**: temp=0.8, top_p=0.95, max_length=500\n",
        "- **Code Generation**: temp=0.2, top_p=0.9, max_length=200\n",
        "- **Classification**: temp=0.0, top_p=0.8, max_length=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "654d1335",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß™ HYPERPARAMETER EXPERIMENTATION FRAMEWORK\n",
        "# Systematic exploration of LLM hyperparameters\n",
        "\n",
        "class HyperparameterExplorer:\n",
        "    \"\"\"\n",
        "    Systematically test different hyperparameter combinations\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_manager):\n",
        "        self.model_manager = model_manager\n",
        "        self.results_df = pd.DataFrame()\n",
        "        \n",
        "    def create_hyperparameter_grid(self, task_type: str = \"classification\") -> List[Dict]:\n",
        "        \"\"\"Create systematic hyperparameter combinations for testing\"\"\"\n",
        "        \n",
        "        if task_type == \"classification\":\n",
        "            return [\n",
        "                {\"temperature\": 0.0, \"max_length\": 20, \"label\": \"Deterministic\"},\n",
        "                {\"temperature\": 0.3, \"max_length\": 20, \"label\": \"Low creativity\"},\n",
        "                {\"temperature\": 0.7, \"max_length\": 20, \"label\": \"Balanced\"},\n",
        "                {\"temperature\": 1.0, \"max_length\": 20, \"label\": \"Creative\"},\n",
        "            ]\n",
        "        elif task_type == \"generation\":\n",
        "            return [\n",
        "                {\"temperature\": 0.1, \"max_length\": 100, \"label\": \"Conservative\"}, \n",
        "                {\"temperature\": 0.5, \"max_length\": 100, \"label\": \"Moderate\"},\n",
        "                {\"temperature\": 0.8, \"max_length\": 100, \"label\": \"Creative\"},\n",
        "                {\"temperature\": 1.2, \"max_length\": 100, \"label\": \"Very creative\"},\n",
        "            ]\n",
        "        else:\n",
        "            return [\n",
        "                {\"temperature\": 0.2, \"max_length\": 50, \"label\": \"Default low\"},\n",
        "                {\"temperature\": 0.7, \"max_length\": 50, \"label\": \"Default balanced\"},\n",
        "            ]\n",
        "    \n",
        "    def test_hyperparameter_grid(self, prompt: str, task_type: str = \"classification\", \n",
        "                                runs_per_config: int = 3) -> pd.DataFrame:\n",
        "        \"\"\"Test systematic combinations of hyperparameters\"\"\"\n",
        "        \n",
        "        param_grid = self.create_hyperparameter_grid(task_type)\n",
        "        results = []\n",
        "        \n",
        "        print(f\"üß™ HYPERPARAMETER GRID SEARCH\")\n",
        "        print(f\"   Prompt: {prompt[:50]}...\")\n",
        "        print(f\"   Configurations: {len(param_grid)}\")\n",
        "        print(f\"   Runs per config: {runs_per_config}\")\n",
        "        print(f\"   Total experiments: {len(param_grid) * runs_per_config}\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        for i, config in enumerate(param_grid):\n",
        "            print(f\"\\\\nüéõÔ∏è  Config {i+1}/{len(param_grid)}: {config['label']}\")\n",
        "            print(f\"   Temperature: {config['temperature']}\")\n",
        "            print(f\"   Max length: {config['max_length']}\")\n",
        "            \n",
        "            config_results = []\n",
        "            \n",
        "            for run in range(runs_per_config):\n",
        "                print(f\"   Run {run+1}/{runs_per_config}...\", end=\"\")\n",
        "                \n",
        "                result = self.model_manager.generate_text_systematic(\n",
        "                    prompt,\n",
        "                    max_length=config[\"max_length\"],\n",
        "                    temperature=config[\"temperature\"]\n",
        "                )\n",
        "                \n",
        "                if result[\"error\"]:\n",
        "                    print(\" ‚ùå\")\n",
        "                    continue\n",
        "                \n",
        "                # Store systematic results\n",
        "                results.append({\n",
        "                    \"config_id\": i,\n",
        "                    \"config_label\": config[\"label\"],\n",
        "                    \"temperature\": config[\"temperature\"],\n",
        "                    \"max_length\": config[\"max_length\"],\n",
        "                    \"run_id\": run,\n",
        "                    \"output\": result[\"output\"],\n",
        "                    \"generation_time_ms\": result[\"metrics\"].get(\"generation_time_ms\", 0),\n",
        "                    \"output_tokens\": result[\"metrics\"].get(\"output_tokens\", 0),\n",
        "                    \"output_length\": len(result[\"output\"]),\n",
        "                    \"timestamp\": time.time()\n",
        "                })\n",
        "                \n",
        "                config_results.append(result[\"output\"])\n",
        "                print(\" ‚úÖ\")\n",
        "            \n",
        "            # Show variety within this configuration\n",
        "            if config_results:\n",
        "                unique_outputs = len(set(config_results))\n",
        "                print(f\"   Unique outputs: {unique_outputs}/{len(config_results)}\")\n",
        "                if len(config_results) > 1:\n",
        "                    print(f\"   Sample outputs:\")\n",
        "                    for j, output in enumerate(config_results[:2]):\n",
        "                        print(f\"     {j+1}: {output[:60]}...\")\n",
        "        \n",
        "        # Convert to DataFrame for analysis\n",
        "        results_df = pd.DataFrame(results)\n",
        "        self.results_df = pd.concat([self.results_df, results_df], ignore_index=True)\n",
        "        \n",
        "        return results_df\n",
        "    \n",
        "    def analyze_hyperparameter_effects(self, results_df: pd.DataFrame):\n",
        "        \"\"\"Analyze the effects of different hyperparameter settings\"\"\"\n",
        "        \n",
        "        print(f\"\\\\nüìä HYPERPARAMETER ANALYSIS\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        # Group by configuration\n",
        "        config_analysis = results_df.groupby(['config_label', 'temperature']).agg({\n",
        "            'output_length': ['mean', 'std'],\n",
        "            'generation_time_ms': ['mean', 'std'],\n",
        "            'output': lambda x: len(set(x))  # Unique outputs (diversity)\n",
        "        }).round(2)\n",
        "        \n",
        "        config_analysis.columns = ['Avg_Length', 'Std_Length', 'Avg_Time_ms', 'Std_Time_ms', 'Diversity']\n",
        "        \n",
        "        print(\"\\\\nüéØ Configuration Performance:\")\n",
        "        display(config_analysis)\n",
        "        \n",
        "        # Temperature effect analysis\n",
        "        print(f\"\\\\nüå°Ô∏è TEMPERATURE EFFECTS:\")\n",
        "        temp_effects = results_df.groupby('temperature').agg({\n",
        "            'output_length': 'mean',\n",
        "            'output': lambda x: len(set(x)) / len(x)  # Diversity ratio\n",
        "        }).round(3)\n",
        "        \n",
        "        for temp, row in temp_effects.iterrows():\n",
        "            diversity_pct = row['output'] * 100\n",
        "            print(f\"   Temperature {temp}: Avg length {row['output_length']:.0f}, Diversity {diversity_pct:.0f}%\")\n",
        "        \n",
        "        # Visualization if matplotlib is available\n",
        "        try:\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "            \n",
        "            # Length vs Temperature\n",
        "            results_df.groupby('temperature')['output_length'].mean().plot(kind='bar', ax=ax1)\n",
        "            ax1.set_title('Output Length vs Temperature')\n",
        "            ax1.set_ylabel('Average Output Length')\n",
        "            \n",
        "            # Generation time vs Temperature  \n",
        "            results_df.groupby('temperature')['generation_time_ms'].mean().plot(kind='bar', ax=ax2)\n",
        "            ax2.set_title('Generation Time vs Temperature')\n",
        "            ax2.set_ylabel('Average Time (ms)')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "        except:\n",
        "            print(\"   (Visualization requires matplotlib)\")\n",
        "        \n",
        "        return config_analysis\n",
        "\n",
        "# Initialize hyperparameter explorer (if model available)\n",
        "if 'model_manager' in locals() and model_manager.current_model:\n",
        "    hp_explorer = HyperparameterExplorer(model_manager)\n",
        "    \n",
        "    print(\"üéõÔ∏è HYPERPARAMETER EXPLORER READY!\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Demo hyperparameter experimentation\n",
        "    demo_prompt = \"Classify this conversation: A: Can you help me reset my password? B: Of course, I'll send you a link.\"\n",
        "    \n",
        "    print(\"\\\\nüß™ DEMO: Hyperparameter Grid Search\")\n",
        "    print(\"Testing different temperature settings...\")\n",
        "    \n",
        "    # Run hyperparameter grid search\n",
        "    hp_results = hp_explorer.test_hyperparameter_grid(\n",
        "        demo_prompt, \n",
        "        task_type=\"classification\",\n",
        "        runs_per_config=2  # Use 2 for demo (normally 3-5)\n",
        "    )\n",
        "    \n",
        "    # Analyze results\n",
        "    analysis = hp_explorer.analyze_hyperparameter_effects(hp_results)\n",
        "    \n",
        "    print(\"\\\\nüí° KEY INSIGHTS:\")\n",
        "    print(\"   üå°Ô∏è  Lower temperature = more consistent outputs\")\n",
        "    print(\"   üå°Ô∏è  Higher temperature = more diverse/creative outputs\") \n",
        "    print(\"   ‚è±Ô∏è  Temperature has minimal effect on generation speed\")\n",
        "    print(\"   üìè Max length controls output verbosity\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Model manager not available - showing hyperparameter concepts\")\n",
        "    \n",
        "print(\"\\\\nüéØ HYPERPARAMETER BEST PRACTICES:\")\n",
        "print(\"   üìã Classification tasks: temperature = 0.0-0.3\")\n",
        "print(\"   ‚úçÔ∏è  Creative generation: temperature = 0.7-1.0\") \n",
        "print(\"   üîç Factual Q&A: temperature = 0.0-0.2\")\n",
        "print(\"   üó£Ô∏è  Dialogue: temperature = 0.4-0.7\")\n",
        "print(\"   üßÆ Code generation: temperature = 0.1-0.4\")\n",
        "\n",
        "print(\"\\\\n‚ö° PERFORMANCE TIPS:\")\n",
        "print(\"   ‚úÖ Start with temperature=0.0 for reproducible results\")\n",
        "print(\"   ‚úÖ Increase temperature gradually for more creativity\")\n",
        "print(\"   ‚úÖ Use max_length to prevent overly long responses\")\n",
        "print(\"   ‚úÖ Test multiple runs to understand variability\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f0f14e1",
      "metadata": {},
      "source": [
        "## üéì SESSION COMPLETE: Advanced Prompt Engineering Mastery\n",
        "\n",
        "### ‚úÖ What You've Mastered Today\n",
        "\n",
        "**üèóÔ∏è Pre-trained Models:**\n",
        "- ‚úÖ Model family comparison (local vs API access)\n",
        "- ‚úÖ Systematic evaluation framework with performance tracking\n",
        "- ‚úÖ Free API integration for Colab (Hugging Face, Gemini)\n",
        "\n",
        "**üé® Prompt Engineering & Design:**\n",
        "- ‚úÖ Zero-shot prompting for immediate results\n",
        "- ‚úÖ Few-shot prompting with cross-lingual examples\n",
        "- ‚úÖ Chain-of-thought for step-by-step reasoning\n",
        "- ‚úÖ **Self-consistency for improved accuracy** (NEW!)\n",
        "\n",
        "**üéõÔ∏è LLM Hyperparameters:**\n",
        "- ‚úÖ Temperature, top-p, max_length optimization\n",
        "- ‚úÖ Task-specific parameter tuning\n",
        "- ‚úÖ Systematic hyperparameter grid search (NEW!)\n",
        "\n",
        "**üåç Low-Resource Languages:**\n",
        "- ‚úÖ Cross-lingual prompt transfer strategies\n",
        "- ‚úÖ Cultural adaptation and sensitivity evaluation\n",
        "- ‚úÖ Few-shot learning for resource-constrained languages\n",
        "\n",
        "**üìä Systematic Evaluation:**\n",
        "- ‚úÖ Research-grade methodology with pandas tracking\n",
        "- ‚úÖ Quantitative metrics and performance analysis\n",
        "- ‚úÖ Structured comparison across techniques\n",
        "\n",
        "### üöÄ Next Steps for Your Projects\n",
        "\n",
        "**üìã Immediate Actions:**\n",
        "1. **Choose your API**: Set up Hugging Face or Gemini free API\n",
        "2. **Test with your language**: Apply techniques to your specific use case\n",
        "3. **Document insights**: Record what works best for your domain\n",
        "4. **Experiment systematically**: Use the evaluation frameworks provided\n",
        "\n",
        "**üî¨ Advanced Experiments:**\n",
        "- Compare self-consistency vs single-shot for your tasks\n",
        "- Optimize hyperparameters for your specific language/domain\n",
        "- Create custom few-shot examples for your cultural context\n",
        "- Build evaluation metrics for your specific requirements\n",
        "\n",
        "### üí° Key Production Insights\n",
        "\n",
        "**üéØ For Classification Tasks:**\n",
        "- Use temperature=0.0 with self-consistency (3-5 responses)\n",
        "- Few-shot examples improve cross-lingual transfer\n",
        "- Cultural context matters more than linguistic accuracy\n",
        "\n",
        "**‚úçÔ∏è For Text Generation:**\n",
        "- Start with temperature=0.7, adjust based on creativity needs\n",
        "- Chain-of-thought improves factual accuracy significantly  \n",
        "- Self-consistency reduces hallucination in factual tasks\n",
        "\n",
        "**‚ö° For Performance:**\n",
        "- Local mT5 models: Best for privacy and customization\n",
        "- Free APIs: Excellent for learning and small-scale projects\n",
        "- Systematic evaluation saves time in the long run\n",
        "\n",
        "### üåü Remember: Ethics and Responsible AI\n",
        "\n",
        "**Always consider:**\n",
        "- Cultural sensitivity in your prompts and evaluations\n",
        "- Privacy implications of your chosen API service\n",
        "- Bias evaluation across different demographic groups\n",
        "- Environmental impact of your model choices\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Congratulations!** You now have production-ready prompt engineering skills with systematic evaluation methodology. Keep experimenting and building responsibly! üöÄ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
