{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7406ca",
   "metadata": {},
   "source": [
    "# Session 3: Fine-tuning LLMs for Low-Resource Languages üöÄ\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**üìö Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/3_tutorial.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
    "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "**Advanced Parameter-Efficient Fine-Tuning for Low-Resource Languages**\n",
    "\n",
    "Welcome to **Session 3**! You'll master the art and science of adapting pretrained LLMs to specialized tasks using systematic fine-tuning techniques, with focus on practical applications for low-resource languages.\n",
    "\n",
    "**üéØ Focus:** Parameter-efficient fine-tuning, LoRA, systematic evaluation  \n",
    "**üíª Requirements:** GPU recommended (Colab free tier sufficient)  \n",
    "**üî¨ Methodology:** Production-ready techniques with systematic comparison\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**üìã Recommended learning path:**\n",
    "1. **Session 0:** Setup and tokenization analysis ‚úÖ  \n",
    "2. **Session 1:** Systematic baseline techniques ‚úÖ\n",
    "3. **Session 2:** Systematic prompt engineering ‚úÖ  \n",
    "4. **This session (Session 3):** Advanced fine-tuning techniques ‚Üê You are here!\n",
    "\n",
    "## What You Will Master\n",
    "\n",
    "1. **üèóÔ∏è Fine-tuning fundamentals** - Full vs. parameter-efficient approaches with cost analysis\n",
    "2. **‚ö° LoRA and advanced PEFT** - Low-Rank Adaptation with systematic parameter optimization\n",
    "3. **üìä Instruction tuning** - Task-specific adaptation with systematic evaluation  \n",
    "4. **üéØ Preference optimization** - Alignment techniques for better outputs\n",
    "5. **üìà Systematic monitoring** - Training metrics, loss analysis, convergence patterns\n",
    "6. **üåç Low-resource adaptation** - Strategies for data-scarce languages\n",
    "7. **üè≠ Production deployment** - Real-world considerations and best practices\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will:\n",
    "- ‚úÖ **Distinguish systematically** between full and parameter-efficient fine-tuning approaches\n",
    "- ‚úÖ **Implement LoRA fine-tuning** with optimal hyperparameter selection  \n",
    "- ‚úÖ **Monitor training systematically** using multiple metrics and visualizations\n",
    "- ‚úÖ **Evaluate model improvements** quantitatively across multiple dimensions\n",
    "- ‚úÖ **Design production pipelines** for low-resource language fine-tuning\n",
    "- ‚úÖ **Apply cost-benefit analysis** for real-world deployment decisions\n",
    "\n",
    "## üî¨ Advanced Methodology\n",
    "\n",
    "**This session uses production-grade practices:**\n",
    "- **üìä Systematic Comparison:** Multiple fine-tuning approaches with quantitative evaluation\n",
    "- **üí∞ Cost Analysis:** Resource requirements and ROI calculations for each approach\n",
    "- **üéØ Task-Specific Evaluation:** Beyond perplexity - task-relevant metrics\n",
    "- **üåç Cross-Lingual Validation:** Systematic evaluation across language boundaries  \n",
    "- **üìà Production Readiness:** Deployment considerations and scalability analysis\n",
    "\n",
    "## How This Session Works\n",
    "\n",
    "- **üéì Theory ‚Üí Practice ‚Üí Analysis:** Learn concepts ‚Üí Apply systematically ‚Üí Measure results\n",
    "- **üîß Hands-on Implementation:** Real code, real models, real data\n",
    "- **üìä Quantitative Evaluation:** Every claim backed by systematic measurement\n",
    "- **üíº Production Focus:** Techniques you can use in real projects immediately\n",
    "- **üåç Low-Resource Emphasis:** Special attention to resource-constrained scenarios\n",
    "\n",
    "**‚ö†Ô∏è Important Note:**  \n",
    "This is a **production-oriented demonstration** using systematic methodology. While we use a small dataset for speed, all techniques scale to production systems. The focus is on **understanding systematic approaches** and **building production-ready intuitions**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6142189",
   "metadata": {},
   "source": [
    "## 0. üèóÔ∏è Fine-Tuning Fundamentals: Theory and Practice\n",
    "\n",
    "### 0.1 Fine-Tuning Taxonomy: A Systematic Overview\n",
    "\n",
    "**Fine-tuning** is the process of adapting a pretrained language model to specialized tasks or domains using additional labeled data. Understanding the landscape of approaches is crucial for making informed decisions.\n",
    "\n",
    "| **Approach** | **Parameters Updated** | **Memory Requirement** | **Training Speed** | **Best For** | **Cost** |\n",
    "|--------------|----------------------|----------------------|-------------------|--------------|----------|\n",
    "| **üî• Full Fine-tuning** | All parameters (100%) | Very High (4x model size) | Slow | High-resource tasks | $$$$$ |\n",
    "| **‚ö° Parameter-Efficient (PEFT)** | Small subset (0.1-10%) | Low (1.2x model size) | Fast | Low-resource languages | $$ |\n",
    "| **üéØ LoRA** | Low-rank adapters (~1%) | Very Low | Very Fast | Most practical cases | $ |\n",
    "| **üìö Instruction Tuning** | Task-specific layers | Medium | Medium | Following instructions | $$$ |\n",
    "| **üé™ Preference Optimization** | Value/reward layers | Medium | Medium | Human alignment | $$$ |\n",
    "\n",
    "### 0.2 üî¨ Deep Dive: Parameter-Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "**Why PEFT Matters for Low-Resource Languages:**\n",
    "\n",
    "1. **üí∞ Cost Effectiveness:** Train with 1000x less GPU memory\n",
    "2. **‚ö° Speed:** 10x faster training and deployment  \n",
    "3. **üõ°Ô∏è Catastrophic Forgetting Prevention:** Preserve original capabilities\n",
    "4. **üîÑ Task Switching:** Multiple adapters for different tasks\n",
    "5. **üì¶ Storage Efficiency:** Adapters are ~10MB vs full models at ~10GB\n",
    "\n",
    "### 0.3 üéØ LoRA (Low-Rank Adaptation) Deep Dive\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "```\n",
    "W = W‚ÇÄ + ŒîW = W‚ÇÄ + BA\n",
    "```\n",
    "Where:\n",
    "- `W‚ÇÄ`: Frozen pretrained weights\n",
    "- `B`, `A`: Low-rank matrices (rank r << d) \n",
    "- `ŒîW = BA`: Learned adaptation with r << original rank\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- **Rank (r):** Higher = more expressive but slower (typical: 4-64)\n",
    "- **Alpha (Œ±):** Scaling factor for adaptation strength (typical: 16-32) \n",
    "- **Target Modules:** Which layers to adapt (attention vs MLP vs both)\n",
    "- **Dropout:** Regularization for adaptation layers (typical: 0.05-0.1)\n",
    "\n",
    "### 0.4 üìä Systematic Approach to Fine-Tuning\n",
    "\n",
    "**Our methodology follows production best practices:**\n",
    "\n",
    "1. **üß™ Baseline Establishment:** Test pretrained model performance\n",
    "2. **üìä Systematic Hyperparameter Search:** Grid search over key parameters\n",
    "3. **üìà Multi-Metric Evaluation:** Beyond perplexity - task-specific metrics\n",
    "4. **üîç Ablation Studies:** Understand what drives improvements\n",
    "5. **üíº Production Planning:** Cost analysis and deployment considerations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Systematic Setup: GPU Configuration and Environment Check\n",
    "# Professional setup with comprehensive system analysis\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def check_system_capabilities():\n",
    "    \"\"\"Comprehensive system analysis for fine-tuning requirements\"\"\"\n",
    "    \n",
    "    print(\"üîß SYSTEM CAPABILITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # GPU Analysis\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
    "        print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "        \n",
    "        # Memory recommendations\n",
    "        if gpu_memory >= 15:\n",
    "            print(\"   Recommendation: Can handle base models up to 7B parameters\")\n",
    "        elif gpu_memory >= 10:\n",
    "            print(\"   Recommendation: Optimal for 1-3B parameter models (TinyLlama perfect)\")\n",
    "        else:\n",
    "            print(\"   Recommendation: Use smallest models or reduce batch size\")\n",
    "            \n",
    "        # Verify CUDA version compatibility\n",
    "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "        \n",
    "        recommendation = \"üöÄ OPTIMAL: GPU detected - fast training enabled\"\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No GPU detected\")\n",
    "        print(\"   Training will be 10-50x slower on CPU\")\n",
    "        print(\"   üí° For Google Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "        \n",
    "        recommendation = \"‚ö†Ô∏è  SUBOPTIMAL: CPU-only mode - expect slow training\"\n",
    "    \n",
    "    # Python environment analysis\n",
    "    print(f\"\\nüêç PYTHON ENVIRONMENT:\")\n",
    "    print(f\"   Version: {sys.version.split()[0]}\")\n",
    "    print(f\"   Platform: {sys.platform}\")\n",
    "    \n",
    "    # Memory analysis\n",
    "    try:\n",
    "        import psutil\n",
    "        ram_gb = psutil.virtual_memory().total / 1e9\n",
    "        print(f\"   System RAM: {ram_gb:.1f} GB\")\n",
    "        if ram_gb < 8:\n",
    "            print(\"   ‚ö†Ô∏è  Low RAM detected - reduce batch sizes\")\n",
    "    except ImportError:\n",
    "        print(\"   System RAM: Unable to detect (install psutil for details)\")\n",
    "    \n",
    "    print(f\"\\nüéØ OVERALL RECOMMENDATION:\")\n",
    "    print(f\"   {recommendation}\")\n",
    "    \n",
    "    return gpu_available\n",
    "\n",
    "# Run system analysis\n",
    "gpu_available = check_system_capabilities()\n",
    "\n",
    "# Set optimal device configuration\n",
    "device = \"cuda\" if gpu_available else \"cpu\"\n",
    "print(f\"\\n‚öôÔ∏è  Using device: {device.upper()}\")\n",
    "\n",
    "# Configure memory optimization if needed\n",
    "if gpu_available:\n",
    "    # Enable memory fraction for shared environments like Colab\n",
    "    torch.cuda.empty_cache()  # Clear any existing cache\n",
    "    print(\"‚úÖ GPU memory optimized for shared environments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Systematic Package Installation for Advanced Fine-Tuning\n",
    "# Production-grade setup with systematic dependency management\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages_systematic():\n",
    "    \"\"\"Install packages with systematic dependency management and verification\"\"\"\n",
    "    \n",
    "    print(\"üöÄ SYSTEMATIC PACKAGE INSTALLATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è±Ô∏è  This will take 2-4 minutes in Colab...\")\n",
    "    \n",
    "    # Core fine-tuning packages (order matters for dependencies)\n",
    "    core_packages = [\n",
    "        \"transformers>=4.35.0\",  # Core transformers library\n",
    "        \"datasets>=2.14.0\",     # Dataset management\n",
    "        \"accelerate>=0.23.0\",   # Multi-GPU and mixed precision\n",
    "        \"peft>=0.6.0\",          # Parameter-efficient fine-tuning\n",
    "        \"sentencepiece\",        # Tokenization support\n",
    "    ]\n",
    "    \n",
    "    # Data science and evaluation packages\n",
    "    analysis_packages = [\n",
    "        \"pandas>=1.5.0\",        # Data analysis\n",
    "        \"matplotlib>=3.5.0\",    # Plotting\n",
    "        \"seaborn>=0.11.0\",      # Statistical visualization  \n",
    "        \"numpy>=1.21.0\",        # Numerical computing\n",
    "        \"scikit-learn>=1.0.0\",  # Metrics and evaluation\n",
    "        \"tqdm\",                 # Progress bars\n",
    "    ]\n",
    "    \n",
    "    # Optional packages for enhanced functionality\n",
    "    optional_packages = [\n",
    "        \"wandb\",                # Experiment tracking (optional)\n",
    "        \"tensorboard\",          # TensorBoard logging (optional)\n",
    "        \"psutil\",               # System monitoring (optional)\n",
    "    ]\n",
    "    \n",
    "    def install_group(packages, group_name):\n",
    "        \"\"\"Install a group of packages with error handling\"\"\"\n",
    "        print(f\"\\nüìä Installing {group_name}...\")\n",
    "        \n",
    "        for package in packages:\n",
    "            try:\n",
    "                print(f\"  üì• {package}\")\n",
    "                subprocess.check_call([\n",
    "                    sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                    \"-q\", \"--upgrade\", package\n",
    "                ])\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"  ‚ùå Failed to install {package}: {e}\")\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    # Install package groups\n",
    "    success = True\n",
    "    success &= install_group(core_packages, \"CORE FINE-TUNING PACKAGES\")\n",
    "    success &= install_group(analysis_packages, \"DATA ANALYSIS PACKAGES\")\n",
    "    \n",
    "    print(f\"\\nüì¶ Installing optional packages (failures are OK)...\")\n",
    "    for package in optional_packages:\n",
    "        try:\n",
    "            subprocess.check_call([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package\n",
    "            ])\n",
    "            print(f\"  ‚úÖ {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"  ‚ö†Ô∏è  {package} (optional - skipped)\")\n",
    "    \n",
    "    # Verification step\n",
    "    print(f\"\\nüîç PACKAGE VERIFICATION:\")\n",
    "    verification_imports = [\n",
    "        (\"transformers\", \"transformers\"),\n",
    "        (\"datasets\", \"datasets\"),\n",
    "        (\"peft\", \"peft\"),\n",
    "        (\"pandas\", \"pd\"),\n",
    "        (\"matplotlib.pyplot\", \"plt\"),\n",
    "        (\"numpy\", \"np\"),\n",
    "    ]\n",
    "    \n",
    "    for module, alias in verification_imports:\n",
    "        try:\n",
    "            __import__(module)\n",
    "            print(f\"  ‚úÖ {module}\")\n",
    "        except ImportError:\n",
    "            print(f\"  ‚ùå {module} - CRITICAL ERROR\")\n",
    "            success = False\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\n‚úÖ INSTALLATION COMPLETE!\")\n",
    "        print(f\"üéØ Ready for advanced fine-tuning experiments\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå INSTALLATION ISSUES DETECTED\")\n",
    "        print(f\"üí° Try: Runtime ‚Üí Restart Runtime, then re-run this cell\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "# Run systematic installation\n",
    "installation_success = install_packages_systematic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß∞ Systematic Imports and Configuration\n",
    "# Production-grade imports with systematic evaluation capabilities\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# Core ML and fine-tuning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "# Data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Metrics and evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure professional plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Device configuration with memory optimization\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üîß DEVICE CONFIGURATION:\")\n",
    "print(f\"   Primary device: {device.upper()}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    # Optimize memory usage for fine-tuning\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Systematic reproducibility configuration\n",
    "GLOBAL_CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"device\": device,\n",
    "    \"torch_dtype\": torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    \"max_memory_fraction\": 0.8,  # Reserve some GPU memory\n",
    "    \"evaluation_batch_size\": 1,  # Conservative for memory\n",
    "}\n",
    "\n",
    "def set_reproducible_seed(seed: int = GLOBAL_CONFIG[\"seed\"]):\n",
    "    \"\"\"Set seeds for reproducible experiments across all libraries\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    print(f\"üéØ Reproducible seed set: {seed}\")\n",
    "\n",
    "# Initialize reproducible environment\n",
    "set_reproducible_seed()\n",
    "\n",
    "# Create systematic experiment tracking\n",
    "experiment_tracker = {\n",
    "    \"session_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"device\": device,\n",
    "    \"experiments\": [],\n",
    "    \"model_configs\": [],\n",
    "    \"training_logs\": [],\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ SYSTEMATIC ENVIRONMENT READY\")\n",
    "print(f\"   Session ID: {experiment_tracker['session_id']}\")\n",
    "print(f\"   Reproducible seed: {GLOBAL_CONFIG['seed']}\")\n",
    "print(f\"   Memory optimization: {'Enabled' if device == 'cuda' else 'CPU mode'}\")\n",
    "print(f\"   Experiment tracking: Initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44716915",
   "metadata": {},
   "source": [
    "## 1. ü§ñ Systematic Model Selection and Loading\n",
    "\n",
    "### 1.1 Model Selection Strategy for Low-Resource Languages\n",
    "\n",
    "**Strategic model selection** is crucial for successful fine-tuning. Here's our systematic approach:\n",
    "\n",
    "| **Model Family** | **Parameters** | **GPU Memory** | **Languages** | **Fine-tuning Efficiency** | **Best For** |\n",
    "|-----------------|----------------|----------------|---------------|---------------------------|--------------|\n",
    "| **TinyLlama** | 1.1B | ~3GB | Good multilingual | Excellent | Learning, prototyping |\n",
    "| **Phi-2** | 2.7B | ~6GB | English-focused | Very Good | High-quality English |\n",
    "| **Mistral-7B** | 7B | ~14GB | Strong multilingual | Good | Production applications |\n",
    "| **Llama2-7B** | 7B | ~14GB | Good multilingual | Good | Open-source production |\n",
    "\n",
    "**Why TinyLlama for this tutorial:**\n",
    "1. **üí∞ Resource Efficient:** Fits comfortably in Colab's free GPU tier\n",
    "2. **üåç Multilingual Capable:** Decent performance on low-resource languages\n",
    "3. **‚ö° Fast Training:** Quick iterations for learning\n",
    "4. **üìö Chat-Tuned:** Already instruction-following capable\n",
    "5. **üîì Permissive License:** Can be used for any purpose\n",
    "\n",
    "### 1.2 Advanced Model Loading with Performance Monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Systematic Model Loading with Performance Analysis\n",
    "# Production-grade model loading with comprehensive monitoring\n",
    "\n",
    "class ModelLoadingManager:\n",
    "    \"\"\"Advanced model loading with systematic tracking and optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.loading_metrics = {}\n",
    "        \n",
    "    def load_model_systematic(self) -> Dict[str, Any]:\n",
    "        \"\"\"Load model with comprehensive performance tracking\"\"\"\n",
    "        \n",
    "        print(\"ü§ñ SYSTEMATIC MODEL LOADING\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üì• Loading: {self.model_name}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        initial_memory = torch.cuda.memory_allocated() if device == \"cuda\" else 0\n",
    "        \n",
    "        try:\n",
    "            # Load tokenizer with optimization\n",
    "            print(\"üî§ Loading tokenizer...\")\n",
    "            tokenizer_start = time.time()\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                use_fast=True,  # Use fast tokenizer when available\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # Configure tokenizer for training\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                print(\"   ‚úÖ Configured pad token\")\n",
    "            \n",
    "            tokenizer_time = time.time() - tokenizer_start\n",
    "            vocab_size = len(self.tokenizer)\n",
    "            \n",
    "            print(f\"   ‚úÖ Tokenizer loaded: {vocab_size:,} tokens ({tokenizer_time:.2f}s)\")\n",
    "            \n",
    "            # Load model with memory optimization\n",
    "            print(\"üß† Loading model...\")\n",
    "            model_start = time.time()\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=GLOBAL_CONFIG[\"torch_dtype\"],\n",
    "                device_map=\"auto\" if device == \"cuda\" else None,\n",
    "                trust_remote_code=True,\n",
    "                attn_implementation=\"flash_attention_2\" if device == \"cuda\" else \"eager\",\n",
    "            )\n",
    "            \n",
    "            # Move to device and optimize\n",
    "            if device == \"cpu\":\n",
    "                self.model = self.model.to(device)\n",
    "            \n",
    "            # Configure for training\n",
    "            self.model.config.use_cache = False  # Required for gradient checkpointing\n",
    "            if hasattr(self.model.config, \"pad_token_id\") and self.model.config.pad_token_id is None:\n",
    "                self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "            \n",
    "            model_time = time.time() - model_start\n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate model statistics\n",
    "            param_count = sum(p.numel() for p in self.model.parameters())\n",
    "            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "            \n",
    "            # Memory analysis\n",
    "            current_memory = torch.cuda.memory_allocated() if device == \"cuda\" else 0\n",
    "            memory_used = (current_memory - initial_memory) / 1e9  # GB\n",
    "            \n",
    "            # Store comprehensive metrics\n",
    "            self.loading_metrics = {\n",
    "                \"model_name\": self.model_name,\n",
    "                \"total_parameters\": param_count,\n",
    "                \"trainable_parameters\": trainable_params,\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"tokenizer_load_time\": tokenizer_time,\n",
    "                \"model_load_time\": model_time,\n",
    "                \"total_load_time\": total_time,\n",
    "                \"memory_usage_gb\": memory_used,\n",
    "                \"dtype\": str(GLOBAL_CONFIG[\"torch_dtype\"]),\n",
    "                \"device\": device,\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "            # Display comprehensive results\n",
    "            print(f\"‚úÖ MODEL LOADED SUCCESSFULLY!\")\n",
    "            print(f\"   üìä Parameters: {param_count/1e6:.1f}M total, {trainable_params/1e6:.1f}M trainable\")\n",
    "            print(f\"   üî§ Vocabulary: {vocab_size:,} tokens\")\n",
    "            print(f\"   ‚è±Ô∏è  Loading time: {total_time:.2f}s (tokenizer: {tokenizer_time:.2f}s, model: {model_time:.2f}s)\")\n",
    "            print(f\"   üíæ Memory usage: {memory_used:.2f}GB\")\n",
    "            print(f\"   üéØ Device: {device} ({GLOBAL_CONFIG['torch_dtype']})\")\n",
    "            \n",
    "            # Test model with a quick inference\n",
    "            print(\"\\\\nüß™ QUICK MODEL TEST:\")\n",
    "            test_prompt = \"Translate to Luxembourgish: Hello, how are you?\"\n",
    "            test_result = self._quick_generation_test(test_prompt)\n",
    "            \n",
    "            if test_result[\"success\"]:\n",
    "                print(f\"   ‚úÖ Generation test passed\")\n",
    "                print(f\"   üìù Test output: {test_result['output'][:100]}...\")\n",
    "                print(f\"   ‚ö° Generation speed: {test_result['tokens_per_second']:.1f} tokens/s\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Generation test failed: {test_result['error']}\")\n",
    "            \n",
    "            # Add to experiment tracker\n",
    "            experiment_tracker[\"model_configs\"].append(self.loading_metrics)\n",
    "            \n",
    "            return self.loading_metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_metrics = {\n",
    "                \"model_name\": self.model_name,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"total_load_time\": time.time() - start_time\n",
    "            }\n",
    "            \n",
    "            print(f\"‚ùå MODEL LOADING FAILED:\")\n",
    "            print(f\"   Error: {str(e)}\")\n",
    "            print(f\"   üí° Try: Restart runtime or use a smaller model\")\n",
    "            \n",
    "            return error_metrics\n",
    "    \n",
    "    def _quick_generation_test(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Quick generation test to verify model functionality\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=20,\n",
    "                    do_sample=False,\n",
    "                    temperature=0.1,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id\n",
    "                )\n",
    "            \n",
    "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            generation_time = time.time() - start_time\n",
    "            tokens_generated = len(outputs[0]) - len(inputs.input_ids[0])\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"output\": generated_text,\n",
    "                \"generation_time\": generation_time,\n",
    "                \"tokens_generated\": tokens_generated,\n",
    "                \"tokens_per_second\": tokens_generated / generation_time if generation_time > 0 else 0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# Initialize and run systematic model loading\n",
    "model_manager = ModelLoadingManager()\n",
    "loading_results = model_manager.load_model_systematic()\n",
    "\n",
    "# Make model and tokenizer available globally\n",
    "model = model_manager.model\n",
    "tokenizer = model_manager.tokenizer\n",
    "\n",
    "# Display summary for systematic analysis\n",
    "if loading_results.get(\"success\", False):\n",
    "    print(f\"\\\\nüéØ READY FOR FINE-TUNING:\")\n",
    "    print(f\"   Model: {loading_results['total_parameters']/1e6:.1f}M parameters\")\n",
    "    print(f\"   Memory: {loading_results['memory_usage_gb']:.2f}GB allocated\")\n",
    "    print(f\"   Setup time: {loading_results['total_load_time']:.2f}s\")\n",
    "    print(f\"   ‚úÖ All systems operational!\")\n",
    "else:\n",
    "    print(f\"\\\\n‚ùå SETUP FAILED - Check errors above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5960f",
   "metadata": {},
   "source": [
    "## 2. Build a tiny low resource toy dataset\n",
    "\n",
    "We construct a minimal dataset of English to Luxembourgish translation pairs directly in the notebook.  \n",
    "\n",
    "- We treat Luxembourgish (lb) as the low resource language.  \n",
    "- In a real project, you would replace this list with real parallel data or task specific instances.  \n",
    "- The tiny size is intentional so that training finishes in a few minutes for demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f3c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Good morning, how are you?\",\n",
    "        \"target\": \"Gudde Moien, w√©i geet et dir?\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Thank you very much for your help.\",\n",
    "        \"target\": \"Villmools Merci fir deng H√´llef.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"I would like a coffee with milk, please.\",\n",
    "        \"target\": \"Ech h√§tt g√§r eng Taass Kaffi mat M√´llech, wann ech gelift.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Where is the train station?\",\n",
    "        \"target\": \"Wou ass d'Eisebunnsstatioun?\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Today the weather is very cold.\",\n",
    "        \"target\": \"Haut ass d'Wieder ganz kal.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"My name is Anna and I live in Luxembourg.\",\n",
    "        \"target\": \"Ech heeschen Anna an ech wunnen zu L√´tzebuerg.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Could you please speak a little more slowly?\",\n",
    "        \"target\": \"Kanns du w.e.g. e b√´sse m√©i lues schw√§tzen?\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"I am learning Luxembourgish because I work here.\",\n",
    "        \"target\": \"Ech l√©ieren L√´tzebuergesch, well ech hei schaffen.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"The next bus arrives in ten minutes.\",\n",
    "        \"target\": \"Den n√§chste Bus k√´nnt an z√©ng Minutten un.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"This food is delicious.\",\n",
    "        \"target\": \"D√´st Iessen ass lecker.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 11,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"I do not understand, can you repeat that?\",\n",
    "        \"target\": \"Ech verstinn net, kanns du dat widderhuelen?\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 12,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Have a nice evening.\",\n",
    "        \"target\": \"Sch√©inen Owend nach.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(toy_data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f94cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple split: 75 percent train, 25 percent test.\n",
    "split_dataset = dataset.train_test_split(test_size=0.25, seed=SEED)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Eval size:\", len(eval_dataset))\n",
    "\n",
    "for example in eval_dataset:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd6b29",
   "metadata": {},
   "source": [
    "## 3. Define an instruction style prompt template\n",
    "\n",
    "We wrap each example into a simple instruction prompt so that the model sees:\n",
    "\n",
    "- A system like description.\n",
    "- The English sentence.\n",
    "- A cue to produce the Luxembourgish translation.\n",
    "\n",
    "For training, we construct a single text sequence that contains both the prompt and the target translation.  \n",
    "The model learns to generate the full sequence.  \n",
    "At inference time, we will provide only the prompt and ask the model to continue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b902744",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\n",
    "    \"You are a helpful assistant that translates from English to Luxembourgish.\\n\"\n",
    "    \"Translate the following sentence into Luxembourgish.\\n\\n\"\n",
    "    \"English: {source}\\n\"\n",
    "    \"Luxembourgish:\"\n",
    ")\n",
    "\n",
    "def format_example(example: Dict) -> Dict:\n",
    "    prompt = PROMPT_TEMPLATE.format(source=example[\"source\"])\n",
    "    full_text = prompt + \" \" + example[\"target\"]\n",
    "    return {\n",
    "        \"text\": full_text,\n",
    "        \"language\": example[\"language\"],\n",
    "        \"id\": example[\"id\"],\n",
    "    }\n",
    "\n",
    "formatted_train = train_dataset.map(format_example)\n",
    "formatted_eval = eval_dataset.map(format_example)\n",
    "\n",
    "for e in formatted_train.select(range(2)):\n",
    "    print(\"----\")\n",
    "    print(e[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be6189d",
   "metadata": {},
   "source": [
    "## 4. Baseline model behaviour before fine tuning\n",
    "\n",
    "Before we change any parameters, we check how the base TinyLlama model behaves on our evaluation set.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Use only the prompt part of each example.\n",
    "- Let the model generate a continuation.\n",
    "- Compare the output qualitatively to the target translation.\n",
    "\n",
    "Keep expectations realistic.  \n",
    "The base model may already know some Luxembourgish, but it was not trained specifically for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a41c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(source_sentence: str) -> str:\n",
    "    return PROMPT_TEMPLATE.format(source=source_sentence)\n",
    "\n",
    "def generate_translation(model, tokenizer, source_sentence: str, max_new_tokens: int = 64) -> str:\n",
    "    prompt = build_prompt(source_sentence)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated\n",
    "\n",
    "print(\"### Baseline outputs before fine tuning ###\\n\")\n",
    "\n",
    "for example in eval_dataset:\n",
    "    src = example[\"source\"]\n",
    "    tgt = example[\"target\"]\n",
    "    generated = generate_translation(model, tokenizer, src)\n",
    "    print(\"English:\", src)\n",
    "    print(\"Target Luxembourgish:\", tgt)\n",
    "    print(\"Model output:\")\n",
    "    print(generated)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448174e",
   "metadata": {},
   "source": [
    "## 5. Prepare data for causal language model training\n",
    "\n",
    "We now convert the formatted text examples into token ids suitable for causal language modeling.\n",
    "\n",
    "- Each training instance is a sequence of tokens.\n",
    "- The model will learn to predict the next token given previous tokens.\n",
    "- For simplicity, we use the same token ids as both `input_ids` and `labels`.\n",
    "\n",
    "In a more careful setup, you might mask the loss on prompt tokens and only train on the answer part.  \n",
    "Here we keep the configuration simple so that the mechanics of parameter efficient fine tuning are clear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "def tokenize_function(example: Dict) -> Dict:\n",
    "    result = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # For simple language modeling we use the same ids as labels.\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_train = formatted_train.map(tokenize_function, remove_columns=[\"text\", \"language\", \"id\"])\n",
    "tokenized_eval = formatted_eval.map(tokenize_function, remove_columns=[\"text\", \"language\", \"id\"])\n",
    "\n",
    "print(tokenized_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919deaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for causal language modeling. No masked language modeling.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7998140",
   "metadata": {},
   "source": [
    "## 6. Configure LoRA parameter efficient fine tuning\n",
    "\n",
    "Instead of updating all model parameters, we use LoRA:\n",
    "\n",
    "- LoRA adds small trainable matrices (low rank adapters) to selected linear layers.\n",
    "- The base model weights stay frozen.\n",
    "- This makes fine tuning lighter and more feasible on modest hardware.\n",
    "- It also reduces the risk of catastrophic forgetting.\n",
    "\n",
    "We choose a small rank and apply LoRA to attention projection layers only.  \n",
    "This is a typical starting point for LLaMA like models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be471d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # typical for LLaMA family models\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a35be",
   "metadata": {},
   "source": [
    "## 7. Training configuration\n",
    "\n",
    "We set very conservative training hyper parameters:\n",
    "\n",
    "- Small batch size.\n",
    "- A few epochs over a tiny dataset.\n",
    "- No checkpoint saving to keep the run light.\n",
    "- Logging every step so that you can watch the loss.\n",
    "\n",
    "In a realistic low resource project you would:\n",
    "\n",
    "- Use many more examples.\n",
    "- Run for longer.\n",
    "- Tune hyper parameters carefully.\n",
    "- Monitor validation loss and task specific metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40eee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"tiny_llama_lb_lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    weight_decay=0.0,\n",
    "    fp16=(device == \"cuda\"),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "print(\"\\nTraining completed.\")\n",
    "print(train_result)\n",
    "\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(\"\\nEvaluation metrics:\")\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e80ee",
   "metadata": {},
   "source": [
    "## 8. Compare outputs before and after fine tuning\n",
    "\n",
    "Now we generate translations again using the fine tuned model.  \n",
    "We keep the prompts identical and inspect:\n",
    "\n",
    "- Whether the model is more likely to produce Luxembourgish.\n",
    "- Whether the translations are closer to our target references.\n",
    "- Any side effects such as overfitting to the tiny dataset style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66659223",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Outputs after LoRA fine tuning ###\\n\")\n",
    "\n",
    "for example in eval_dataset:\n",
    "    src = example[\"source\"]\n",
    "    tgt = example[\"target\"]\n",
    "    prompt = build_prompt(src)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = peft_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"English:\", src)\n",
    "    print(\"Target Luxembourgish:\", tgt)\n",
    "    print(\"Model output after fine tuning:\")\n",
    "    print(generated)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ca830",
   "metadata": {},
   "source": [
    "## 9. Quick discussion prompts\n",
    "\n",
    "Discuss in small groups or write down short notes.\n",
    "\n",
    "1. **Data size and quality.**  \n",
    "   - We used 12 examples.  \n",
    "   - What kinds of errors or biases can appear if we deploy a system trained on such a tiny sample?  \n",
    "   - How would you scale the dataset for a real project in a low resource setting?\n",
    "\n",
    "2. **Evaluation.**  \n",
    "   - We only looked at qualitative outputs and language modeling loss.  \n",
    "   - Which task specific metrics would you design for a real application such as translation, classification, or dialogue for a low resource language?  \n",
    "   - How would you build a reliable test set?\n",
    "\n",
    "3. **Safety and robustness.**  \n",
    "   - Fine tuning can change model behaviour in unexpected ways.  \n",
    "   - What additional checks would you perform before using a fine tuned model with real users in a low resource community?\n",
    "\n",
    "4. **Transfer to your language of interest.**  \n",
    "   - Suppose you want to adapt the same pipeline to Armenian or another language.  \n",
    "   - What would you need to change in this notebook?  \n",
    "   - Which parts are reusable, and which parts are specific to the Luxembourgish toy dataset?\n",
    "\n",
    "5. **Beyond LoRA.**  \n",
    "   - Parameter efficient fine tuning is one piece of the puzzle.  \n",
    "   - What other techniques could you combine with LoRA for low resource languages, for example prompting, retrieval augmented generation, multilingual pre training, or synthetic data generation?\n",
    "\n",
    "Use these questions to connect the small scale exercise with the broader methodological and ethical questions of building LLMs for low resource languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c104cc2",
   "metadata": {},
   "source": [
    "## 10. Optional extensions (if you have time)\n",
    "\n",
    "If you finish early, you can explore one of these directions:\n",
    "\n",
    "1. **Add a second language.**  \n",
    "   Extend the toy dataset with a few examples for another low resource language, for example Armenian or Kurdish, and see how the model behaves.\n",
    "\n",
    "2. **Loss masking.**  \n",
    "   Modify the tokenization step so that the loss is computed only on the answer part of each example and not on the prompt.\n",
    "\n",
    "3. **Temperature sweep.**  \n",
    "   Generate outputs with different sampling temperatures and reflect on how diversity and correctness trade off.\n",
    "\n",
    "4. **Save and reload adapters.**  \n",
    "   Use `peft_model.save_pretrained` to save only the LoRA adapters, then reload them on top of the base model in a fresh notebook.\n",
    "\n",
    "These small experiments help build intuition about how parameter efficient fine tuning interacts with low resource data and multilingual models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
