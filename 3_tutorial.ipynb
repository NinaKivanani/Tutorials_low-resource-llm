{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7406ca",
   "metadata": {},
   "source": [
    "# Yerevan Winter School Tutorial\n",
    "## Parameter Efficient Fine Tuning for a Low Resource Language in Colab\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**ðŸ“š Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/3_tutorial.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "**Context.**  \n",
    "This hands on session shows how to adapt a compact open source LLM to a low resource language with only a tiny toy dataset.  \n",
    "We will use Luxembourgish as the example low resource language, but the workflow is applicable to other languages such as Armenian.\n",
    "\n",
    "**What you will do.**\n",
    "\n",
    "1. Set up a Colab runtime and install libraries.\n",
    "2. Load a small multilingual instruction tuned model.\n",
    "3. Build a tiny English to Luxembourgish translation dataset inside the notebook.\n",
    "4. Run the base model before fine tuning and inspect its behaviour.\n",
    "5. Apply LoRA based parameter efficient fine tuning for a few short epochs.\n",
    "6. Monitor training logs and basic metrics.\n",
    "7. Compare model outputs before and after fine tuning on held out examples.\n",
    "8. Reflect on what would be needed to make such fine tuning robust and reliable for real world low resource applications.\n",
    "\n",
    "**Important note.**  \n",
    "This is a demonstration.  \n",
    "We work with a tiny dataset and very few training steps.  \n",
    "The goal is to understand the mechanics and pitfalls, not to train a production system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are in Google Colab, make sure the runtime has a GPU:\n",
    "# Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected. Training will be very slow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries.\n",
    "# In Colab, this cell may take a couple of minutes.\n",
    "!pip install -q transformers datasets accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44716915",
   "metadata": {},
   "source": [
    "## 1. Load a compact multilingual model\n",
    "\n",
    "We choose a relatively small chat tuned model so that:\n",
    "\n",
    "- It fits in Colab GPU memory.\n",
    "- It supports many languages, including low resource ones reasonably well.\n",
    "\n",
    "Here we use the TinyLlama chat model (about 1.1B parameters).  \n",
    "In a real project, the model choice would depend on license constraints, hardware, and language coverage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Some causal language models do not have a pad token set.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Disable cache during training to avoid warnings.\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5960f",
   "metadata": {},
   "source": [
    "## 2. Build a tiny low resource toy dataset\n",
    "\n",
    "We construct a minimal dataset of English to Luxembourgish translation pairs directly in the notebook.  \n",
    "\n",
    "- We treat Luxembourgish (lb) as the low resource language.  \n",
    "- In a real project, you would replace this list with real parallel data or task specific instances.  \n",
    "- The tiny size is intentional so that training finishes in a few minutes for demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f3c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Good morning, how are you?\",\n",
    "        \"target\": \"Gudde Moien, wÃ©i geet et dir?\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Thank you very much for your help.\",\n",
    "        \"target\": \"Villmools Merci fir deng HÃ«llef.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"I would like a coffee with milk, please.\",\n",
    "        \"target\": \"Ech hÃ¤tt gÃ¤r eng Taass Kaffi mat MÃ«llech, wann ech gelift.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Where is the train station?\",\n",
    "        \"target\": \"Wou ass d'Eisebunnsstatioun?\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Today the weather is very cold.\",\n",
    "        \"target\": \"Haut ass d'Wieder ganz kal.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"My name is Anna and I live in Luxembourg.\",\n",
    "        \"target\": \"Ech heeschen Anna an ech wunnen zu LÃ«tzebuerg.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Could you please speak a little more slowly?\",\n",
    "        \"target\": \"Kanns du w.e.g. e bÃ«sse mÃ©i lues schwÃ¤tzen?\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"I am learning Luxembourgish because I work here.\",\n",
    "        \"target\": \"Ech lÃ©ieren LÃ«tzebuergesch, well ech hei schaffen.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"The next bus arrives in ten minutes.\",\n",
    "        \"target\": \"Den nÃ¤chste Bus kÃ«nnt an zÃ©ng Minutten un.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"This food is delicious.\",\n",
    "        \"target\": \"DÃ«st Iessen ass lecker.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 11,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"I do not understand, can you repeat that?\",\n",
    "        \"target\": \"Ech verstinn net, kanns du dat widderhuelen?\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 12,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Have a nice evening.\",\n",
    "        \"target\": \"SchÃ©inen Owend nach.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(toy_data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f94cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple split: 75 percent train, 25 percent test.\n",
    "split_dataset = dataset.train_test_split(test_size=0.25, seed=SEED)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Eval size:\", len(eval_dataset))\n",
    "\n",
    "for example in eval_dataset:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd6b29",
   "metadata": {},
   "source": [
    "## 3. Define an instruction style prompt template\n",
    "\n",
    "We wrap each example into a simple instruction prompt so that the model sees:\n",
    "\n",
    "- A system like description.\n",
    "- The English sentence.\n",
    "- A cue to produce the Luxembourgish translation.\n",
    "\n",
    "For training, we construct a single text sequence that contains both the prompt and the target translation.  \n",
    "The model learns to generate the full sequence.  \n",
    "At inference time, we will provide only the prompt and ask the model to continue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b902744",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\n",
    "    \"You are a helpful assistant that translates from English to Luxembourgish.\\n\"\n",
    "    \"Translate the following sentence into Luxembourgish.\\n\\n\"\n",
    "    \"English: {source}\\n\"\n",
    "    \"Luxembourgish:\"\n",
    ")\n",
    "\n",
    "def format_example(example: Dict) -> Dict:\n",
    "    prompt = PROMPT_TEMPLATE.format(source=example[\"source\"])\n",
    "    full_text = prompt + \" \" + example[\"target\"]\n",
    "    return {\n",
    "        \"text\": full_text,\n",
    "        \"language\": example[\"language\"],\n",
    "        \"id\": example[\"id\"],\n",
    "    }\n",
    "\n",
    "formatted_train = train_dataset.map(format_example)\n",
    "formatted_eval = eval_dataset.map(format_example)\n",
    "\n",
    "for e in formatted_train.select(range(2)):\n",
    "    print(\"----\")\n",
    "    print(e[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be6189d",
   "metadata": {},
   "source": [
    "## 4. Baseline model behaviour before fine tuning\n",
    "\n",
    "Before we change any parameters, we check how the base TinyLlama model behaves on our evaluation set.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Use only the prompt part of each example.\n",
    "- Let the model generate a continuation.\n",
    "- Compare the output qualitatively to the target translation.\n",
    "\n",
    "Keep expectations realistic.  \n",
    "The base model may already know some Luxembourgish, but it was not trained specifically for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a41c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(source_sentence: str) -> str:\n",
    "    return PROMPT_TEMPLATE.format(source=source_sentence)\n",
    "\n",
    "def generate_translation(model, tokenizer, source_sentence: str, max_new_tokens: int = 64) -> str:\n",
    "    prompt = build_prompt(source_sentence)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated\n",
    "\n",
    "print(\"### Baseline outputs before fine tuning ###\\n\")\n",
    "\n",
    "for example in eval_dataset:\n",
    "    src = example[\"source\"]\n",
    "    tgt = example[\"target\"]\n",
    "    generated = generate_translation(model, tokenizer, src)\n",
    "    print(\"English:\", src)\n",
    "    print(\"Target Luxembourgish:\", tgt)\n",
    "    print(\"Model output:\")\n",
    "    print(generated)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448174e",
   "metadata": {},
   "source": [
    "## 5. Prepare data for causal language model training\n",
    "\n",
    "We now convert the formatted text examples into token ids suitable for causal language modeling.\n",
    "\n",
    "- Each training instance is a sequence of tokens.\n",
    "- The model will learn to predict the next token given previous tokens.\n",
    "- For simplicity, we use the same token ids as both `input_ids` and `labels`.\n",
    "\n",
    "In a more careful setup, you might mask the loss on prompt tokens and only train on the answer part.  \n",
    "Here we keep the configuration simple so that the mechanics of parameter efficient fine tuning are clear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "def tokenize_function(example: Dict) -> Dict:\n",
    "    result = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # For simple language modeling we use the same ids as labels.\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_train = formatted_train.map(tokenize_function, remove_columns=[\"text\", \"language\", \"id\"])\n",
    "tokenized_eval = formatted_eval.map(tokenize_function, remove_columns=[\"text\", \"language\", \"id\"])\n",
    "\n",
    "print(tokenized_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919deaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for causal language modeling. No masked language modeling.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7998140",
   "metadata": {},
   "source": [
    "## 6. Configure LoRA parameter efficient fine tuning\n",
    "\n",
    "Instead of updating all model parameters, we use LoRA:\n",
    "\n",
    "- LoRA adds small trainable matrices (low rank adapters) to selected linear layers.\n",
    "- The base model weights stay frozen.\n",
    "- This makes fine tuning lighter and more feasible on modest hardware.\n",
    "- It also reduces the risk of catastrophic forgetting.\n",
    "\n",
    "We choose a small rank and apply LoRA to attention projection layers only.  \n",
    "This is a typical starting point for LLaMA like models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be471d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # typical for LLaMA family models\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a35be",
   "metadata": {},
   "source": [
    "## 7. Training configuration\n",
    "\n",
    "We set very conservative training hyper parameters:\n",
    "\n",
    "- Small batch size.\n",
    "- A few epochs over a tiny dataset.\n",
    "- No checkpoint saving to keep the run light.\n",
    "- Logging every step so that you can watch the loss.\n",
    "\n",
    "In a realistic low resource project you would:\n",
    "\n",
    "- Use many more examples.\n",
    "- Run for longer.\n",
    "- Tune hyper parameters carefully.\n",
    "- Monitor validation loss and task specific metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40eee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"tiny_llama_lb_lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    weight_decay=0.0,\n",
    "    fp16=(device == \"cuda\"),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "print(\"\\nTraining completed.\")\n",
    "print(train_result)\n",
    "\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(\"\\nEvaluation metrics:\")\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e80ee",
   "metadata": {},
   "source": [
    "## 8. Compare outputs before and after fine tuning\n",
    "\n",
    "Now we generate translations again using the fine tuned model.  \n",
    "We keep the prompts identical and inspect:\n",
    "\n",
    "- Whether the model is more likely to produce Luxembourgish.\n",
    "- Whether the translations are closer to our target references.\n",
    "- Any side effects such as overfitting to the tiny dataset style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66659223",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Outputs after LoRA fine tuning ###\\n\")\n",
    "\n",
    "for example in eval_dataset:\n",
    "    src = example[\"source\"]\n",
    "    tgt = example[\"target\"]\n",
    "    prompt = build_prompt(src)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = peft_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"English:\", src)\n",
    "    print(\"Target Luxembourgish:\", tgt)\n",
    "    print(\"Model output after fine tuning:\")\n",
    "    print(generated)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ca830",
   "metadata": {},
   "source": [
    "## 9. Quick discussion prompts\n",
    "\n",
    "Discuss in small groups or write down short notes.\n",
    "\n",
    "1. **Data size and quality.**  \n",
    "   - We used 12 examples.  \n",
    "   - What kinds of errors or biases can appear if we deploy a system trained on such a tiny sample?  \n",
    "   - How would you scale the dataset for a real project in a low resource setting?\n",
    "\n",
    "2. **Evaluation.**  \n",
    "   - We only looked at qualitative outputs and language modeling loss.  \n",
    "   - Which task specific metrics would you design for a real application such as translation, classification, or dialogue for a low resource language?  \n",
    "   - How would you build a reliable test set?\n",
    "\n",
    "3. **Safety and robustness.**  \n",
    "   - Fine tuning can change model behaviour in unexpected ways.  \n",
    "   - What additional checks would you perform before using a fine tuned model with real users in a low resource community?\n",
    "\n",
    "4. **Transfer to your language of interest.**  \n",
    "   - Suppose you want to adapt the same pipeline to Armenian or another language.  \n",
    "   - What would you need to change in this notebook?  \n",
    "   - Which parts are reusable, and which parts are specific to the Luxembourgish toy dataset?\n",
    "\n",
    "5. **Beyond LoRA.**  \n",
    "   - Parameter efficient fine tuning is one piece of the puzzle.  \n",
    "   - What other techniques could you combine with LoRA for low resource languages, for example prompting, retrieval augmented generation, multilingual pre training, or synthetic data generation?\n",
    "\n",
    "Use these questions to connect the small scale exercise with the broader methodological and ethical questions of building LLMs for low resource languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c104cc2",
   "metadata": {},
   "source": [
    "## 10. Optional extensions (if you have time)\n",
    "\n",
    "If you finish early, you can explore one of these directions:\n",
    "\n",
    "1. **Add a second language.**  \n",
    "   Extend the toy dataset with a few examples for another low resource language, for example Armenian or Kurdish, and see how the model behaves.\n",
    "\n",
    "2. **Loss masking.**  \n",
    "   Modify the tokenization step so that the loss is computed only on the answer part of each example and not on the prompt.\n",
    "\n",
    "3. **Temperature sweep.**  \n",
    "   Generate outputs with different sampling temperatures and reflect on how diversity and correctness trade off.\n",
    "\n",
    "4. **Save and reload adapters.**  \n",
    "   Use `peft_model.save_pretrained` to save only the LoRA adapters, then reload them on top of the base model in a fresh notebook.\n",
    "\n",
    "These small experiments help build intuition about how parameter efficient fine tuning interacts with low resource data and multilingual models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
