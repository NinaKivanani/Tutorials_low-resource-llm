{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7406ca",
   "metadata": {},
   "source": [
    "# Session 3: Fine-tuning LLMs for Low-Resource Languages üöÄ\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**üìö Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/3_tutorial.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
    "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "**Advanced Parameter-Efficient Fine-Tuning for Low-Resource Languages**\n",
    "\n",
    "Welcome to **Session 3**! You'll master the art and science of adapting pretrained LLMs to specialized tasks using systematic fine-tuning techniques, with focus on practical applications for low-resource languages.\n",
    "\n",
    "**üéØ Focus:** Parameter-efficient fine-tuning, LoRA, systematic evaluation  \n",
    "**üíª Requirements:** GPU recommended (Colab free tier sufficient)  \n",
    "**üî¨ Methodology:** Production-ready techniques with systematic comparison\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**üìã Recommended learning path:**\n",
    "1. **Session 0:** Setup and tokenization analysis ‚úÖ  \n",
    "2. **Session 1:** Systematic baseline techniques ‚úÖ\n",
    "3. **Session 2:** Systematic prompt engineering ‚úÖ  \n",
    "4. **This session (Session 3):** Advanced fine-tuning techniques ‚Üê You are here!\n",
    "\n",
    "## What You Will Master\n",
    "\n",
    "1. **üèóÔ∏è Fine-tuning fundamentals** - Full vs. parameter-efficient approaches with cost analysis\n",
    "2. **‚ö° LoRA and advanced PEFT** - Low-Rank Adaptation with systematic parameter optimization\n",
    "3. **üìä Instruction tuning** - Task-specific adaptation with systematic evaluation  \n",
    "4. **üéØ Preference optimization** - Alignment techniques for better outputs\n",
    "5. **üìà Systematic monitoring** - Training metrics, loss analysis, convergence patterns\n",
    "6. **üåç Low-resource adaptation** - Strategies for data-scarce languages\n",
    "7. **üè≠ Production deployment** - Real-world considerations and best practices\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will:\n",
    "- ‚úÖ **Distinguish systematically** between full and parameter-efficient fine-tuning approaches\n",
    "- ‚úÖ **Implement LoRA fine-tuning** with optimal hyperparameter selection  \n",
    "- ‚úÖ **Monitor training systematically** using multiple metrics and visualizations\n",
    "- ‚úÖ **Evaluate model improvements** quantitatively across multiple dimensions\n",
    "- ‚úÖ **Design production pipelines** for low-resource language fine-tuning\n",
    "- ‚úÖ **Apply cost-benefit analysis** for real-world deployment decisions\n",
    "\n",
    "## üî¨ Advanced Methodology\n",
    "\n",
    "**This session uses production-grade practices:**\n",
    "- **üìä Systematic Comparison:** Multiple fine-tuning approaches with quantitative evaluation\n",
    "- **üí∞ Cost Analysis:** Resource requirements and ROI calculations for each approach\n",
    "- **üéØ Task-Specific Evaluation:** Beyond perplexity - task-relevant metrics\n",
    "- **üåç Cross-Lingual Validation:** Systematic evaluation across language boundaries  \n",
    "- **üìà Production Readiness:** Deployment considerations and scalability analysis\n",
    "\n",
    "## How This Session Works\n",
    "\n",
    "- **üéì Theory ‚Üí Practice ‚Üí Analysis:** Learn concepts ‚Üí Apply systematically ‚Üí Measure results\n",
    "- **üîß Hands-on Implementation:** Real code, real models, real data\n",
    "- **üìä Quantitative Evaluation:** Every claim backed by systematic measurement\n",
    "- **üíº Production Focus:** Techniques you can use in real projects immediately\n",
    "- **üåç Low-Resource Emphasis:** Special attention to resource-constrained scenarios\n",
    "\n",
    "**‚ö†Ô∏è Important Note:**  \n",
    "This is a **production-oriented demonstration** using systematic methodology. While we use a small dataset for speed, all techniques scale to production systems. The focus is on **understanding systematic approaches** and **building production-ready intuitions**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6142189",
   "metadata": {},
   "source": [
    "## 0. üèóÔ∏è Fine-Tuning Fundamentals: Theory and Practice\n",
    "\n",
    "### 0.1 Fine-Tuning Taxonomy: A Systematic Overview\n",
    "\n",
    "**Fine-tuning** is the process of adapting a pretrained language model to specialized tasks or domains using additional labeled data. Understanding the landscape of approaches is crucial for making informed decisions.\n",
    "\n",
    "| **Approach** | **Parameters Updated** | **Memory Requirement** | **Training Speed** | **Best For** | **Cost** |\n",
    "|--------------|----------------------|----------------------|-------------------|--------------|----------|\n",
    "| **üî• Full Fine-tuning** | All parameters (100%) | Very High (4x model size) | Slow | High-resource tasks | $$$$$ |\n",
    "| **‚ö° Parameter-Efficient (PEFT)** | Small subset (0.1-10%) | Low (1.2x model size) | Fast | Low-resource languages | $$ |\n",
    "| **üéØ LoRA** | Low-rank adapters (~1%) | Very Low | Very Fast | Most practical cases | $ |\n",
    "| **üìö Instruction Tuning** | Task-specific layers | Medium | Medium | Following instructions | $$$ |\n",
    "| **üé™ Preference Optimization** | Value/reward layers | Medium | Medium | Human alignment | $$$ |\n",
    "\n",
    "### 0.2 üî¨ Deep Dive: Parameter-Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "**Why PEFT Matters for Low-Resource Languages:**\n",
    "\n",
    "1. **üí∞ Cost Effectiveness:** Train with 1000x less GPU memory\n",
    "2. **‚ö° Speed:** 10x faster training and deployment  \n",
    "3. **üõ°Ô∏è Catastrophic Forgetting Prevention:** Preserve original capabilities\n",
    "4. **üîÑ Task Switching:** Multiple adapters for different tasks\n",
    "5. **üì¶ Storage Efficiency:** Adapters are ~10MB vs full models at ~10GB\n",
    "\n",
    "### 0.3 üéØ LoRA (Low-Rank Adaptation) Deep Dive\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "```\n",
    "W = W‚ÇÄ + ŒîW = W‚ÇÄ + BA\n",
    "```\n",
    "Where:\n",
    "- `W‚ÇÄ`: Frozen pretrained weights\n",
    "- `B`, `A`: Low-rank matrices (rank r << d) \n",
    "- `ŒîW = BA`: Learned adaptation with r << original rank\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- **Rank (r):** Higher = more expressive but slower (typical: 4-64)\n",
    "- **Alpha (Œ±):** Scaling factor for adaptation strength (typical: 16-32) \n",
    "- **Target Modules:** Which layers to adapt (attention vs MLP vs both)\n",
    "- **Dropout:** Regularization for adaptation layers (typical: 0.05-0.1)\n",
    "\n",
    "### 0.4 üìä Systematic Approach to Fine-Tuning\n",
    "\n",
    "**Our methodology follows production best practices:**\n",
    "\n",
    "1. **üß™ Baseline Establishment:** Test pretrained model performance\n",
    "2. **üìä Systematic Hyperparameter Search:** Grid search over key parameters\n",
    "3. **üìà Multi-Metric Evaluation:** Beyond perplexity - task-specific metrics\n",
    "4. **üîç Ablation Studies:** Understand what drives improvements\n",
    "5. **üíº Production Planning:** Cost analysis and deployment considerations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are in Google Colab, make sure the runtime has a GPU:\n",
    "# Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected. Training will be very slow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries.\n",
    "# In Colab, this cell may take a couple of minutes.\n",
    "!pip install -q transformers datasets accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44716915",
   "metadata": {},
   "source": [
    "## 1. Load a compact multilingual model\n",
    "\n",
    "We choose a relatively small chat tuned model so that:\n",
    "\n",
    "- It fits in Colab GPU memory.\n",
    "- It supports many languages, including low resource ones reasonably well.\n",
    "\n",
    "Here we use the TinyLlama chat model (about 1.1B parameters).  \n",
    "In a real project, the model choice would depend on license constraints, hardware, and language coverage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Some causal language models do not have a pad token set.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Disable cache during training to avoid warnings.\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5960f",
   "metadata": {},
   "source": [
    "## 2. Build a tiny low resource toy dataset\n",
    "\n",
    "We construct a minimal dataset of English to Luxembourgish translation pairs directly in the notebook.  \n",
    "\n",
    "- We treat Luxembourgish (lb) as the low resource language.  \n",
    "- In a real project, you would replace this list with real parallel data or task specific instances.  \n",
    "- The tiny size is intentional so that training finishes in a few minutes for demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f3c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Good morning, how are you?\",\n",
    "        \"target\": \"Gudde Moien, w√©i geet et dir?\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Thank you very much for your help.\",\n",
    "        \"target\": \"Villmools Merci fir deng H√´llef.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"I would like a coffee with milk, please.\",\n",
    "        \"target\": \"Ech h√§tt g√§r eng Taass Kaffi mat M√´llech, wann ech gelift.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Where is the train station?\",\n",
    "        \"target\": \"Wou ass d'Eisebunnsstatioun?\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Today the weather is very cold.\",\n",
    "        \"target\": \"Haut ass d'Wieder ganz kal.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"My name is Anna and I live in Luxembourg.\",\n",
    "        \"target\": \"Ech heeschen Anna an ech wunnen zu L√´tzebuerg.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Could you please speak a little more slowly?\",\n",
    "        \"target\": \"Kanns du w.e.g. e b√´sse m√©i lues schw√§tzen?\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"I am learning Luxembourgish because I work here.\",\n",
    "        \"target\": \"Ech l√©ieren L√´tzebuergesch, well ech hei schaffen.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"The next bus arrives in ten minutes.\",\n",
    "        \"target\": \"Den n√§chste Bus k√´nnt an z√©ng Minutten un.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"This food is delicious.\",\n",
    "        \"target\": \"D√´st Iessen ass lecker.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 11,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"I do not understand, can you repeat that?\",\n",
    "        \"target\": \"Ech verstinn net, kanns du dat widderhuelen?\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 12,\n",
    "        \"language\": \"lb\",\n",
    "        \"source\": \"Have a nice evening.\",\n",
    "        \"target\": \"Sch√©inen Owend nach.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(toy_data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f94cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple split: 75 percent train, 25 percent test.\n",
    "split_dataset = dataset.train_test_split(test_size=0.25, seed=SEED)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Eval size:\", len(eval_dataset))\n",
    "\n",
    "for example in eval_dataset:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd6b29",
   "metadata": {},
   "source": [
    "## 3. Define an instruction style prompt template\n",
    "\n",
    "We wrap each example into a simple instruction prompt so that the model sees:\n",
    "\n",
    "- A system like description.\n",
    "- The English sentence.\n",
    "- A cue to produce the Luxembourgish translation.\n",
    "\n",
    "For training, we construct a single text sequence that contains both the prompt and the target translation.  \n",
    "The model learns to generate the full sequence.  \n",
    "At inference time, we will provide only the prompt and ask the model to continue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b902744",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\n",
    "    \"You are a helpful assistant that translates from English to Luxembourgish.\\n\"\n",
    "    \"Translate the following sentence into Luxembourgish.\\n\\n\"\n",
    "    \"English: {source}\\n\"\n",
    "    \"Luxembourgish:\"\n",
    ")\n",
    "\n",
    "def format_example(example: Dict) -> Dict:\n",
    "    prompt = PROMPT_TEMPLATE.format(source=example[\"source\"])\n",
    "    full_text = prompt + \" \" + example[\"target\"]\n",
    "    return {\n",
    "        \"text\": full_text,\n",
    "        \"language\": example[\"language\"],\n",
    "        \"id\": example[\"id\"],\n",
    "    }\n",
    "\n",
    "formatted_train = train_dataset.map(format_example)\n",
    "formatted_eval = eval_dataset.map(format_example)\n",
    "\n",
    "for e in formatted_train.select(range(2)):\n",
    "    print(\"----\")\n",
    "    print(e[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be6189d",
   "metadata": {},
   "source": [
    "## 4. Baseline model behaviour before fine tuning\n",
    "\n",
    "Before we change any parameters, we check how the base TinyLlama model behaves on our evaluation set.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Use only the prompt part of each example.\n",
    "- Let the model generate a continuation.\n",
    "- Compare the output qualitatively to the target translation.\n",
    "\n",
    "Keep expectations realistic.  \n",
    "The base model may already know some Luxembourgish, but it was not trained specifically for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a41c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(source_sentence: str) -> str:\n",
    "    return PROMPT_TEMPLATE.format(source=source_sentence)\n",
    "\n",
    "def generate_translation(model, tokenizer, source_sentence: str, max_new_tokens: int = 64) -> str:\n",
    "    prompt = build_prompt(source_sentence)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated\n",
    "\n",
    "print(\"### Baseline outputs before fine tuning ###\\n\")\n",
    "\n",
    "for example in eval_dataset:\n",
    "    src = example[\"source\"]\n",
    "    tgt = example[\"target\"]\n",
    "    generated = generate_translation(model, tokenizer, src)\n",
    "    print(\"English:\", src)\n",
    "    print(\"Target Luxembourgish:\", tgt)\n",
    "    print(\"Model output:\")\n",
    "    print(generated)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448174e",
   "metadata": {},
   "source": [
    "## 5. Prepare data for causal language model training\n",
    "\n",
    "We now convert the formatted text examples into token ids suitable for causal language modeling.\n",
    "\n",
    "- Each training instance is a sequence of tokens.\n",
    "- The model will learn to predict the next token given previous tokens.\n",
    "- For simplicity, we use the same token ids as both `input_ids` and `labels`.\n",
    "\n",
    "In a more careful setup, you might mask the loss on prompt tokens and only train on the answer part.  \n",
    "Here we keep the configuration simple so that the mechanics of parameter efficient fine tuning are clear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "def tokenize_function(example: Dict) -> Dict:\n",
    "    result = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # For simple language modeling we use the same ids as labels.\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_train = formatted_train.map(tokenize_function, remove_columns=[\"text\", \"language\", \"id\"])\n",
    "tokenized_eval = formatted_eval.map(tokenize_function, remove_columns=[\"text\", \"language\", \"id\"])\n",
    "\n",
    "print(tokenized_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919deaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for causal language modeling. No masked language modeling.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7998140",
   "metadata": {},
   "source": [
    "## 6. Configure LoRA parameter efficient fine tuning\n",
    "\n",
    "Instead of updating all model parameters, we use LoRA:\n",
    "\n",
    "- LoRA adds small trainable matrices (low rank adapters) to selected linear layers.\n",
    "- The base model weights stay frozen.\n",
    "- This makes fine tuning lighter and more feasible on modest hardware.\n",
    "- It also reduces the risk of catastrophic forgetting.\n",
    "\n",
    "We choose a small rank and apply LoRA to attention projection layers only.  \n",
    "This is a typical starting point for LLaMA like models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be471d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # typical for LLaMA family models\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a35be",
   "metadata": {},
   "source": [
    "## 7. Training configuration\n",
    "\n",
    "We set very conservative training hyper parameters:\n",
    "\n",
    "- Small batch size.\n",
    "- A few epochs over a tiny dataset.\n",
    "- No checkpoint saving to keep the run light.\n",
    "- Logging every step so that you can watch the loss.\n",
    "\n",
    "In a realistic low resource project you would:\n",
    "\n",
    "- Use many more examples.\n",
    "- Run for longer.\n",
    "- Tune hyper parameters carefully.\n",
    "- Monitor validation loss and task specific metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40eee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"tiny_llama_lb_lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    weight_decay=0.0,\n",
    "    fp16=(device == \"cuda\"),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "print(\"\\nTraining completed.\")\n",
    "print(train_result)\n",
    "\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(\"\\nEvaluation metrics:\")\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e80ee",
   "metadata": {},
   "source": [
    "## 8. Compare outputs before and after fine tuning\n",
    "\n",
    "Now we generate translations again using the fine tuned model.  \n",
    "We keep the prompts identical and inspect:\n",
    "\n",
    "- Whether the model is more likely to produce Luxembourgish.\n",
    "- Whether the translations are closer to our target references.\n",
    "- Any side effects such as overfitting to the tiny dataset style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66659223",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Outputs after LoRA fine tuning ###\\n\")\n",
    "\n",
    "for example in eval_dataset:\n",
    "    src = example[\"source\"]\n",
    "    tgt = example[\"target\"]\n",
    "    prompt = build_prompt(src)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = peft_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"English:\", src)\n",
    "    print(\"Target Luxembourgish:\", tgt)\n",
    "    print(\"Model output after fine tuning:\")\n",
    "    print(generated)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ca830",
   "metadata": {},
   "source": [
    "## 9. Quick discussion prompts\n",
    "\n",
    "Discuss in small groups or write down short notes.\n",
    "\n",
    "1. **Data size and quality.**  \n",
    "   - We used 12 examples.  \n",
    "   - What kinds of errors or biases can appear if we deploy a system trained on such a tiny sample?  \n",
    "   - How would you scale the dataset for a real project in a low resource setting?\n",
    "\n",
    "2. **Evaluation.**  \n",
    "   - We only looked at qualitative outputs and language modeling loss.  \n",
    "   - Which task specific metrics would you design for a real application such as translation, classification, or dialogue for a low resource language?  \n",
    "   - How would you build a reliable test set?\n",
    "\n",
    "3. **Safety and robustness.**  \n",
    "   - Fine tuning can change model behaviour in unexpected ways.  \n",
    "   - What additional checks would you perform before using a fine tuned model with real users in a low resource community?\n",
    "\n",
    "4. **Transfer to your language of interest.**  \n",
    "   - Suppose you want to adapt the same pipeline to Armenian or another language.  \n",
    "   - What would you need to change in this notebook?  \n",
    "   - Which parts are reusable, and which parts are specific to the Luxembourgish toy dataset?\n",
    "\n",
    "5. **Beyond LoRA.**  \n",
    "   - Parameter efficient fine tuning is one piece of the puzzle.  \n",
    "   - What other techniques could you combine with LoRA for low resource languages, for example prompting, retrieval augmented generation, multilingual pre training, or synthetic data generation?\n",
    "\n",
    "Use these questions to connect the small scale exercise with the broader methodological and ethical questions of building LLMs for low resource languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c104cc2",
   "metadata": {},
   "source": [
    "## 10. Optional extensions (if you have time)\n",
    "\n",
    "If you finish early, you can explore one of these directions:\n",
    "\n",
    "1. **Add a second language.**  \n",
    "   Extend the toy dataset with a few examples for another low resource language, for example Armenian or Kurdish, and see how the model behaves.\n",
    "\n",
    "2. **Loss masking.**  \n",
    "   Modify the tokenization step so that the loss is computed only on the answer part of each example and not on the prompt.\n",
    "\n",
    "3. **Temperature sweep.**  \n",
    "   Generate outputs with different sampling temperatures and reflect on how diversity and correctness trade off.\n",
    "\n",
    "4. **Save and reload adapters.**  \n",
    "   Use `peft_model.save_pretrained` to save only the LoRA adapters, then reload them on top of the base model in a fresh notebook.\n",
    "\n",
    "These small experiments help build intuition about how parameter efficient fine tuning interacts with low resource data and multilingual models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
