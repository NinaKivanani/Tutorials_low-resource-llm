{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfdb72f8",
   "metadata": {},
   "source": [
    "# Session 2: Pretrained Models and Prompt Engineering ü§ñ\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**üìö Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/yerevan_winter_school_cross_lingual_prompting_tutorial.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
    "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to **Session 2**! You'll master prompt engineering strategies that work across languages, with special focus on low-resource language challenges.\n",
    "\n",
    "**üéØ Focus:** LLM prompting, few-shot learning, Chain-of-Thought reasoning  \n",
    "**üíª Requirements:** Internet access for APIs OR local GPU for models\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**üìã Recommended learning path:**\n",
    "1. **Session 0:** Setup and tokenization basics ‚úÖ  \n",
    "2. **Session 1:** Baseline summarization techniques ‚úÖ\n",
    "3. **This session (Session 2):** LLM prompt engineering ‚Üê You are here!\n",
    "\n",
    "## What You Will Master\n",
    "\n",
    "1. **üèóÔ∏è Pretrained model families** and access patterns (APIs vs. local vs. hosted)\n",
    "2. **üé® Prompt engineering vs. prompt design** - the crucial distinction\n",
    "3. **üéØ Advanced prompting strategies** (zero-shot, few-shot, Chain-of-Thought)\n",
    "4. **üåç Cross-lingual prompt transfer** and adaptation techniques\n",
    "5. **üìä Systematic evaluation** for correctness, fluency, and cultural appropriateness\n",
    "6. **üí¨ Evidence-based discussion** on what works for low-resource languages\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will:\n",
    "- ‚úÖ Choose the right model access pattern for your use case\n",
    "- ‚úÖ Design culturally appropriate prompts for your target language  \n",
    "- ‚úÖ Apply systematic prompt engineering methodology\n",
    "- ‚úÖ Evaluate LLM outputs using multiple dimensions\n",
    "- ‚úÖ Lead evidence-based discussions on cross-lingual performance\n",
    "- ‚úÖ Create actionable recommendations for low-resource language projects\n",
    "\n",
    "## How This Session Works\n",
    "\n",
    "- **üéì Theory + Practice:** Learn concepts then apply them immediately\n",
    "- **üî¨ Systematic Experiments:** Structured methodology, not random testing\n",
    "- **üìä Data-Driven Analysis:** Quantitative evaluation with pandas/visualization\n",
    "- **üí¨ Collaborative Learning:** Small-group discussions with concrete evidence\n",
    "- **üåç Language Focus:** English + your chosen low-resource language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22207704",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Run the following cell to install and import the Python libraries we will use to organize and summarize your observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef6d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a2071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"‚úÖ Ready for Session 2: Prompt Engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d471dc",
   "metadata": {},
   "source": [
    "## 1. Pretrained Model Families and Access Patterns üèóÔ∏è\n",
    "\n",
    "### 1.1 Understanding the Landscape\n",
    "\n",
    "**The prompt engineering revolution** has transformed how we interact with language models. Instead of fine-tuning models for each task, we now craft instructions that guide pretrained models to solve problems through natural language interfaces.\n",
    "\n",
    "### 1.2 Model Families Overview\n",
    "\n",
    "| **Model Family** | **Strengths** | **Multilingual Support** | **Best For** |\n",
    "|------------------|---------------|-------------------------|--------------|\n",
    "| **ü§ñ GPT Family** | Creative, conversational | Limited (English-centric) | Creative writing, coding |\n",
    "| **üåç Claude Family** | Helpful, harmless, honest | Growing multilingual | Analysis, reasoning |\n",
    "| **üìö T5/mT5 Family** | Text-to-text, multilingual | Excellent (mT5) | Structured tasks, low-resource languages |\n",
    "| **üîÑ LLaMA/Mistral** | Open source, efficient | Varies by model | Research, customization |\n",
    "| **üåê PaLM/Gemini** | Multimodal, reasoning | Strong multilingual | Complex reasoning tasks |\n",
    "\n",
    "### 1.3 Access Patterns: How to Use These Models\n",
    "\n",
    "**üîë Key Decision:** How will you access language model capabilities?\n",
    "\n",
    "| **Access Pattern** | **Pros** | **Cons** | **Best For** |\n",
    "|-------------------|----------|----------|--------------|\n",
    "| **üì° API Access** | No setup, latest models, scalable | Cost per token, internet required | Production apps, experiments |\n",
    "| **üè† Local Models** | Privacy, offline, customizable | Requires GPU, setup complexity | Research, sensitive data |\n",
    "| **‚òÅÔ∏è Hosted Interfaces** | Easy to use, free tiers | Limited control, may have usage limits | Learning, prototyping |\n",
    "\n",
    "### 1.4 Prompt Engineering vs. Prompt Design\n",
    "\n",
    "**üé® Prompt Design** = Creative craft of writing individual prompts  \n",
    "**üîß Prompt Engineering** = Systematic methodology for consistent results\n",
    "\n",
    "**Key Principles:**\n",
    "1. **üéØ Clarity over Creativity:** Be explicit about what you want\n",
    "2. **üìè Structure Matters:** Use consistent formatting \n",
    "3. **üåç Cultural Context:** Adapt for your target language/culture\n",
    "4. **üî¨ Test Systematically:** Use data, not intuition\n",
    "5. **üìä Measure Everything:** Correctness, fluency, appropriateness\n",
    "\n",
    "### 1.5 Why Low-Resource Languages Are Different\n",
    "\n",
    "**Common Challenges:**\n",
    "- **üìö Limited Training Data:** Models see less text in your language\n",
    "- **üî§ Tokenization Issues:** Poor subword splitting increases costs\n",
    "- **üåç Cultural Gaps:** Western-centric training data\n",
    "- **üìù Script Variations:** Multiple writing systems or romanization\n",
    "- **üí¨ Code-Switching:** Mixed language use in real conversations\n",
    "\n",
    "**Success Strategies:**\n",
    "- Use multilingual models (mT5, XLM-R based systems)\n",
    "- Provide examples in your target language (few-shot learning)\n",
    "- Be explicit about cultural context\n",
    "- Test with native speakers\n",
    "- Start with simpler tasks and build complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a1e60",
   "metadata": {},
   "source": [
    "## 2. üöÄ Quick Start: Model Access Setup\n",
    "\n",
    "Choose your approach based on your resources and requirements:\n",
    "\n",
    "### Option A: üì° API Access (Recommended for Production)\n",
    "```python\n",
    "# Example API setup (uncomment and configure)\n",
    "# import openai\n",
    "# openai.api_key = \"your-api-key\"  # Never commit API keys!\n",
    "\n",
    "# Example usage function\n",
    "def call_api_model(prompt, model=\"gpt-3.5-turbo\", max_tokens=150):\n",
    "    \\\"\\\"\\\"Call API model - replace with your preferred API\\\"\\\"\\\"\n",
    "    # return openai.Completion.create(model=model, prompt=prompt, max_tokens=max_tokens)\n",
    "    return \"[API call would happen here - paste actual response]\"\n",
    "\n",
    "print(\"üì° API Access: Great for latest models, requires internet + API key\")\n",
    "```\n",
    "\n",
    "### Option B: üè† Local Models (For Privacy/Research)\n",
    "```python\n",
    "# Example local model setup (uncomment if you have GPU)\n",
    "# from transformers import pipeline\n",
    "# generator = pipeline(\"text-generation\", model=\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "def call_local_model(prompt, max_length=150):\n",
    "    \\\"\\\"\\\"Call local model - uncomment if you have local setup\\\"\\\"\\\"\n",
    "    # return generator(prompt, max_length=max_length)\n",
    "    return \"[Local model would run here - paste actual response]\"\n",
    "\n",
    "print(\"üè† Local Models: Great for privacy, requires GPU setup\")\n",
    "```\n",
    "\n",
    "### Option C: ‚òÅÔ∏è Web Interfaces (For Learning)\n",
    "```python\n",
    "def call_web_interface(prompt):\n",
    "    \\\"\\\"\\\"Use ChatGPT, Claude, Poe, etc. - manual copy/paste\\\"\\\"\\\"\n",
    "    print(f\"üìã Copy this prompt to your web interface:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(prompt)\n",
    "    print(\"-\" * 50)\n",
    "    return \"[Paste the response from your web interface here]\"\n",
    "\n",
    "print(\"‚òÅÔ∏è Web Interfaces: Easy to start, good for experiments\")\n",
    "```\n",
    "\n",
    "**üí° For this tutorial:** We'll design prompts systematically and test them using your preferred method above.\n",
    "\n",
    "## 3. üéØ Strategic Task and Language Selection\n",
    "\n",
    "### 3.1 Why Strategic Selection Matters\n",
    "\n",
    "**üéØ Task Selection Strategy:**\n",
    "- **Start Simple:** Classification/QA before complex reasoning\n",
    "- **Real Impact:** Choose tasks relevant to your community's needs  \n",
    "- **Measurable:** Pick tasks with clear success criteria\n",
    "- **Testable:** Ensure you can evaluate results objectively\n",
    "\n",
    "**üåç Language Selection Strategy:**\n",
    "- **Always Include English:** Baseline for comparison\n",
    "- **Choose Meaningfully:** Pick languages you can evaluate properly\n",
    "- **Consider Resources:** How much training data exists?\n",
    "- **Think Culturally:** Are there cultural nuances to test?\n",
    "\n",
    "### 3.2 Recommended Task Types for Cross-Lingual Evaluation\n",
    "\n",
    "| **Task Type** | **Complexity** | **Cultural Sensitivity** | **Evaluation** | **Good For** |\n",
    "|---------------|----------------|------------------------|----------------|--------------|\n",
    "| **üìä Classification** | Low | Medium | Clear metrics | Beginners, systematic testing |\n",
    "| **‚ùì Factual QA** | Medium | Low | Objective answers | Knowledge transfer testing |\n",
    "| **üîç Sentiment Analysis** | Medium | High | Cultural context matters | Cross-cultural analysis |\n",
    "| **üìù Summarization** | High | Medium | Subjective evaluation | Advanced prompting |\n",
    "| **üîÑ Translation Quality** | High | High | Native speaker needed | Language pair analysis |\n",
    "\n",
    "### 3.3 Configure Your Experiment\n",
    "\n",
    "You will work with:\n",
    "- **At least one task type** (start with classification or factual QA)\n",
    "- **English** (always include for baseline comparison)  \n",
    "- **At least one low-resource language** (Armenian, Luxembourgish, Kurdish, etc.)\n",
    "\n",
    "**üí° Pro tip:** Pick languages where you can judge output quality or have native speaker access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295141c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Model Access Setup - Choose Your Approach\n",
    "# Run the cell that matches your preferred setup\n",
    "\n",
    "# Option A: API Access (replace with your API)\n",
    "def call_api_model(prompt, model=\"gpt-3.5-turbo\", max_tokens=150):\n",
    "    \"\"\"Call API model - replace with your preferred API\"\"\"\n",
    "    print(\"üì° API Mode: Copy prompt to your API interface\")\n",
    "    print(\"=\"*50)\n",
    "    print(prompt)\n",
    "    print(\"=\"*50)\n",
    "    return \"[Replace with actual API response]\"\n",
    "\n",
    "# Option B: Local model (uncomment if you have GPU + transformers)\n",
    "def call_local_model(prompt, max_length=150):\n",
    "    \"\"\"Call local model - requires local setup\"\"\"\n",
    "    print(\"üè† Local Mode: Copy prompt to your local model\")\n",
    "    print(\"=\"*50) \n",
    "    print(prompt)\n",
    "    print(\"=\"*50)\n",
    "    return \"[Replace with local model response]\"\n",
    "\n",
    "# Option C: Web interface (manual copy-paste)\n",
    "def call_web_interface(prompt):\n",
    "    \"\"\"Use web interfaces like ChatGPT, Claude, etc.\"\"\"\n",
    "    print(\"‚òÅÔ∏è Web Interface Mode:\")\n",
    "    print(\"1. Copy the prompt below\")\n",
    "    print(\"2. Paste into ChatGPT/Claude/Poe/etc.\")\n",
    "    print(\"3. Copy response back to experiments table\")\n",
    "    print(\"=\"*50)\n",
    "    print(prompt)\n",
    "    print(\"=\"*50)\n",
    "    return \"[Replace with web interface response]\"\n",
    "\n",
    "# Choose your preferred method\n",
    "selected_method = call_web_interface  # Change to call_api_model or call_local_model as needed\n",
    "\n",
    "print(\"‚úÖ Model access configured!\")\n",
    "print(\"üí° You can switch methods during experiments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe7d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define languages you will use.\n",
    "# You must include English (`en`) plus at least one low resource language.\n",
    "\n",
    "languages = [\n",
    "    {\"code\": \"en\", \"name\": \"English\"},\n",
    "    {\"code\": \"lb\", \"name\": \"Luxembourgish\"},  # change to your low resource language if you prefer\n",
    "]\n",
    "\n",
    "languages_df = pd.DataFrame(languages)\n",
    "languages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd941e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tasks you want to explore.\n",
    "# Example tasks. you can modify names and descriptions.\n",
    "\n",
    "tasks = [\n",
    "    {\n",
    "        \"task_id\": 1,\n",
    "        \"task_name\": \"sentiment_classification\",\n",
    "        \"description\": \"Classify a short review as positive or negative.\",\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": 2,\n",
    "        \"task_name\": \"factoid_qa\",\n",
    "        \"description\": \"Answer a short factual question based on background knowledge.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "tasks_df = pd.DataFrame(tasks)\n",
    "tasks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6f7da",
   "metadata": {},
   "source": [
    "You can modify `languages` and `tasks` above to fit your interests.\n",
    "\n",
    "- Add or remove rows in the lists.  \n",
    "- Make sure each `task_id` is unique.  \n",
    "- Use task names that are short and descriptive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e74118",
   "metadata": {},
   "source": [
    "## 2. Define input items for each task and language\n",
    "\n",
    "For each `(task, language)` pair, you will define a small set of input items and their expected answers or labels.\n",
    "\n",
    "Examples.\n",
    "\n",
    "- For sentiment classification, the input might be a short review and the label is `positive` or `negative`.  \n",
    "- For question answering, the input might be a question and the expected answer is a short phrase.\n",
    "\n",
    "Fill in the table below with your own items. The examples are just placeholders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4652a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input items per task and language.\n",
    "# You can replace the examples with your own entries.\n",
    "\n",
    "items = [\n",
    "    # Sentiment classification. English.\n",
    "    {\n",
    "        \"item_id\": 1,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"en\",\n",
    "        \"input_text\": \"The restaurant was amazing, the food was fresh and the staff were friendly.\",\n",
    "        \"expected_output\": \"positive\",\n",
    "    },\n",
    "    {\n",
    "        \"item_id\": 2,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"en\",\n",
    "        \"input_text\": \"The movie was too long and incredibly boring.\",\n",
    "        \"expected_output\": \"negative\",\n",
    "    },\n",
    "    # Sentiment classification. Luxembourgish (or your low resource language).\n",
    "    {\n",
    "        \"item_id\": 3,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"input_text\": \"D'Iessen am Restaurant war super, an d'Personal war ganz fr√´ndlech.\",\n",
    "        \"expected_output\": \"positive\",\n",
    "    },\n",
    "    {\n",
    "        \"item_id\": 4,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"input_text\": \"De Film war ze laang an immens langweileg.\",\n",
    "        \"expected_output\": \"negative\",\n",
    "    },\n",
    "    # Factoid QA. English.\n",
    "    {\n",
    "        \"item_id\": 5,\n",
    "        \"task_id\": 2,\n",
    "        \"language_code\": \"en\",\n",
    "        \"input_text\": \"Which city is the capital of Armenia?\",\n",
    "        \"expected_output\": \"Yerevan\",\n",
    "    },\n",
    "    # Factoid QA. Luxembourgish (or your low resource language).\n",
    "    {\n",
    "        \"item_id\": 6,\n",
    "        \"task_id\": 2,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"input_text\": \"W√©i eng Stad ass d'Haaptstad vun Armenien?\",\n",
    "        \"expected_output\": \"Yerevan\",\n",
    "    },\n",
    "]\n",
    "\n",
    "items_df = pd.DataFrame(items)\n",
    "items_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b209d",
   "metadata": {},
   "source": [
    "Feel free to.\n",
    "\n",
    "- Add more `item_id` rows for each `(task, language)`.  \n",
    "- Change the texts to match domains relevant to your context.  \n",
    "- Keep items short so that you can run experiments quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08bdb4d",
   "metadata": {},
   "source": [
    "## 3. Prompt design. zero shot, few shot, and Chain of Thought\n",
    "\n",
    "For each task and language, you will design three categories of prompts.\n",
    "\n",
    "1. **Zero shot**. The model sees only the task instruction and the input.  \n",
    "2. **Few shot**. The model sees the instruction plus a few example pairs (input and correct output).  \n",
    "3. **Chain of Thought (CoT)**. The model sees an instruction that explicitly asks for step by step reasoning before the final answer.\n",
    "\n",
    "You can define prompt *templates* that you will adapt to each input item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e160515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base instruction templates for each task and language.\n",
    "# These are generic instructions that you will adapt for zero shot, few shot, and CoT variants.\n",
    "\n",
    "prompt_templates = [\n",
    "    {\n",
    "        \"template_id\": 1,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"en\",\n",
    "        \"prompt_type\": \"zero_shot\",\n",
    "        \"template_text\": (\n",
    "            \"You are a helpful assistant.\\n\"\n",
    "            \"Classify the sentiment of the following review as 'positive' or 'negative'.\\n\\n\"\n",
    "            \"Review. {input_text}\\n\"\n",
    "            \"Sentiment.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 2,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"en\",\n",
    "        \"prompt_type\": \"cot\",\n",
    "        \"template_text\": (\n",
    "            \"You are a helpful assistant.\\n\"\n",
    "            \"First, briefly explain why the review is positive or negative.\\n\"\n",
    "            \"Then, on a new line, answer with 'positive' or 'negative'.\\n\\n\"\n",
    "            \"Review. {input_text}\\n\"\n",
    "            \"Explanation and answer.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 3,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"prompt_type\": \"zero_shot\",\n",
    "        \"template_text\": (\n",
    "            \"Du bass en h√´llefsbereeten Assistent.\\n\"\n",
    "            \"Klassifiz√©ier d'St√´mmung vun d√´ser Kritik als 'positiv' oder 'negativ'.\\n\\n\"\n",
    "            \"Kritik. {input_text}\\n\"\n",
    "            \"St√´mmung.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 4,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"prompt_type\": \"cot\",\n",
    "        \"template_text\": (\n",
    "            \"Du bass en h√´llefsbereeten Assistent.\\n\"\n",
    "            \"Erkl√§r kuerz firwat d'Kritik positiv oder negativ ass.\\n\"\n",
    "            \"Duerno, op enger neier Zeil, √§ntwere just mat 'positiv' oder 'negativ'.\\n\\n\"\n",
    "            \"Kritik. {input_text}\\n\"\n",
    "            \"Erkl√§rung an √Ñntwert.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 5,\n",
    "        \"task_id\": 2,\n",
    "        \"language_code\": \"en\",\n",
    "        \"prompt_type\": \"zero_shot\",\n",
    "        \"template_text\": (\n",
    "            \"You are a helpful assistant.\\n\"\n",
    "            \"Answer the question briefly and accurately.\\n\\n\"\n",
    "            \"Question. {input_text}\\n\"\n",
    "            \"Answer.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 6,\n",
    "        \"task_id\": 2,\n",
    "        \"language_code\": \"en\",\n",
    "        \"prompt_type\": \"cot\",\n",
    "        \"template_text\": (\n",
    "            \"You are a helpful assistant.\\n\"\n",
    "            \"Think step by step to answer the question, then give a short final answer on a new line.\\n\\n\"\n",
    "            \"Question. {input_text}\\n\"\n",
    "            \"Reasoning and final answer.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 7,\n",
    "        \"task_id\": 2,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"prompt_type\": \"zero_shot\",\n",
    "        \"template_text\": (\n",
    "            \"Du bass en h√´llefsbereeten Assistent.\\n\"\n",
    "            \"Be√§ntwer d'Fro kuerz a pr√§zis.\\n\\n\"\n",
    "            \"Fro. {input_text}\\n\"\n",
    "            \"√Ñntwert.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 8,\n",
    "        \"task_id\": 2,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"prompt_type\": \"cot\",\n",
    "        \"template_text\": (\n",
    "            \"Du bass en h√´llefsbereeten Assistent.\\n\"\n",
    "            \"Denke Schr√´tt fir Schr√´tt fir d'Fro ze be√§ntweren, an duerno ginn eng kuerz √Ñntwert op enger neier Zeil.\\n\\n\"\n",
    "            \"Fro. {input_text}\\n\"\n",
    "            \"Iwwerleeung an √Ñntwert.\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "templates_df = pd.DataFrame(prompt_templates)\n",
    "templates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f69c0",
   "metadata": {},
   "source": [
    "Few shot prompts are typically created by adding one or more example pairs above the current input.\n",
    "\n",
    "You can construct few shot prompts manually in the evaluation table later, or you can add explicit few shot templates with placeholders for examples.\n",
    "\n",
    "For this tutorial, we keep few shot prompts flexible and let you write them directly in the experiments table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8d7ad",
   "metadata": {},
   "source": [
    "## 4. Running experiments and recording outputs\n",
    "\n",
    "You will now define an **experiments table** where each row represents one model run.\n",
    "\n",
    "Each row stores.\n",
    "\n",
    "- Task and language.  \n",
    "- Model name.  \n",
    "- Prompting strategy (zero shot, few shot, CoT).  \n",
    "- The exact prompt you used.  \n",
    "- The model output.  \n",
    "- Your ratings for correctness, fluency, and cultural appropriateness.\n",
    "\n",
    "Start with a few example rows, then duplicate and edit them as you run more experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e81a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for experiments with a small example.\n",
    "# Replace the placeholder values as you run real experiments.\n",
    "\n",
    "experiments = [\n",
    "    {\n",
    "        \"experiment_id\": 1,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"en\",\n",
    "        \"item_id\": 1,\n",
    "        \"model_name\": \"ModelA\",          # e.g. \"gpt4o\", \"claude\", \"llama3\", etc.\n",
    "        \"prompt_type\": \"zero_shot\",      # \"zero_shot\", \"few_shot\", or \"cot\"\n",
    "        \"shots_used\": 0,                 # number of in context examples (0 for zero shot)\n",
    "        \"prompt_text\": (\n",
    "            \"You are a helpful assistant.\\n\"\n",
    "            \"Classify the sentiment of the following review as 'positive' or 'negative'.\\n\\n\"\n",
    "            \"Review. The restaurant was amazing, the food was fresh and the staff were friendly.\\n\"\n",
    "            \"Sentiment.\"\n",
    "        ),\n",
    "        \"model_output\": \"[paste output here]\",\n",
    "        # Ratings (see section 5 for guidance).\n",
    "        \"correctness\": None,             # 0. incorrect, 1. partially correct, 2. fully correct\n",
    "        \"fluency\": None,                 # 0. poor, 1. acceptable, 2. very fluent\n",
    "        \"cultural_appropriateness\": None,# 0. problematic, 1. acceptable, 2. very appropriate\n",
    "        \"notes\": \"\",\n",
    "    },\n",
    "    # Add more rows here for other prompt types, models, languages, and items.\n",
    "]\n",
    "\n",
    "experiments_df = pd.DataFrame(experiments)\n",
    "experiments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acffcb02",
   "metadata": {},
   "source": [
    "For each new experiment you run.\n",
    "\n",
    "1. Duplicate an existing row in `experiments`.  \n",
    "2. Update `experiment_id` to a new unique number.  \n",
    "3. Set `task_id`, `language_code`, `item_id`, `model_name`, and `prompt_type`.  \n",
    "4. Copy the exact `prompt_text` you used.  \n",
    "5. Paste the `model_output`.  \n",
    "6. Fill in the rating fields after reading the output carefully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935daa30",
   "metadata": {},
   "source": [
    "## 5. Rating guidelines\n",
    "\n",
    "Use the following scales when annotating each experiment.\n",
    "\n",
    "### 5.1 Correctness (`correctness`)\n",
    "\n",
    "- **2. fully correct**. The answer matches the expected label or answer, with no major errors.  \n",
    "- **1. partially correct**. The answer is close but not perfect (for example correct label but with an incorrect explanation, or correct core fact with minor mistakes).  \n",
    "- **0. incorrect**. The answer does not match the expected output or is clearly wrong.\n",
    "\n",
    "### 5.2 Fluency (`fluency`)\n",
    "\n",
    "- **2. very fluent**. The output is natural and grammatical in the target language.  \n",
    "- **1. acceptable**. Some minor issues, but overall understandable.  \n",
    "- **0. poor**. The output has many errors or is difficult to understand.\n",
    "\n",
    "### 5.3 Cultural appropriateness (`cultural_appropriateness`)\n",
    "\n",
    "- **2. very appropriate**. The output is respectful and fits the cultural context well.  \n",
    "- **1. acceptable**. No serious issues, but maybe slightly awkward.  \n",
    "- **0. problematic**. The output feels insensitive, inappropriate, or culturally misleading.\n",
    "\n",
    "Use the `notes` field to document interesting cases (for example when CoT reasoning is correct in English but fails in your low resource language).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4777d6e",
   "metadata": {},
   "source": [
    "## 6. Summarizing results\n",
    "\n",
    "Once you have filled `experiments_df` with several rows, you can compute simple summaries by language, model, and prompt type.\n",
    "\n",
    "Run the cell below for an overview.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(experiments_df) == 0:\n",
    "    print(\"experiments_df is empty. Please add some experiments first.\")\n",
    "else:\n",
    "    summary = experiments_df.groupby([\"model_name\", \"language_code\", \"prompt_type\"])[\n",
    "        [\"correctness\", \"fluency\", \"cultural_appropriateness\"]\n",
    "    ].mean()\n",
    "\n",
    "    count = experiments_df.groupby([\"model_name\", \"language_code\", \"prompt_type\"])[\n",
    "        \"experiment_id\"\n",
    "    ].count().rename(\"num_examples\")\n",
    "\n",
    "    summary = summary.join(count)\n",
    "    print(\"Average scores by model, language, and prompt type (scale 0-2):\")\n",
    "    display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d48e11",
   "metadata": {},
   "source": [
    "### 6.1 Visualizing the effect of prompt type\n",
    "\n",
    "If you have enough experiments, you can plot average correctness by prompt type for a given model and language.\n",
    "\n",
    "Edit the variables in the cell below to match your setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure which model and language you want to visualize.\n",
    "target_model = \"ModelA\"   # change this to a real model name used in your experiments\n",
    "target_language = \"en\"    # e.g. \"lb\" for your low resource language\n",
    "\n",
    "subset = experiments_df[\n",
    "    (experiments_df[\"model_name\"] == target_model)\n",
    "    & (experiments_df[\"language_code\"] == target_language)\n",
    "]\n",
    "\n",
    "if len(subset) == 0:\n",
    "    print(\"No experiments found for the chosen model and language.\")\n",
    "else:\n",
    "    avg_by_prompt = subset.groupby(\"prompt_type\")[\n",
    "        [\"correctness\", \"fluency\", \"cultural_appropriateness\"]\n",
    "    ].mean()\n",
    "\n",
    "    avg_by_prompt = avg_by_prompt.reindex([\"zero_shot\", \"few_shot\", \"cot\"]).dropna(how=\"all\")\n",
    "\n",
    "    if avg_by_prompt.empty:\n",
    "        print(\"No data to plot. Check that you used 'zero_shot', 'few_shot', and 'cot' labels.\")\n",
    "    else:\n",
    "        avg_by_prompt[\"correctness\"].plot(kind=\"bar\")\n",
    "        plt.ylim(0, 2)\n",
    "        plt.ylabel(\"Average correctness (0-2)\")\n",
    "        plt.title(f\"Effect of prompt type on correctness ({target_model}, {target_language})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1d135",
   "metadata": {},
   "source": [
    "You can duplicate and adapt this cell to compare.\n",
    "\n",
    "- English vs your low resource language for the same model.  \n",
    "- Different models for the same language and prompt type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2975c61",
   "metadata": {},
   "source": [
    "## 7. Exporting your annotations\n",
    "\n",
    "If you want to share your results or combine them with other groups, you can export the experiments table to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"cross_lingual_prompting_experiments.csv\"\n",
    "experiments_df.to_csv(output_path, index=False)\n",
    "print(f\"Experiments exported to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d93f30",
   "metadata": {},
   "source": [
    "## 8. Reflection and small group discussion\n",
    "\n",
    "Use the questions below for your final discussion.\n",
    "\n",
    "1. **Prompt type effects.**  \n",
    "   - In your experiments, did few shot prompting improve correctness compared to zero shot for your low resource language.  \n",
    "   - Did Chain of Thought instructions help or confuse the model output in your language.\n",
    "\n",
    "2. **Cross lingual differences.**  \n",
    "   - Did the same prompt design work equally well in English and in your low resource language.  \n",
    "   - Were there cases where reasoning was correct in English but failed when you translated the prompt.\n",
    "\n",
    "3. **Fluency and cultural appropriateness.**  \n",
    "   - Did CoT prompts degrade fluency or make outputs more verbose than needed.  \n",
    "   - Were there outputs that felt culturally inappropriate or out of place in your language.\n",
    "\n",
    "4. **Practical recommendations.**  \n",
    "   - Based on your observations, what simple rules of thumb would you give to someone designing prompts for your language.  \n",
    "   - Which combinations of prompt type, language, and model would you recommend for real applications.\n",
    "\n",
    "You can use the markdown cell below to write down key points from your discussion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa65e57d",
   "metadata": {},
   "source": [
    "### 8.1 Notes from your group\n",
    "\n",
    "Use this space to summarize your main takeaways.\n",
    "\n",
    "- What worked well (prompt types, models, languages).  \n",
    "- What did not work well.  \n",
    "- Surprising observations.  \n",
    "- Open questions you would like to explore in more systematic studies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
