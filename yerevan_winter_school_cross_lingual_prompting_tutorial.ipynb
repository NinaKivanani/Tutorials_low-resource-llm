{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfdb72f8",
   "metadata": {},
   "source": [
    "# Yerevan Winter School Tutorial 4\n",
    "## Cross Lingual Prompting. Zero Shot, Few Shot, and Chain of Thought\n",
    "\n",
    "**Context.**  \n",
    "In this hands on session, you will experiment with prompt design for simple tasks in English and in a chosen low resource language.  \n",
    "You will compare zero shot and few shot prompting, explore Chain of Thought (CoT) instructions, and rate model outputs on several dimensions.\n",
    "\n",
    "**What you will do.**\n",
    "\n",
    "1. Select one or two simple tasks (for example text classification or question answering).  \n",
    "2. Design prompts in English and in a low resource language of your choice.  \n",
    "3. Compare zero shot and few shot prompting.  \n",
    "4. Add Chain of Thought instructions and see whether the model follows reasoning steps across languages.  \n",
    "5. Rate outputs for correctness, fluency, and cultural appropriateness using a small evaluation sheet.  \n",
    "6. Discuss which prompt design choices improved or harmed performance for your target language.\n",
    "\n",
    "**Important note.**  \n",
    "You will run LLM queries outside this notebook (for example in Poe, ChatGPT, or another interface) and paste outputs back into the tables here.  \n",
    "The notebook acts as a structured worksheet and analysis tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22207704",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Run the following cell to install and import the Python libraries we will use to organize and summarize your observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef6d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a2071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a1e60",
   "metadata": {},
   "source": [
    "## 1. Choose tasks and languages\n",
    "\n",
    "You will work with.\n",
    "\n",
    "- At least one task type (for example classification, question answering, translation quality judgement).  \n",
    "- English (`en`).  \n",
    "- At least one low resource language of your choice (for example Armenian `hy`, Luxembourgish `lb`, Kurdish `ku`, etc.).\n",
    "\n",
    "Edit the configuration below to match your choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe7d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define languages you will use.\n",
    "# You must include English (`en`) plus at least one low resource language.\n",
    "\n",
    "languages = [\n",
    "    {\"code\": \"en\", \"name\": \"English\"},\n",
    "    {\"code\": \"lb\", \"name\": \"Luxembourgish\"},  # change to your low resource language if you prefer\n",
    "]\n",
    "\n",
    "languages_df = pd.DataFrame(languages)\n",
    "languages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd941e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tasks you want to explore.\n",
    "# Example tasks. you can modify names and descriptions.\n",
    "\n",
    "tasks = [\n",
    "    {\n",
    "        \"task_id\": 1,\n",
    "        \"task_name\": \"sentiment_classification\",\n",
    "        \"description\": \"Classify a short review as positive or negative.\",\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": 2,\n",
    "        \"task_name\": \"factoid_qa\",\n",
    "        \"description\": \"Answer a short factual question based on background knowledge.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "tasks_df = pd.DataFrame(tasks)\n",
    "tasks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6f7da",
   "metadata": {},
   "source": [
    "You can modify `languages` and `tasks` above to fit your interests.\n",
    "\n",
    "- Add or remove rows in the lists.  \n",
    "- Make sure each `task_id` is unique.  \n",
    "- Use task names that are short and descriptive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e74118",
   "metadata": {},
   "source": [
    "## 2. Define input items for each task and language\n",
    "\n",
    "For each `(task, language)` pair, you will define a small set of input items and their expected answers or labels.\n",
    "\n",
    "Examples.\n",
    "\n",
    "- For sentiment classification, the input might be a short review and the label is `positive` or `negative`.  \n",
    "- For question answering, the input might be a question and the expected answer is a short phrase.\n",
    "\n",
    "Fill in the table below with your own items. The examples are just placeholders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4652a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input items per task and language.\n",
    "# You can replace the examples with your own entries.\n",
    "\n",
    "items = [\n",
    "    # Sentiment classification. English.\n",
    "    {\n",
    "        \"item_id\": 1,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"en\",\n",
    "        \"input_text\": \"The restaurant was amazing, the food was fresh and the staff were friendly.\",\n",
    "        \"expected_output\": \"positive\",\n",
    "    },\n",
    "    {\n",
    "        \"item_id\": 2,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"en\",\n",
    "        \"input_text\": \"The movie was too long and incredibly boring.\",\n",
    "        \"expected_output\": \"negative\",\n",
    "    },\n",
    "    # Sentiment classification. Luxembourgish (or your low resource language).\n",
    "    {\n",
    "        \"item_id\": 3,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"input_text\": \"D'Iessen am Restaurant war super, an d'Personal war ganz frëndlech.\",\n",
    "        \"expected_output\": \"positive\",\n",
    "    },\n",
    "    {\n",
    "        \"item_id\": 4,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"input_text\": \"De Film war ze laang an immens langweileg.\",\n",
    "        \"expected_output\": \"negative\",\n",
    "    },\n",
    "    # Factoid QA. English.\n",
    "    {\n",
    "        \"item_id\": 5,\n",
    "        \"task_id\": 2,\n",
    "        \"language_code\": \"en\",\n",
    "        \"input_text\": \"Which city is the capital of Armenia?\",\n",
    "        \"expected_output\": \"Yerevan\",\n",
    "    },\n",
    "    # Factoid QA. Luxembourgish (or your low resource language).\n",
    "    {\n",
    "        \"item_id\": 6,\n",
    "        \"task_id\": 2,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"input_text\": \"Wéi eng Stad ass d'Haaptstad vun Armenien?\",\n",
    "        \"expected_output\": \"Yerevan\",\n",
    "    },\n",
    "]\n",
    "\n",
    "items_df = pd.DataFrame(items)\n",
    "items_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b209d",
   "metadata": {},
   "source": [
    "Feel free to.\n",
    "\n",
    "- Add more `item_id` rows for each `(task, language)`.  \n",
    "- Change the texts to match domains relevant to your context.  \n",
    "- Keep items short so that you can run experiments quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08bdb4d",
   "metadata": {},
   "source": [
    "## 3. Prompt design. zero shot, few shot, and Chain of Thought\n",
    "\n",
    "For each task and language, you will design three categories of prompts.\n",
    "\n",
    "1. **Zero shot**. The model sees only the task instruction and the input.  \n",
    "2. **Few shot**. The model sees the instruction plus a few example pairs (input and correct output).  \n",
    "3. **Chain of Thought (CoT)**. The model sees an instruction that explicitly asks for step by step reasoning before the final answer.\n",
    "\n",
    "You can define prompt *templates* that you will adapt to each input item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e160515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base instruction templates for each task and language.\n",
    "# These are generic instructions that you will adapt for zero shot, few shot, and CoT variants.\n",
    "\n",
    "prompt_templates = [\n",
    "    {\n",
    "        \"template_id\": 1,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"en\",\n",
    "        \"prompt_type\": \"zero_shot\",\n",
    "        \"template_text\": (\n",
    "            \"You are a helpful assistant.\\n\"\n",
    "            \"Classify the sentiment of the following review as 'positive' or 'negative'.\\n\\n\"\n",
    "            \"Review. {input_text}\\n\"\n",
    "            \"Sentiment.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 2,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"en\",\n",
    "        \"prompt_type\": \"cot\",\n",
    "        \"template_text\": (\n",
    "            \"You are a helpful assistant.\\n\"\n",
    "            \"First, briefly explain why the review is positive or negative.\\n\"\n",
    "            \"Then, on a new line, answer with 'positive' or 'negative'.\\n\\n\"\n",
    "            \"Review. {input_text}\\n\"\n",
    "            \"Explanation and answer.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 3,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"prompt_type\": \"zero_shot\",\n",
    "        \"template_text\": (\n",
    "            \"Du bass en hëllefsbereeten Assistent.\\n\"\n",
    "            \"Klassifizéier d'Stëmmung vun dëser Kritik als 'positiv' oder 'negativ'.\\n\\n\"\n",
    "            \"Kritik. {input_text}\\n\"\n",
    "            \"Stëmmung.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 4,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"prompt_type\": \"cot\",\n",
    "        \"template_text\": (\n",
    "            \"Du bass en hëllefsbereeten Assistent.\\n\"\n",
    "            \"Erklär kuerz firwat d'Kritik positiv oder negativ ass.\\n\"\n",
    "            \"Duerno, op enger neier Zeil, äntwere just mat 'positiv' oder 'negativ'.\\n\\n\"\n",
    "            \"Kritik. {input_text}\\n\"\n",
    "            \"Erklärung an Äntwert.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 5,\n",
    "        \"task_id\": 2,\n",
    "        \"language_code\": \"en\",\n",
    "        \"prompt_type\": \"zero_shot\",\n",
    "        \"template_text\": (\n",
    "            \"You are a helpful assistant.\\n\"\n",
    "            \"Answer the question briefly and accurately.\\n\\n\"\n",
    "            \"Question. {input_text}\\n\"\n",
    "            \"Answer.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 6,\n",
    "        \"task_id\": 2,\n",
    "        \"language_code\": \"en\",\n",
    "        \"prompt_type\": \"cot\",\n",
    "        \"template_text\": (\n",
    "            \"You are a helpful assistant.\\n\"\n",
    "            \"Think step by step to answer the question, then give a short final answer on a new line.\\n\\n\"\n",
    "            \"Question. {input_text}\\n\"\n",
    "            \"Reasoning and final answer.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 7,\n",
    "        \"task_id\": 2,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"prompt_type\": \"zero_shot\",\n",
    "        \"template_text\": (\n",
    "            \"Du bass en hëllefsbereeten Assistent.\\n\"\n",
    "            \"Beäntwer d'Fro kuerz a präzis.\\n\\n\"\n",
    "            \"Fro. {input_text}\\n\"\n",
    "            \"Äntwert.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"template_id\": 8,\n",
    "        \"task_id\": 2,\n",
    "        \"language_code\": \"lb\",\n",
    "        \"prompt_type\": \"cot\",\n",
    "        \"template_text\": (\n",
    "            \"Du bass en hëllefsbereeten Assistent.\\n\"\n",
    "            \"Denke Schrëtt fir Schrëtt fir d'Fro ze beäntweren, an duerno ginn eng kuerz Äntwert op enger neier Zeil.\\n\\n\"\n",
    "            \"Fro. {input_text}\\n\"\n",
    "            \"Iwwerleeung an Äntwert.\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "templates_df = pd.DataFrame(prompt_templates)\n",
    "templates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f69c0",
   "metadata": {},
   "source": [
    "Few shot prompts are typically created by adding one or more example pairs above the current input.\n",
    "\n",
    "You can construct few shot prompts manually in the evaluation table later, or you can add explicit few shot templates with placeholders for examples.\n",
    "\n",
    "For this tutorial, we keep few shot prompts flexible and let you write them directly in the experiments table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8d7ad",
   "metadata": {},
   "source": [
    "## 4. Running experiments and recording outputs\n",
    "\n",
    "You will now define an **experiments table** where each row represents one model run.\n",
    "\n",
    "Each row stores.\n",
    "\n",
    "- Task and language.  \n",
    "- Model name.  \n",
    "- Prompting strategy (zero shot, few shot, CoT).  \n",
    "- The exact prompt you used.  \n",
    "- The model output.  \n",
    "- Your ratings for correctness, fluency, and cultural appropriateness.\n",
    "\n",
    "Start with a few example rows, then duplicate and edit them as you run more experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e81a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for experiments with a small example.\n",
    "# Replace the placeholder values as you run real experiments.\n",
    "\n",
    "experiments = [\n",
    "    {\n",
    "        \"experiment_id\": 1,\n",
    "        \"task_id\": 1,\n",
    "        \"language_code\": \"en\",\n",
    "        \"item_id\": 1,\n",
    "        \"model_name\": \"ModelA\",          # e.g. \"gpt4o\", \"claude\", \"llama3\", etc.\n",
    "        \"prompt_type\": \"zero_shot\",      # \"zero_shot\", \"few_shot\", or \"cot\"\n",
    "        \"shots_used\": 0,                 # number of in context examples (0 for zero shot)\n",
    "        \"prompt_text\": (\n",
    "            \"You are a helpful assistant.\\n\"\n",
    "            \"Classify the sentiment of the following review as 'positive' or 'negative'.\\n\\n\"\n",
    "            \"Review. The restaurant was amazing, the food was fresh and the staff were friendly.\\n\"\n",
    "            \"Sentiment.\"\n",
    "        ),\n",
    "        \"model_output\": \"[paste output here]\",\n",
    "        # Ratings (see section 5 for guidance).\n",
    "        \"correctness\": None,             # 0. incorrect, 1. partially correct, 2. fully correct\n",
    "        \"fluency\": None,                 # 0. poor, 1. acceptable, 2. very fluent\n",
    "        \"cultural_appropriateness\": None,# 0. problematic, 1. acceptable, 2. very appropriate\n",
    "        \"notes\": \"\",\n",
    "    },\n",
    "    # Add more rows here for other prompt types, models, languages, and items.\n",
    "]\n",
    "\n",
    "experiments_df = pd.DataFrame(experiments)\n",
    "experiments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acffcb02",
   "metadata": {},
   "source": [
    "For each new experiment you run.\n",
    "\n",
    "1. Duplicate an existing row in `experiments`.  \n",
    "2. Update `experiment_id` to a new unique number.  \n",
    "3. Set `task_id`, `language_code`, `item_id`, `model_name`, and `prompt_type`.  \n",
    "4. Copy the exact `prompt_text` you used.  \n",
    "5. Paste the `model_output`.  \n",
    "6. Fill in the rating fields after reading the output carefully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935daa30",
   "metadata": {},
   "source": [
    "## 5. Rating guidelines\n",
    "\n",
    "Use the following scales when annotating each experiment.\n",
    "\n",
    "### 5.1 Correctness (`correctness`)\n",
    "\n",
    "- **2. fully correct**. The answer matches the expected label or answer, with no major errors.  \n",
    "- **1. partially correct**. The answer is close but not perfect (for example correct label but with an incorrect explanation, or correct core fact with minor mistakes).  \n",
    "- **0. incorrect**. The answer does not match the expected output or is clearly wrong.\n",
    "\n",
    "### 5.2 Fluency (`fluency`)\n",
    "\n",
    "- **2. very fluent**. The output is natural and grammatical in the target language.  \n",
    "- **1. acceptable**. Some minor issues, but overall understandable.  \n",
    "- **0. poor**. The output has many errors or is difficult to understand.\n",
    "\n",
    "### 5.3 Cultural appropriateness (`cultural_appropriateness`)\n",
    "\n",
    "- **2. very appropriate**. The output is respectful and fits the cultural context well.  \n",
    "- **1. acceptable**. No serious issues, but maybe slightly awkward.  \n",
    "- **0. problematic**. The output feels insensitive, inappropriate, or culturally misleading.\n",
    "\n",
    "Use the `notes` field to document interesting cases (for example when CoT reasoning is correct in English but fails in your low resource language).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4777d6e",
   "metadata": {},
   "source": [
    "## 6. Summarizing results\n",
    "\n",
    "Once you have filled `experiments_df` with several rows, you can compute simple summaries by language, model, and prompt type.\n",
    "\n",
    "Run the cell below for an overview.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(experiments_df) == 0:\n",
    "    print(\"experiments_df is empty. Please add some experiments first.\")\n",
    "else:\n",
    "    summary = experiments_df.groupby([\"model_name\", \"language_code\", \"prompt_type\"])[\n",
    "        [\"correctness\", \"fluency\", \"cultural_appropriateness\"]\n",
    "    ].mean()\n",
    "\n",
    "    count = experiments_df.groupby([\"model_name\", \"language_code\", \"prompt_type\"])[\n",
    "        \"experiment_id\"\n",
    "    ].count().rename(\"num_examples\")\n",
    "\n",
    "    summary = summary.join(count)\n",
    "    print(\"Average scores by model, language, and prompt type (scale 0-2):\")\n",
    "    display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d48e11",
   "metadata": {},
   "source": [
    "### 6.1 Visualizing the effect of prompt type\n",
    "\n",
    "If you have enough experiments, you can plot average correctness by prompt type for a given model and language.\n",
    "\n",
    "Edit the variables in the cell below to match your setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure which model and language you want to visualize.\n",
    "target_model = \"ModelA\"   # change this to a real model name used in your experiments\n",
    "target_language = \"en\"    # e.g. \"lb\" for your low resource language\n",
    "\n",
    "subset = experiments_df[\n",
    "    (experiments_df[\"model_name\"] == target_model)\n",
    "    & (experiments_df[\"language_code\"] == target_language)\n",
    "]\n",
    "\n",
    "if len(subset) == 0:\n",
    "    print(\"No experiments found for the chosen model and language.\")\n",
    "else:\n",
    "    avg_by_prompt = subset.groupby(\"prompt_type\")[\n",
    "        [\"correctness\", \"fluency\", \"cultural_appropriateness\"]\n",
    "    ].mean()\n",
    "\n",
    "    avg_by_prompt = avg_by_prompt.reindex([\"zero_shot\", \"few_shot\", \"cot\"]).dropna(how=\"all\")\n",
    "\n",
    "    if avg_by_prompt.empty:\n",
    "        print(\"No data to plot. Check that you used 'zero_shot', 'few_shot', and 'cot' labels.\")\n",
    "    else:\n",
    "        avg_by_prompt[\"correctness\"].plot(kind=\"bar\")\n",
    "        plt.ylim(0, 2)\n",
    "        plt.ylabel(\"Average correctness (0-2)\")\n",
    "        plt.title(f\"Effect of prompt type on correctness ({target_model}, {target_language})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1d135",
   "metadata": {},
   "source": [
    "You can duplicate and adapt this cell to compare.\n",
    "\n",
    "- English vs your low resource language for the same model.  \n",
    "- Different models for the same language and prompt type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2975c61",
   "metadata": {},
   "source": [
    "## 7. Exporting your annotations\n",
    "\n",
    "If you want to share your results or combine them with other groups, you can export the experiments table to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"cross_lingual_prompting_experiments.csv\"\n",
    "experiments_df.to_csv(output_path, index=False)\n",
    "print(f\"Experiments exported to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d93f30",
   "metadata": {},
   "source": [
    "## 8. Reflection and small group discussion\n",
    "\n",
    "Use the questions below for your final discussion.\n",
    "\n",
    "1. **Prompt type effects.**  \n",
    "   - In your experiments, did few shot prompting improve correctness compared to zero shot for your low resource language.  \n",
    "   - Did Chain of Thought instructions help or confuse the model output in your language.\n",
    "\n",
    "2. **Cross lingual differences.**  \n",
    "   - Did the same prompt design work equally well in English and in your low resource language.  \n",
    "   - Were there cases where reasoning was correct in English but failed when you translated the prompt.\n",
    "\n",
    "3. **Fluency and cultural appropriateness.**  \n",
    "   - Did CoT prompts degrade fluency or make outputs more verbose than needed.  \n",
    "   - Were there outputs that felt culturally inappropriate or out of place in your language.\n",
    "\n",
    "4. **Practical recommendations.**  \n",
    "   - Based on your observations, what simple rules of thumb would you give to someone designing prompts for your language.  \n",
    "   - Which combinations of prompt type, language, and model would you recommend for real applications.\n",
    "\n",
    "You can use the markdown cell below to write down key points from your discussion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa65e57d",
   "metadata": {},
   "source": [
    "### 8.1 Notes from your group\n",
    "\n",
    "Use this space to summarize your main takeaways.\n",
    "\n",
    "- What worked well (prompt types, models, languages).  \n",
    "- What did not work well.  \n",
    "- Surprising observations.  \n",
    "- Open questions you would like to explore in more systematic studies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
