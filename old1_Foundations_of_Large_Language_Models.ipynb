{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ§  Session 1: Foundations of Large Language Models\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "**ğŸ“š Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/1_Foundations_of_Large_Language_Models.ipynb)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ What You'll Learn\n",
        "\n",
        "This notebook introduces the **fundamental concepts** behind Large Language Models through hands-on exploration. You'll understand how LLMs process text and work with multilingual models to see how well your language is represented.\n",
        "\n",
        "**ğŸ”¬ Core Focus:** Understanding tokenization, embeddings, and transformers  \n",
        "**ğŸ’» Requirements:** CPU is sufficient - no GPU needed!  \n",
        "**ğŸŒ Language Support:** Multilingual examples with customizable content\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“‹ Table of Contents\n",
        "\n",
        "1. [ğŸ› ï¸ Setup & Installation](#setup)\n",
        "2. [ğŸ¤” Why LLMs Matter](#why-llms)\n",
        "3. [ğŸ”¤ Tokenization Deep Dive](#tokenization)\n",
        "4. [ğŸ¯ Sentence Embeddings](#embeddings)\n",
        "5. [âš™ï¸ Inside Transformers](#transformers)\n",
        "6. [ğŸŒ Multilingual Exploration](#multilingual)\n",
        "7. [ğŸ“ Wrap-up & Next Steps](#wrap-up)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ Learning Objectives\n",
        "\n",
        "By the end of this session, you will:\n",
        "\n",
        "âœ… **Understand** why LLMs revolutionized NLP  \n",
        "âœ… **Explore** how tokenization affects different languages  \n",
        "âœ… **Visualize** sentence embeddings in 2D space  \n",
        "âœ… **Examine** transformer architecture basics  \n",
        "âœ… **Compare** multilingual model performance  \n",
        "âœ… **Reflect** on low-resource language representation\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“– Prerequisites\n",
        "\n",
        "**Recommended learning path:**\n",
        "1. **Session 0:** Basic Python and NLP concepts *(optional)*\n",
        "2. **This session:** LLM foundations *(you are here)*\n",
        "3. **Session 2:** Advanced applications and fine-tuning\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸš€ How to Use This Notebook\n",
        "\n",
        "- ğŸ” **Checkpoint cells** mark good stopping points for discussion\n",
        "- ğŸ¯ **Challenge cells** contain optional exercises\n",
        "- ğŸ“ **Reflection cells** encourage you to think about the concepts\n",
        "- âš¡ **Run cells in order** - dependencies build on each other\n",
        "- ğŸ”„ **If stuck:** Restart runtime and re-run from the top\n"
      ],
      "id": "b48c569d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ› ï¸ Setup & Installation {#setup}\n",
        "\n",
        "This section will install all the required packages for our LLM exploration. We'll use minimal dependencies to keep things lightweight and fast.\n",
        "\n",
        "### ğŸ“¦ What We'll Install\n",
        "\n",
        "- **transformers** - For working with pre-trained models\n",
        "- **sentence-transformers** - For sentence embeddings\n",
        "- **scikit-learn** - For PCA and basic ML utilities\n",
        "- **matplotlib** - For visualizations\n",
        "- **numpy & pandas** - For data manipulation\n",
        "\n",
        "### ğŸš€ Quick Installation\n",
        "\n",
        "Choose your preferred installation method:\n"
      ],
      "id": "506e85f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ Quick Installation (Option A - Recommended)\n",
        "# This installs only what we need for this notebook\n",
        "\n",
        "!pip install -q transformers sentence-transformers scikit-learn matplotlib numpy pandas torch"
      ],
      "id": "db52aba1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ”§ Alternative Installation (Option B - More Control)\n",
        "# Use this if you want to see exactly what's being installed\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Install required packages with error handling\"\"\"\n",
        "    packages = [\n",
        "        \"transformers==4.49.0\",\n",
        "        \"sentence-transformers\", \n",
        "        \"scikit-learn\",\n",
        "        \"matplotlib\",\n",
        "        \"numpy\",\n",
        "        \"pandas\",\n",
        "        \"torch\"\n",
        "    ]\n",
        "    \n",
        "    print(\"ğŸ”„ Installing packages...\")\n",
        "    for pkg in packages:\n",
        "        try:\n",
        "            print(f\"  ğŸ“¦ Installing {pkg}\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ Failed to install {pkg}: {e}\")\n",
        "            \n",
        "    print(\"âœ… Installation complete!\")\n",
        "\n",
        "# Uncomment the line below to run the installation\n",
        "# install_packages()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f24fd6c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š Import Libraries & Setup\n",
        "\n",
        "Let's import all the libraries we'll need and set up our environment for reproducible results."
      ],
      "id": "6ac2b033"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ“š Core imports\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "from typing import List, Dict\n",
        "\n",
        "# ğŸ”¢ Data & ML libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# ğŸ¤— Hugging Face libraries\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# ğŸ¨ Configure plotting\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# ğŸ¯ Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ğŸ–¥ï¸ Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ğŸ–¥ï¸  Using device: {device}\")\n",
        "print(f\"ğŸ² Random seed: {SEED}\")\n",
        "\n",
        "# ğŸ”‡ Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b9611d62"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤” Why Do LLMs Matter? {#why-llms}\n",
        "\n",
        "Large Language Models have revolutionized how we approach natural language processing. Let's understand why they're so powerful.\n",
        "\n",
        "### ğŸ”„ The Old Way vs. The New Way\n",
        "\n",
        "**Traditional NLP (Pre-2017):**\n",
        "- ğŸ”§ **Task-specific models** - Different model for each task\n",
        "- âœ‹ **Hand-crafted features** - Manual feature engineering\n",
        "- ğŸ“Š **Limited data** - Smaller, task-specific datasets\n",
        "- ğŸ¯ **Single purpose** - One model = one task\n",
        "\n",
        "**LLMs (2017+):**\n",
        "- ğŸ§  **One large model** - Single model for multiple tasks\n",
        "- ğŸ¤– **Learned representations** - Features learned automatically\n",
        "- ğŸ“š **Massive data** - Trained on internet-scale text\n",
        "- ğŸ­ **Multi-purpose** - Same model for many tasks\n",
        "\n",
        "### ğŸ¯ The Magic of Prompting\n",
        "\n",
        "With **prompting**, the same LLM can:\n",
        "\n",
        "- ğŸŒ **Translate** languages\n",
        "- ğŸ“ **Summarize** documents  \n",
        "- â“ **Answer** questions\n",
        "- ğŸ’» **Generate** code\n",
        "- ğŸ¨ **Write** creatively\n",
        "\n",
        "### ğŸ”¬ How LLMs Work (3 Key Steps)\n",
        "\n",
        "1. **ğŸ”¤ Tokenization** - Break text into pieces (tokens)\n",
        "2. **ğŸ“Š Embeddings** - Convert tokens to numerical vectors\n",
        "3. **âš™ï¸ Transformers** - Process sequences with attention\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ¯ What We'll Explore\n",
        "\n",
        "In this notebook, we won't train a model from scratch. Instead, we'll **look inside** existing multilingual models to understand:\n",
        "\n",
        "- How they process your language\n",
        "- What they \"see\" when they read text\n",
        "- How well they represent low-resource languages\n"
      ],
      "id": "581a653f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸŒ Multilingual Text Examples {#multilingual}\n",
        "\n",
        "Let's start our exploration with sample sentences in two languages. This will help us understand how LLMs handle different languages.\n",
        "\n",
        "### ğŸ“ Sample Languages\n",
        "\n",
        "- **ğŸ‡¬ğŸ‡§ English** (`en`) - High-resource language\n",
        "- **ğŸ‡±ğŸ‡º Luxembourgish** (`lb`) - Low-resource language *(you can change this!)*\n",
        "\n",
        "### âœï¸ Customize Your Language\n",
        "\n",
        "Feel free to replace the Luxembourgish examples with sentences in your own language of interest. Just update the `LOW_RESOURCE_LANG` variable and the example sentences below.\n"
      ],
      "id": "e70c5822"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸŒ Define your languages and example sentences\n",
        "# ğŸ’¡ Feel free to change LOW_RESOURCE_LANG and the example sentences!\n",
        "\n",
        "LOW_RESOURCE_LANG = \"lb\"  # Options: \"lb\", \"hy\", \"fa\", \"sw\", etc.\n",
        "\n",
        "examples = [\n",
        "    # ğŸ‡¬ğŸ‡§ English examples\n",
        "    {\"id\": \"en_1\", \"lang\": \"en\", \"text\": \"The doctor explains the diagnosis carefully.\"},\n",
        "    {\"id\": \"en_2\", \"lang\": \"en\", \"text\": \"Students are learning about large language models.\"},\n",
        "    {\"id\": \"en_3\", \"lang\": \"en\", \"text\": \"The weather is cloudy today but we still go for a walk.\"},\n",
        "\n",
        "    # ğŸ‡±ğŸ‡º Luxembourgish examples (replace with your language!)\n",
        "    {\"id\": f\"{LOW_RESOURCE_LANG}_1\", \"lang\": LOW_RESOURCE_LANG, \"text\": \"Den Dokter erklÃ¤ert d'Diagnos ganz roueg.\"},\n",
        "    {\"id\": f\"{LOW_RESOURCE_LANG}_2\", \"lang\": LOW_RESOURCE_LANG, \"text\": \"D'Studenten lÃ©ieren iwwer grouss Sproochmodeller.\"},\n",
        "    {\"id\": f\"{LOW_RESOURCE_LANG}_3\", \"lang\": LOW_RESOURCE_LANG, \"text\": \"D'Wieder ass haut wollekeg, mee mir ginn trotzdeem spadsÃ©ieren.\"},\n",
        "]\n",
        "\n",
        "print(\"ğŸ“ Our example sentences:\")\n",
        "print(\"=\" * 50)\n",
        "for ex in examples:\n",
        "    flag = \"ğŸ‡¬ğŸ‡§\" if ex['lang'] == 'en' else \"ğŸ‡±ğŸ‡º\"\n",
        "    print(f\"{flag} [{ex['lang']}] {ex['id']}: {ex['text']}\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5a21f94c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”¤ Tokenization Deep Dive {#tokenization}\n",
        "\n",
        "**Tokenization** is the first step in how LLMs process text. Let's explore how different models break down our sentences into tokens."
      ],
      "id": "9a695143"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ” What is Tokenization?\n",
        "\n",
        "Models don't see raw text - they see **tokens**! Tokens are usually subword units that help models handle:\n",
        "- **Unknown words** by breaking them into familiar pieces\n",
        "- **Different languages** with shared subword patterns\n",
        "- **Morphologically rich languages** by capturing stems and affixes\n",
        "\n",
        "### ğŸ¯ Our Exploration Plan\n",
        "\n",
        "1. **Load** a multilingual tokenizer\n",
        "2. **Analyze** how it splits our sentences  \n",
        "3. **Compare** token counts across languages\n",
        "4. **Reflect** on implications for low-resource languages\n",
        "\n",
        "### ğŸ¤– Models to Try\n",
        "\n",
        "Feel free to experiment with different tokenizers:\n",
        "- `bert-base-multilingual-cased` - BERT's multilingual tokenizer\n",
        "- `xlm-roberta-base` - XLM-RoBERTa (often better for low-resource)\n",
        "- `google/mt5-small` - Multilingual T5\n"
      ],
      "id": "0c43e386"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ”¬ Tokenization Analysis"
      ],
      "id": "bf63ac15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ¤– Load a multilingual tokenizer\n",
        "multilingual_model_name = \"xlm-roberta-base\"  # Feel free to change this!\n",
        "\n",
        "print(f\"ğŸ”„ Loading tokenizer: {multilingual_model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(multilingual_model_name)\n",
        "print(f\"âœ… Tokenizer loaded successfully!\")\n",
        "\n",
        "def inspect_tokenization(examples, tokenizer, max_tokens_to_show=15):\n",
        "    \"\"\"Analyze how the tokenizer processes our example sentences\"\"\"\n",
        "    results = []\n",
        "    for ex in examples:\n",
        "        tokens = tokenizer.tokenize(ex[\"text\"])\n",
        "        results.append({\n",
        "            \"id\": ex[\"id\"],\n",
        "            \"lang\": ex[\"lang\"], \n",
        "            \"text\": ex[\"text\"],\n",
        "            \"n_tokens\": len(tokens),\n",
        "            \"tokens_preview\": tokens[:max_tokens_to_show],\n",
        "            \"tokens_full\": tokens\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# ğŸ” Analyze tokenization\n",
        "print(f\"\\nğŸ”¤ Tokenization Analysis with {multilingual_model_name}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "tokenization_results = inspect_tokenization(examples, tokenizer)\n",
        "\n",
        "for result in tokenization_results:\n",
        "    flag = \"ğŸ‡¬ğŸ‡§\" if result['lang'] == 'en' else \"ğŸ‡±ğŸ‡º\"\n",
        "    print(f\"\\n{flag} {result['id']} ({result['lang']})\")\n",
        "    print(f\"ğŸ“ Text: {result['text']}\")\n",
        "    print(f\"ğŸ”¢ Tokens: {result['n_tokens']}\")\n",
        "    print(f\"ğŸ”¤ Preview: {result['tokens_preview']}\")\n",
        "    if len(result['tokens_full']) > 15:\n",
        "        print(f\"   ... and {len(result['tokens_full']) - 15} more tokens\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e9d74556"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“Š Token Count Summary"
      ],
      "id": "b9f3ac41"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ“Š Create summary statistics by language\n",
        "df_tokens = pd.DataFrame(tokenization_results)\n",
        "summary = df_tokens.groupby(\"lang\")[\"n_tokens\"].agg([\"mean\", \"min\", \"max\", \"count\"]).round(2)\n",
        "\n",
        "print(\"ğŸ“Š Token Count Statistics by Language:\")\n",
        "print(\"=\" * 40)\n",
        "print(summary)\n",
        "\n",
        "# ğŸ“ˆ Quick comparison\n",
        "en_avg = summary.loc['en', 'mean']\n",
        "lr_avg = summary.loc[LOW_RESOURCE_LANG, 'mean']\n",
        "ratio = lr_avg / en_avg\n",
        "\n",
        "print(f\"\\nğŸ” Key Insights:\")\n",
        "print(f\"   ğŸ‡¬ğŸ‡§ English average: {en_avg:.1f} tokens\")\n",
        "print(f\"   ğŸ‡±ğŸ‡º {LOW_RESOURCE_LANG.upper()} average: {lr_avg:.1f} tokens\")\n",
        "print(f\"   ğŸ“ˆ Ratio: {ratio:.2f}x {'more' if ratio > 1 else 'fewer'} tokens for {LOW_RESOURCE_LANG.upper()}\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b522cdd0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ¤” Reflection: What Do These Results Mean?"
      ],
      "id": "4f5ff389"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take a moment to analyze the tokenization results above. Consider these important questions:\n",
        "\n",
        "#### ğŸ” Analysis Questions\n",
        "\n",
        "1. **Token Efficiency**: Do English sentences use more or fewer tokens than your low-resource language?\n",
        "\n",
        "2. **Quality Issues**: Do you notice any problematic splits?\n",
        "   - Broken accents or diacritics?\n",
        "   - Words split into meaningless pieces?\n",
        "   - Proper nouns fragmented?\n",
        "\n",
        "3. **Practical Implications**: How might different token counts affect:\n",
        "   - âš¡ **Speed** - More tokens = slower processing\n",
        "   - ğŸ“ **Context length** - Fewer words fit in model's context window  \n",
        "   - ğŸ’° **Cost** - Many APIs charge per token\n",
        "\n",
        "#### ğŸ’­ Discussion Points\n",
        "\n",
        "- What does this tell us about model bias toward certain languages?\n",
        "- How might this affect performance on low-resource languages?\n",
        "- What strategies could help mitigate these issues?\n"
      ],
      "id": "5dec25d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“ Your Observations (Interactive)"
      ],
      "id": "0fb0095e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ“ Write your observations here!\n",
        "# Edit the text below to record your insights\n",
        "\n",
        "my_observations = f\"\"\"\n",
        "ğŸ” TOKENIZATION OBSERVATIONS FOR {LOW_RESOURCE_LANG.upper()}:\n",
        "\n",
        "Token Efficiency:\n",
        "- English vs {LOW_RESOURCE_LANG}: [Your comparison here]\n",
        "\n",
        "Quality Issues I Notice:\n",
        "- [e.g., \"Words split into too many pieces\"]\n",
        "- [e.g., \"Accents/diacritics handled poorly\"] \n",
        "- [e.g., \"Named entities fragmented\"]\n",
        "\n",
        "Implications:\n",
        "- Processing speed: [Your thoughts]\n",
        "- Context limitations: [Your thoughts]\n",
        "- Cost considerations: [Your thoughts]\n",
        "\n",
        "Overall Assessment:\n",
        "- [How well does this tokenizer handle your language?]\n",
        "\"\"\"\n",
        "\n",
        "print(my_observations)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8592d529"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ¯ Sentence Embeddings {#embeddings}\n",
        "\n",
        "Now let's explore how LLMs convert text into numerical representations that capture meaning."
      ],
      "id": "bf3d32b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ§  What are Sentence Embeddings?\n",
        "\n",
        "A **sentence embedding** is a dense vector (typically 384-768 dimensions) that captures the semantic meaning of a sentence. Think of it as a \"fingerprint\" that represents what the sentence means.\n",
        "\n",
        "### ğŸ¯ Our Exploration Plan\n",
        "\n",
        "1. **ğŸ¤– Load** a multilingual sentence embedding model\n",
        "2. **ğŸ”¢ Convert** each sentence to a numerical vector  \n",
        "3. **ğŸ“‰ Reduce** high-dimensional vectors to 2D using PCA\n",
        "4. **ğŸ“Š Visualize** sentences in 2D space, colored by language\n",
        "\n",
        "### ğŸŒ Model We'll Use\n",
        "\n",
        "We'll use `paraphrase-multilingual-MiniLM-L12-v2` - a model trained to create similar embeddings for sentences with similar meanings, regardless of language!\n"
      ],
      "id": "178f9f7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ”¢ Computing Sentence Embeddings"
      ],
      "id": "5c5d1750"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ¤– Load multilingual sentence embedding model\n",
        "embedder_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "print(f\"ğŸ”„ Loading sentence embedder: {embedder_name}\")\n",
        "embedder = SentenceTransformer(embedder_name, device=device)\n",
        "print(f\"âœ… Model loaded successfully!\")\n",
        "\n",
        "# ğŸ“ Extract text data for embedding\n",
        "sent_texts = [ex[\"text\"] for ex in examples]\n",
        "sent_langs = [ex[\"lang\"] for ex in examples]\n",
        "sent_ids = [ex[\"id\"] for ex in examples]\n",
        "\n",
        "print(f\"\\nğŸ”¢ Computing embeddings for {len(sent_texts)} sentences...\")\n",
        "\n",
        "# ğŸ§  Generate embeddings\n",
        "embeddings = embedder.encode(sent_texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "\n",
        "print(f\"âœ… Embeddings computed!\")\n",
        "print(f\"ğŸ“Š Shape: {embeddings.shape} (sentences Ã— dimensions)\")\n",
        "print(f\"ğŸ¯ Each sentence is now represented as a {embeddings.shape[1]}-dimensional vector\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "91a7ccb2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“‰ Reducing to 2D for Visualization"
      ],
      "id": "d358f2dd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ“‰ Use PCA to reduce from high dimensions to 2D\n",
        "print(\"ğŸ”„ Reducing embeddings to 2D using PCA...\")\n",
        "pca = PCA(n_components=2, random_state=SEED)\n",
        "coords_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# ğŸ“Š Create DataFrame for easier plotting and analysis\n",
        "df_plot = pd.DataFrame({\n",
        "    \"id\": sent_ids,\n",
        "    \"lang\": sent_langs, \n",
        "    \"text\": sent_texts,\n",
        "    \"x\": coords_2d[:, 0],\n",
        "    \"y\": coords_2d[:, 1],\n",
        "})\n",
        "\n",
        "# ğŸ“ˆ Show how much variance is captured by our 2D projection\n",
        "variance_explained = pca.explained_variance_ratio_\n",
        "total_variance = sum(variance_explained)\n",
        "\n",
        "print(f\"âœ… 2D projection complete!\")\n",
        "print(f\"ğŸ“Š Variance explained: {variance_explained[0]:.1%} (PC1) + {variance_explained[1]:.1%} (PC2) = {total_variance:.1%} total\")\n",
        "print(f\"ğŸ’¡ This means our 2D plot captures {total_variance:.1%} of the original information\")\n",
        "\n",
        "print(f\"\\nğŸ“‹ Data for plotting:\")\n",
        "df_plot\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "947b0073"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“Š Visualizing Sentence Embeddings"
      ],
      "id": "e5ae29e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ“Š Create a beautiful 2D visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# ğŸ¨ Define colors and markers for each language\n",
        "colors = {'en': '#2E86AB', LOW_RESOURCE_LANG: '#A23B72'}  # Blue for English, Purple for other\n",
        "markers = {'en': 'o', LOW_RESOURCE_LANG: 's'}  # Circle for English, Square for other\n",
        "sizes = {'en': 100, LOW_RESOURCE_LANG: 120}  # Slightly larger for low-resource language\n",
        "\n",
        "# ğŸ“ Plot points for each language\n",
        "for lang in df_plot[\"lang\"].unique():\n",
        "    subset = df_plot[df_plot[\"lang\"] == lang]\n",
        "    flag = \"ğŸ‡¬ğŸ‡§\" if lang == 'en' else \"ğŸ‡±ğŸ‡º\"\n",
        "    label = f\"{flag} {lang.upper()}\"\n",
        "    \n",
        "    plt.scatter(subset[\"x\"], subset[\"y\"], \n",
        "               c=colors[lang], marker=markers[lang], s=sizes[lang],\n",
        "               label=label, alpha=0.8, edgecolors='white', linewidth=2)\n",
        "\n",
        "# ğŸ·ï¸ Add labels for each point\n",
        "for _, row in df_plot.iterrows():\n",
        "    plt.annotate(row[\"id\"], \n",
        "                (row[\"x\"], row[\"y\"]),\n",
        "                xytext=(8, 8), textcoords='offset points',\n",
        "                fontsize=10, fontweight='bold',\n",
        "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
        "\n",
        "# ğŸ¨ Styling\n",
        "plt.title(\"ğŸ§  Sentence Embeddings in 2D Space\", fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel(f\"ğŸ“Š First Principal Component ({variance_explained[0]:.1%} variance)\", fontsize=12)\n",
        "plt.ylabel(f\"ğŸ“Š Second Principal Component ({variance_explained[1]:.1%} variance)\", fontsize=12)\n",
        "plt.legend(fontsize=12, loc='best')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# ğŸ” Add some context\n",
        "plt.figtext(0.02, 0.02, \n",
        "           \"ğŸ’¡ Points closer together = more similar meanings\\n\" +\n",
        "           \"ğŸŒ Good multilingual models cluster by meaning, not language\", \n",
        "           fontsize=10, style='italic')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ğŸ“ Calculate distances between similar sentences\n",
        "print(\"\\nğŸ“ Distance Analysis:\")\n",
        "print(\"=\" * 40)\n",
        "for i in range(0, len(examples), 2):  # Compare pairs\n",
        "    if i+1 < len(examples):\n",
        "        p1, p2 = df_plot.iloc[i], df_plot.iloc[i+1] \n",
        "        distance = np.sqrt((p1['x'] - p2['x'])**2 + (p1['y'] - p2['y'])**2)\n",
        "        print(f\"Distance between {p1['id']} and {p2['id']}: {distance:.3f}\")\n",
        "        print(f\"  ğŸ‡¬ğŸ‡§ {p1['text'][:50]}...\")\n",
        "        print(f\"  ğŸ‡±ğŸ‡º {p2['text'][:50]}...\")\n",
        "        print()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "47f61096"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "reflection on embeddings"
      ],
      "id": "2b8ee2ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 . Reflection\n",
        "\n",
        "Look at the plot and discuss.\n",
        "\n",
        "- Do sentences cluster more by **language** or by **meaning**.\n",
        "- Are English and low resource sentences with similar meaning close in the plot.\n",
        "- Does any sentence look like an outlier.\n",
        "\n",
        "Again, use the next cell as a scratch pad.\n"
      ],
      "id": "d253f1b3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "free text reflection for embeddings"
      ],
      "id": "fd4bee48"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reflection_embeddings = \"\"\"\n",
        "Notes about the 2D embedding plot.\n",
        "\n",
        "- Example: The two sentences about doctors are close, even across languages.\n",
        "- Example: One sentence in my language is far away, maybe the model does not know this vocabulary well.\n",
        "\"\"\"\n",
        "\n",
        "print(reflection_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "38a23fb0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 . Inside a transformer (very briefly)\n",
        "\n",
        "We do not derive the full transformer math here. \n",
        "Instead we run a **forward pass** and inspect tensor shapes.\n",
        "\n",
        "We will.\n",
        "\n",
        "1. Use a small multilingual transformer (encoder only).\n",
        "2. Tokenize one sentence.\n",
        "3. See shapes of.\n",
        "   - `input_ids`.\n",
        "   - `attention_mask`.\n",
        "   - `last_hidden_state`.\n"
      ],
      "id": "8aad9286"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ”¬ Transformer Forward Pass Exploration"
      ],
      "id": "13bbafed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ¤– Load a small multilingual transformer model\n",
        "small_model_name = \"distilbert-base-multilingual-cased\"\n",
        "\n",
        "print(f\"ğŸ”„ Loading transformer model: {small_model_name}\")\n",
        "tok_small = AutoTokenizer.from_pretrained(small_model_name)\n",
        "model_small = AutoModel.from_pretrained(small_model_name).to(device)\n",
        "model_small.eval()  # Set to evaluation mode\n",
        "print(\"âœ… Model loaded successfully!\")\n",
        "\n",
        "# ğŸ“ Use one of our example sentences\n",
        "test_sentence = examples[1][\"text\"]  # \"Students are learning about large language models.\"\n",
        "print(f\"\\nğŸ”¤ Test sentence: '{test_sentence}'\")\n",
        "\n",
        "# ğŸ”¢ Tokenize the input\n",
        "inputs = tok_small(test_sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# ğŸ§  Run forward pass through the transformer\n",
        "print(\"\\nğŸ”„ Running forward pass...\")\n",
        "with torch.no_grad():\n",
        "    outputs = model_small(**inputs)\n",
        "\n",
        "# ğŸ“Š Examine tensor shapes\n",
        "print(\"\\nğŸ“Š Tensor Shape Analysis:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"ğŸ“ Original sentence: '{test_sentence}'\")\n",
        "print(f\"ğŸ”¢ input_ids shape: {inputs['input_ids'].shape}\")\n",
        "print(f\"ğŸ‘ï¸  attention_mask shape: {inputs['attention_mask'].shape}\")  \n",
        "print(f\"ğŸ§  last_hidden_state shape: {outputs.last_hidden_state.shape}\")\n",
        "\n",
        "# ğŸ” Explain what these shapes mean\n",
        "batch_size, seq_len, hidden_dim = outputs.last_hidden_state.shape\n",
        "print(f\"\\nğŸ” What these shapes mean:\")\n",
        "print(f\"   ğŸ“¦ Batch size: {batch_size} (we're processing 1 sentence)\")\n",
        "print(f\"   ğŸ“ Sequence length: {seq_len} (number of tokens)\")\n",
        "print(f\"   ğŸ¯ Hidden dimensions: {hidden_dim} (size of each token's representation)\")\n",
        "\n",
        "# ğŸ¯ Create sentence-level embedding via mean pooling\n",
        "print(f\"\\nğŸ¯ Creating sentence embedding via mean pooling...\")\n",
        "last_hidden = outputs.last_hidden_state  # [batch, seq_len, hidden_dim]\n",
        "attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "# Apply attention mask and compute mean\n",
        "mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "sum_embeddings = torch.sum(last_hidden * mask_expanded, dim=1)\n",
        "sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
        "sentence_vector = (sum_embeddings / sum_mask).squeeze(0)\n",
        "\n",
        "print(f\"âœ… Sentence embedding shape: {sentence_vector.shape}\")\n",
        "print(f\"ğŸ’¡ We've converted '{test_sentence}' into a {len(sentence_vector)}-dimensional vector!\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "34fa1d38"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“ Wrap-up & Next Steps {#wrap-up}\n",
        "\n",
        "Congratulations! You've completed your journey through the foundations of Large Language Models. \n",
        "\n",
        "### ğŸ¯ What You've Accomplished\n",
        "\n",
        "âœ… **Understanding LLMs**: You learned why LLMs revolutionized NLP through unified, large-scale models  \n",
        "âœ… **Tokenization Analysis**: You explored how different languages are tokenized and the implications for low-resource languages  \n",
        "âœ… **Embedding Visualization**: You converted sentences to vectors and visualized semantic relationships in 2D space  \n",
        "âœ… **Transformer Internals**: You peeked inside a transformer to understand tensor shapes and data flow  \n",
        "âœ… **Multilingual Insights**: You gained hands-on experience with how models handle multiple languages\n",
        "\n",
        "### ğŸ” Key Insights\n",
        "\n",
        "- **Tokenization matters**: Low-resource languages often require more tokens, affecting cost and performance\n",
        "- **Embeddings capture meaning**: Similar sentences cluster together regardless of language (when models work well)\n",
        "- **Transformers process sequences**: Text flows through attention layers as high-dimensional tensors\n",
        "- **Representation quality varies**: Some languages are better represented than others in multilingual models\n",
        "\n",
        "### ğŸš€ Suggested Next Steps\n",
        "\n",
        "#### ğŸ”¬ **Experiment Further**\n",
        "- Try different multilingual models (`bert-base-multilingual-cased`, `google/mt5-small`)\n",
        "- Replace example sentences with text from your domain/language\n",
        "- Add more languages and analyze clustering patterns\n",
        "- Experiment with different embedding models\n",
        "\n",
        "#### ğŸ“š **Deepen Your Knowledge**\n",
        "- Learn about attention mechanisms in transformers\n",
        "- Explore fine-tuning for your specific language/task\n",
        "- Study cross-lingual transfer learning techniques\n",
        "- Investigate bias and fairness in multilingual models\n",
        "\n",
        "#### ğŸ› ï¸ **Apply Your Skills**\n",
        "- Use these concepts in downstream tasks (classification, summarization, QA)\n",
        "- Build applications using sentence embeddings for semantic search\n",
        "- Contribute to improving low-resource language support\n",
        "\n",
        "### ğŸŒŸ Remember\n",
        "\n",
        "The field of multilingual NLP is rapidly evolving. The techniques you've learned here are foundational - use them as building blocks for more advanced applications and research!\n",
        "\n",
        "---\n",
        "\n",
        "**Happy coding! ğŸš€**\n",
        ""
      ],
      "id": "e258d61a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âš™ï¸ Inside Transformers {#transformers}\n",
        "\n",
        "Let's peek inside a transformer model to understand how it processes our tokenized text.\n"
      ],
      "id": "e76613e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "### ğŸ” What Happens Inside a Transformer?\n",
        "\n",
        "We won't dive into the complex mathematics, but we can run a **forward pass** through a transformer and examine the tensor shapes to understand the data flow.\n",
        "\n",
        "### ğŸ¯ Our Exploration\n",
        "\n",
        "1. **Load** a small multilingual transformer (encoder-only)\n",
        "2. **Tokenize** one of our sentences\n",
        "3. **Inspect** the shapes of key tensors:\n",
        "   - `input_ids` - The tokenized input\n",
        "   - `attention_mask` - Which tokens to pay attention to\n",
        "   - `last_hidden_state` - The final representations\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "83e7fcb1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Create dialogue windows\n",
        "\n",
        "Many dialogue datasets are long conversations. Summarization is easier to teach with smaller windows. We will create overlapping windows of turns, then treat each window as a dialogue sample.\n",
        "\n",
        "You can adjust the window size. Smaller windows are easier for small models. Larger windows stress test context handling.\n"
      ],
      "id": "f97ad9d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def make_dialogue_windows(turns: pd.DataFrame, window_turns: int = 10, stride: int = 6) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert a turn DataFrame into overlapping dialogue windows.\n",
        "\n",
        "    Returns a DataFrame with: sample_id, dialogue_text, speakers_involved, n_turns.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    n = len(turns)\n",
        "    sample_id = 0\n",
        "    for start in range(0, max(1, n - window_turns + 1), stride):\n",
        "        end = min(n, start + window_turns)\n",
        "        chunk = turns.iloc[start:end]\n",
        "        dialogue_lines = [f\"{r.speaker}: {r.text}\" for r in chunk.itertuples()]\n",
        "        dialogue_text = \"\\n\".join(dialogue_lines)\n",
        "        speakers = sorted(set(chunk[\"speaker\"].tolist()))\n",
        "        samples.append(\n",
        "            {\n",
        "                \"sample_id\": sample_id,\n",
        "                \"dialogue_text\": dialogue_text,\n",
        "                \"speakers_involved\": speakers,\n",
        "                \"n_turns\": int(end - start),\n",
        "            }\n",
        "        )\n",
        "        sample_id += 1\n",
        "        if end == n:\n",
        "            break\n",
        "    return pd.DataFrame(samples)\n",
        "\n",
        "samples_df = make_dialogue_windows(turns_df, window_turns=10, stride=6)\n",
        "samples_df\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e0b57c59"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Preview one sample\n",
        "\n",
        "Read the dialogue. Then, in your own words, write a one sentence summary in the next cell. Keep it short. This will become our first human reference.\n"
      ],
      "id": "54a6296f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sample = samples_df.loc[0, \"dialogue_text\"]\n",
        "print(sample)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "070188fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your one sentence reference summary.\n",
        "# You can edit this string. The notebook will still run if you do not.\n",
        "\n",
        "REFERENCE_SUMMARY = \"Jack arrives and learns Algernon is visiting, then Algernon teases Jack and reveals he plans to marry Jack's cousin Cecily.\"\n",
        "\n",
        "print(REFERENCE_SUMMARY)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c84d3f50"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Baseline. Extractive TextRank summarization\n",
        "\n",
        "Before using an LLM, build a baseline that is fast, cheap, and interpretable. TextRank selects the most central sentences using a similarity graph and PageRank.\n",
        "\n",
        "This baseline is language agnostic, as long as you can split text into sentences. That is why it is valuable for low resource languages.\n"
      ],
      "id": "6406b9ca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "\n",
        "def split_sentences(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Very simple sentence splitter.\n",
        "    For robust multilingual splitting, consider spaCy or Stanza.\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    sents = re.split(r\"(?<=[\\.\\?\\!])\\s+\", text)\n",
        "    return [s.strip() for s in sents if s.strip()]\n",
        "\n",
        "def textrank_summarize(dialogue_text: str, max_sentences: int = 2) -> str:\n",
        "    \"\"\"\n",
        "    Extractive summarization using TextRank on sentence similarity.\n",
        "    \"\"\"\n",
        "    content = re.sub(r\"^[A-Z][A-Z\\s'\\-]+:\\s*\", \"\", dialogue_text, flags=re.MULTILINE)\n",
        "    sentences = split_sentences(content)\n",
        "    if not sentences:\n",
        "        return \"\"\n",
        "    if len(sentences) <= max_sentences:\n",
        "        return \" \".join(sentences)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    sim = cosine_similarity(X)\n",
        "    np.fill_diagonal(sim, 0.0)\n",
        "\n",
        "    graph = nx.from_numpy_array(sim)\n",
        "    scores = nx.pagerank(graph, max_iter=200)\n",
        "\n",
        "    ranked = sorted(range(len(sentences)), key=lambda i: scores.get(i, 0.0), reverse=True)\n",
        "    picked = sorted(ranked[:max_sentences])\n",
        "    return \" \".join([sentences[i] for i in picked])\n",
        "\n",
        "baseline_summary = textrank_summarize(sample, max_sentences=2)\n",
        "print(\"Baseline summary:\\n\", baseline_summary)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "db2aed65"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Quick evaluation. ROUGE\n",
        "\n",
        "ROUGE is imperfect, but it is a quick sanity check. We will compute ROUGE 1, ROUGE 2, and ROUGE L against your reference summary.\n"
      ],
      "id": "a18a7d4f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def rouge_scores(pred: str, ref: str) -> Dict[str, float]:\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    scores = scorer.score(ref, pred)\n",
        "    return {k: v.fmeasure for k, v in scores.items()}\n",
        "\n",
        "print(\"ROUGE (baseline vs reference):\")\n",
        "rouge_scores(baseline_summary, REFERENCE_SUMMARY)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "336ed187"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Mini quiz. What makes dialogue summarization harder?\n",
        "\n",
        "Try to answer before running the cell. Then run it for instant feedback.\n"
      ],
      "id": "6a97e0e1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "except Exception:\n",
        "    widgets = None\n",
        "\n",
        "QUESTION = \"Which factor is most specific to dialogue summarization, compared to single speaker summarization?\"\n",
        "OPTIONS = [\n",
        "    \"A. Dialogues contain named entities.\",\n",
        "    \"B. Dialogues include speaker turns and pragmatic intent.\",\n",
        "    \"C. Dialogues use punctuation.\",\n",
        "    \"D. Dialogues are always longer than articles.\",\n",
        "]\n",
        "CORRECT = 1\n",
        "EXPLANATION = \"Speaker turns and pragmatic intent are core. You often need to resolve who said what and why.\"\n",
        "\n",
        "def run_quiz():\n",
        "    if widgets is None:\n",
        "        print(QUESTION)\n",
        "        for opt in OPTIONS:\n",
        "            print(opt)\n",
        "        print(\"\\nCorrect:\", OPTIONS[CORRECT])\n",
        "        print(\"Explanation:\", EXPLANATION)\n",
        "        return\n",
        "\n",
        "    radio = widgets.RadioButtons(options=OPTIONS, description=\"Your answer:\")\n",
        "    out = widgets.Output()\n",
        "\n",
        "    def on_change(change):\n",
        "        if change[\"name\"] != \"value\":\n",
        "            return\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            idx = OPTIONS.index(change[\"new\"])\n",
        "            if idx == CORRECT:\n",
        "                print(\"Correct.\")\n",
        "            else:\n",
        "                print(\"Not quite.\")\n",
        "            print(\"Explanation:\", EXPLANATION)\n",
        "\n",
        "    radio.observe(on_change)\n",
        "    display(radio, out)\n",
        "\n",
        "run_quiz()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "48c54153"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸ” **Checkpoint 1: You now have a solid baseline!** \n",
        "\n",
        "**âœ… What you've accomplished:**\n",
        "- Built a dialogue dataset from raw text\n",
        "- Implemented TextRank extractive summarization  \n",
        "- Evaluated with ROUGE metrics\n",
        "- Learned what makes dialogue summarization challenging\n",
        "\n",
        "**ğŸ¯ Next up:** We'll simulate low-resource conditions and learn adaptation strategies.\n",
        "\n",
        "**ğŸ’¡ For LLM-based summarization:** Check out Session 2 on Prompt Engineering!\n"
      ],
      "id": "2258f279"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ“Š Quick Results Summary\n",
        "\n",
        "# Display summary of our baseline approach\n",
        "results_summary = {\n",
        "    \"Approach\": \"TextRank (Extractive)\",\n",
        "    \"Model Size\": \"No model required\",\n",
        "    \"Hardware\": \"CPU sufficient\", \n",
        "    \"Language Support\": \"Any language\",\n",
        "    \"Training Data\": \"None required\",\n",
        "    \"Key Advantage\": \"Fast, interpretable, language-agnostic\"\n",
        "}\n",
        "\n",
        "print(\"ğŸ¯ BASELINE APPROACH SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "for key, value in results_summary.items():\n",
        "    print(f\"{key:15}: {value}\")\n",
        "    \n",
        "print(f\"\\nâœ… Your ROUGE score: {rouge_scores(baseline_summary, REFERENCE_SUMMARY)}\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9612676b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Low Resource Mode ğŸŒ\n",
        "\n",
        "Now we'll simulate low-resource conditions and learn adaptation strategies.\n",
        "\n",
        "**What makes a language \"low-resource\"?**\n",
        "- Very little labeled data\n",
        "- Limited preprocessing tools  \n",
        "- Domain mismatch with training data\n",
        "- Orthographic variation and noise\n"
      ],
      "id": "fc256477"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "### 4.1 Simulate Low-Resource Conditions\n",
        "\n",
        "We'll corrupt our clean English dialogue to simulate challenges faced by low-resource languages:\n",
        "\n",
        "def low_resource_corrupt(text: str, drop_punct_prob: float = 0.5, typo_prob: float = 0.08) -> str:\n",
        "    \"\"\"Simulate low-resource conditions by adding noise\"\"\"\n",
        "    import random\n",
        "    rng = random.Random(842)\n",
        "    out_chars = []\n",
        "    for ch in text:\n",
        "        # Randomly drop punctuation\n",
        "        if ch in \".?!,\" and rng.random() < drop_punct_prob:\n",
        "            continue\n",
        "        # Add random typos\n",
        "        if ch.isalpha() and rng.random() < typo_prob:\n",
        "            if rng.random() < 0.5:\n",
        "                out_chars.append(ch.swapcase())  # Case error\n",
        "            else:\n",
        "                out_chars.append(chr(((ord(ch.lower()) - 97 + 1) % 26) + 97))  # Letter shift\n",
        "        else:\n",
        "            out_chars.append(ch)\n",
        "    return \"\".join(out_chars)\n",
        "\n",
        "# Apply corruption to our sample\n",
        "low_resource_sample = low_resource_corrupt(sample, drop_punct_prob=0.6, typo_prob=0.04)\n",
        "print(\"ğŸŒ SIMULATED LOW-RESOURCE TEXT:\")\n",
        "print(\"=\"*60)\n",
        "print(low_resource_sample[:400] + \"...\")\n",
        "print(\"\\nğŸ’¡ Notice: Missing punctuation, typos, inconsistent casing\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d2f93aca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Prompt remix playground\n",
        "\n",
        "You will remix a prompt by selecting options. This is a safe way to teach prompt engineering without making it feel abstract.\n",
        "\n",
        "Pick your settings, then run the cell. Try to make the summary both concise and faithful.\n"
      ],
      "id": "8c400a2f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "STYLE_OPTIONS = [\"neutral\", \"bullet\", \"tweet\", \"meeting_minutes\"]\n",
        "FOCUS_OPTIONS = [\"decisions\", \"conflict\", \"relationships\", \"actions\"]\n",
        "\n",
        "def build_prompt(style: str, focus: str, max_sentences: int) -> str:\n",
        "    style = style.lower().strip()\n",
        "    focus = focus.lower().strip()\n",
        "\n",
        "    base = f\"Summarize the conversation in at most {max_sentences} sentences.\"\n",
        "    if focus == \"decisions\":\n",
        "        base += \" Focus on decisions and commitments.\"\n",
        "    elif focus == \"conflict\":\n",
        "        base += \" Focus on disagreements and what caused them.\"\n",
        "    elif focus == \"relationships\":\n",
        "        base += \" Focus on who relates to whom and the social situation.\"\n",
        "    elif focus == \"actions\":\n",
        "        base += \" Focus on actions and next steps.\"\n",
        "\n",
        "    if style == \"bullet\":\n",
        "        base += \" Use 2 to 4 bullet points.\"\n",
        "    elif style == \"tweet\":\n",
        "        base += \" Write it as a single tweet style sentence, under 240 characters.\"\n",
        "    elif style == \"meeting_minutes\":\n",
        "        base += \" Format as meeting minutes with sections: Context, Key Points, Next Steps.\"\n",
        "\n",
        "    base += \" Do not invent facts. Preserve names.\"\n",
        "    return base\n",
        "\n",
        "def run_playground(style=\"neutral\", focus=\"relationships\", max_sentences=2):\n",
        "    prompt = build_prompt(style, focus, max_sentences)\n",
        "    print(\"Prompt:\\n\", prompt, \"\\n\")\n",
        "    out = generate_summary_t5(sample, prompt, max_new_tokens=120, temperature=0.0)\n",
        "    print(\"Model output:\\n\", out)\n",
        "    return out\n",
        "\n",
        "llm_play = run_playground(style=\"meeting_minutes\", focus=\"relationships\", max_sentences=2)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9e2015b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 One shot and few shot prompts\n",
        "\n",
        "When you have little data, examples are powerful. We will create a small in notebook prompt set.\n",
        "\n",
        "You can replace the examples with your own dialogues later.\n"
      ],
      "id": "c375adbb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "EXAMPLE_DIALOGUE = \"\"\"ALICE: Are we still meeting at 3?\n",
        "BOB: Yes, but I will be 10 minutes late.\n",
        "ALICE: Ok. Please bring the slides.\n",
        "BOB: Will do.\"\"\"\n",
        "\n",
        "EXAMPLE_SUMMARY = \"Alice and Bob confirm a 3 pm meeting. Bob will arrive 10 minutes late and will bring the slides.\"\n",
        "\n",
        "ONE_SHOT_PROMPT = f\"\"\"Summarize the conversation in 1 to 2 sentences. Do not invent facts.\n",
        "\n",
        "Example.\n",
        "DIALOGUE:\n",
        "{EXAMPLE_DIALOGUE}\n",
        "\n",
        "SUMMARY:\n",
        "{EXAMPLE_SUMMARY}\n",
        "\n",
        "Now summarize this dialogue.\n",
        "\"\"\"\n",
        "\n",
        "llm_one = generate_summary_t5(sample, ONE_SHOT_PROMPT, max_new_tokens=120, temperature=0.0)\n",
        "print(llm_one)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d9939ace"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Generation parameters. Temperature and length\n",
        "\n",
        "Temperature can change factuality. Length controls how much detail you get.\n",
        "\n",
        "Use the sliders if available. Otherwise, edit the numbers and rerun.\n"
      ],
      "id": "c395098e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def demo_generation_controls(temperature: float = 0.0, max_new_tokens: int = 80):\n",
        "    prompt = build_prompt(style=\"neutral\", focus=\"actions\", max_sentences=2)\n",
        "    out = generate_summary_t5(sample, prompt, max_new_tokens=max_new_tokens, temperature=temperature, top_p=0.95)\n",
        "    print(\"temperature:\", temperature, \"max_new_tokens:\", max_new_tokens)\n",
        "    print(out)\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "    if tokenizer is None or model is None:\n",
        "        raise RuntimeError(\"Model not available, skipping widgets.\")\n",
        "    ui = widgets.interactive(\n",
        "        demo_generation_controls,\n",
        "        temperature=widgets.FloatSlider(min=0.0, max=1.0, step=0.1, value=0.0),\n",
        "        max_new_tokens=widgets.IntSlider(min=30, max=200, step=10, value=80),\n",
        "    )\n",
        "    display(ui)\n",
        "except Exception:\n",
        "    demo_generation_controls(temperature=0.0, max_new_tokens=80)\n",
        "    demo_generation_controls(temperature=0.7, max_new_tokens=120)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a552b490"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compare Baselines vs LLM\n",
        "\n",
        "We compare summaries and compute ROUGE against your reference.\n",
        "\n",
        "In real work, you should also do human evaluation. For example factuality checks, missing action items, and speaker attribution.\n"
      ],
      "id": "ed7860c7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = []\n",
        "results.append((\"TextRank baseline\", baseline_summary))\n",
        "results.append((\"LLM zero shot\", llm_zero))\n",
        "results.append((\"LLM one shot\", llm_one))\n",
        "results.append((\"LLM prompt remix\", llm_play))\n",
        "\n",
        "rows = []\n",
        "for name, pred in results:\n",
        "    rows.append({\"system\": name, \"summary\": pred, **rouge_scores(pred, REFERENCE_SUMMARY)})\n",
        "\n",
        "pd.DataFrame(rows).sort_values(\"rougeL\", ascending=False)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a176f348"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Low resource mode. Make English behave like a low resource language\n",
        "\n",
        "Low resource usually means one or more of the following.\n",
        "- Very little labeled data.\n",
        "- Limited tools for tokenization, sentence splitting, and normalization.\n",
        "- Domain mismatch. Your data looks different from what models saw during pre training.\n",
        "- Orthography variation and borrowing, including code switching.\n",
        "\n",
        "We will simulate these constraints in English by.\n",
        "1) Reducing the available context.\n",
        "2) Corrupting the text with noise and inconsistent spelling.\n",
        "3) Removing punctuation, which hurts naive sentence splitting.\n",
        "\n",
        "Then we apply strategies that transfer to true low resource settings.\n"
      ],
      "id": "d90e6fb1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def low_resource_corrupt(text: str, drop_punct_prob: float = 0.5, typo_prob: float = 0.08) -> str:\n",
        "    rng = random.Random(842)\n",
        "    out_chars = []\n",
        "    for ch in text:\n",
        "        if ch in \".?!,\" and rng.random() < drop_punct_prob:\n",
        "            continue\n",
        "        if ch.isalpha() and rng.random() < typo_prob:\n",
        "            if rng.random() < 0.5:\n",
        "                out_chars.append(ch.swapcase())\n",
        "            else:\n",
        "                out_chars.append(chr(((ord(ch.lower()) - 97 + 1) % 26) + 97))\n",
        "        else:\n",
        "            out_chars.append(ch)\n",
        "    return \"\".join(out_chars)\n",
        "\n",
        "low_text = low_resource_corrupt(sample, drop_punct_prob=0.8, typo_prob=0.05)\n",
        "print(low_text[:600])\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f19d5077"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Baseline on clean text:\")\n",
        "print(textrank_summarize(sample, max_sentences=2))\n",
        "print(\"\\nBaseline on low resource corrupted text:\")\n",
        "print(textrank_summarize(low_text, max_sentences=2))\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "60dc3964"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Strategy toolkit\n",
        "\n",
        "Here are practical tactics that often help in low resource dialogue summarization.\n",
        "\n",
        "1. Normalize input.\n",
        "   - Fix common punctuation issues.\n",
        "   - Normalize whitespace.\n",
        "   - Normalize speaker labels.\n",
        "\n",
        "2. Use robust segmentation.\n",
        "   - If sentence splitting fails, summarize at turn level.\n",
        "\n",
        "3. Constrain generation.\n",
        "   - Use explicit length limits.\n",
        "   - Instruct the model to preserve names, numbers, and decisions.\n",
        "\n",
        "4. Add lightweight context.\n",
        "   - Provide a glossary of names and places.\n",
        "   - Provide a domain hint, such as \"family conversation\" or \"customer support\".\n",
        "\n",
        "5. Evaluate with targeted checks.\n",
        "   - Did we preserve who wants to marry whom.\n",
        "   - Did we hallucinate actions that never happened.\n",
        "\n",
        "We will implement 1 and 2 now.\n"
      ],
      "id": "df866059"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def normalize_dialogue(text: str) -> str:\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"([A-Z][A-Z\\s'\\-]+:)\\s*\", r\"\\n\\1 \", text)\n",
        "    return text.strip()\n",
        "\n",
        "def turn_level_summarize(dialogue_text: str, max_turns: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Extractive turn level summarization, more robust than sentence splitting.\n",
        "    \"\"\"\n",
        "    lines = [ln.strip() for ln in dialogue_text.splitlines() if ln.strip()]\n",
        "    lines = [ln for ln in lines if len(ln) > 10]\n",
        "    if not lines:\n",
        "        return \"\"\n",
        "    if len(lines) <= max_turns:\n",
        "        return \" \".join(lines)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "    X = vectorizer.fit_transform(lines)\n",
        "    sim = cosine_similarity(X)\n",
        "    np.fill_diagonal(sim, 0.0)\n",
        "    graph = nx.from_numpy_array(sim)\n",
        "    scores = nx.pagerank(graph, max_iter=200)\n",
        "    ranked = sorted(range(len(lines)), key=lambda i: scores.get(i, 0.0), reverse=True)\n",
        "    picked = sorted(ranked[:max_turns])\n",
        "    return \" \".join([lines[i] for i in picked])\n",
        "\n",
        "print(\"Before normalization:\\n\", low_text[:250], \"\\n\")\n",
        "norm_low = normalize_dialogue(low_text)\n",
        "print(\"After normalization:\\n\", norm_low[:250])\n",
        "print(\"\\nTurn level summary on corrupted text:\\n\", turn_level_summarize(norm_low, max_turns=3))\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0a203e30"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Low resource prompting\n",
        "\n",
        "If you can use an instruction model, you can push it to behave better on noisy input. The key is to add constraints.\n",
        "\n",
        "We will.\n",
        "- Ask for short output.\n",
        "- Ask it to avoid inventing facts.\n",
        "- Ask it to preserve names.\n"
      ],
      "id": "0a8837fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "LOW_RESOURCE_PROMPT = \"\"\"Summarize the conversation in 1 sentence.\n",
        "Rules.\n",
        "1) Do not invent facts.\n",
        "2) Preserve names exactly as they appear.\n",
        "3) If the text is noisy, infer only what is obvious.\"\"\"\n",
        "\n",
        "if tokenizer is None or model is None:\n",
        "    print(\"Model not available, skipping.\")\n",
        "else:\n",
        "    print(generate_summary_t5(norm_low, LOW_RESOURCE_PROMPT, max_new_tokens=60, temperature=0.0))\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e8bd8227"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional mini dataset hook. Try a real non-English case in two minutes\n",
        "\n",
        "This workshop is designed to start in English, then transfer the same workflow to a low-resource language.\n",
        "\n",
        "Below are two quick options.\n",
        "\n",
        "1. **MiniLux micro-set (synthetic)**. A small set of short Luxembourgish and LU, FR mixed snippets created for teaching. It is intentionally tiny and imperfect, so that you can iterate fast and discuss typical issues, like code-switching, named entities, and spelling variation.\n",
        "\n",
        "2. **Hugging Face low-resource sample (real text)**. Pull 20 examples from a multilingual summarization dataset and run the same prompt, plus the same evaluation, to see how performance changes outside English.\n"
      ],
      "id": "57f285af"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Option 1. MiniLux micro-set (synthetic)\n",
        "# This is only for the workshop. You can replace it with your own low-resource dialogues later.\n",
        "\n",
        "mini_lux = [\n",
        "    {\n",
        "        \"id\": \"lux_001\",\n",
        "        \"dialogue\": \"A: Moien. Hues du ZÃ¤it fir e Kaffi?\\nB: Jo, mÃ¤ just zÃ©ng Minutten. Ech muss glÃ¤ich op d'Aarbecht.\\nA: Ok. Mir treffen eis beim Gare.\\nB: Super, ech kommen direkt.\",\n",
        "        \"reference_summary_en\": \"They agree to meet for a quick coffee at the station before B goes to work.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_002\",\n",
        "        \"dialogue\": \"A: WÃ©i war d'Reunioun haut?\\nB: Ganz laang. Mir hu just d'Agenda diskutÃ©iert.\\nA: An hu mir eng Decisioun?\\nB: Nee, mir maachen et nÃ¤chste Woch nach eng KÃ©ier.\",\n",
        "        \"reference_summary_en\": \"The meeting was long, they only discussed the agenda, and no decision was made.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_003\",\n",
        "        \"dialogue\": \"A: Kanns du mer de Rapport schÃ©cken?\\nB: Jo. Ech schÃ©cken en elo per Mail.\\nA: Merci. Ech muss en nach haut ofginn.\\nB: Kloer, ech maachen et direkt.\",\n",
        "        \"reference_summary_en\": \"B will email A the report immediately because A must submit it today.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_004\",\n",
        "        \"dialogue\": \"A: Ech sinn am Stau op der A6.\\nB: Ok, dann fÃ¤nke mir ouni dech un.\\nA: Gitt mir zÃ©ng Minutten.\\nB: Passt. Mir halen dir e SÃ«tz frÃ¤i.\",\n",
        "        \"reference_summary_en\": \"A is stuck in traffic but will arrive in about ten minutes, and the others will start and save a seat.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_005\",\n",
        "        \"dialogue\": \"A: Ech hu muer en rendez-vous chez le mÃ©decin.\\nB: Bass du ok?\\nA: Jo, just e Check-up.\\nB: Ok, soen mer dono wÃ©i et gaangen ass.\",\n",
        "        \"reference_summary_en\": \"A has a doctor appointment tomorrow for a check-up and will update B afterward.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_006\",\n",
        "        \"dialogue\": \"A: Wou si mir mam Projet?\\nB: Mir hu 80 Prozent fÃ¤erdeg.\\nA: Wat feelt nach?\\nB: D'Dokumentatioun an d'Tester.\",\n",
        "        \"reference_summary_en\": \"The project is about 80 percent done, but documentation and testing are still missing.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_007\",\n",
        "        \"dialogue\": \"A: Ech krÃ©ien Ã«mmer eng Fehlermeldung.\\nB: WÃ©i eng?\\nA: 'Permission denied'.\\nB: Dann hues du wahrscheinlech keng Rechter. ProbÃ©ier et mat sudo oder fro den Admin.\",\n",
        "        \"reference_summary_en\": \"A gets a permission error, and B suggests using sudo or asking the admin for access.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_008\",\n",
        "        \"dialogue\": \"A: Mir treffen eis um 14:00.\\nB: Ech sinn um 14:15 do.\\nA: Ok, ech waarden am CafÃ©.\\nB: Merci. Bis glÃ¤ich.\",\n",
        "        \"reference_summary_en\": \"They planned to meet at 14:00, but B will arrive at 14:15 and A will wait at a cafÃ©.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_009\",\n",
        "        \"dialogue\": \"A: Hues du d'Presentatioun gesinn?\\nB: Jo, si ass gutt, mÃ¤ d'Grafike sinn ze kleng.\\nA: Ok, ech maachen se mÃ©i grouss.\\nB: Super, dann ass et perfekt.\",\n",
        "        \"reference_summary_en\": \"B thinks the presentation is good but the charts are too small, so A will enlarge them.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_010\",\n",
        "        \"dialogue\": \"A: Ech sinn haut am Homeoffice.\\nB: Ok, kÃ«nns du trotzdem an de Call?\\nA: Jo, ech sinn do um 10:00.\\nB: Top, ech schÃ©cken de Link.\",\n",
        "        \"reference_summary_en\": \"A works from home but will join the 10:00 call, and B will send the link.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_011\",\n",
        "        \"dialogue\": \"A: Mir brauche nach e Beispill fir d'Course.\\nB: Wat fir ee Beispill?\\nA: E klengt Dialog-Set fir Zesummefaassung.\\nB: Ok, ech schreiwen 20 kuerz Dialogen.\",\n",
        "        \"reference_summary_en\": \"They need a small dialogue dataset for a summarization course, and B will write 20 short dialogues.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_012\",\n",
        "        \"dialogue\": \"A: Kanns du den Text nach eng KÃ©ier kontrollÃ©ieren?\\nB: Jo, ech kucken no Tippfeeler.\\nA: An och Punktuatioun.\\nB: Maachen ech.\",\n",
        "        \"reference_summary_en\": \"B will proofread the text for typos and punctuation.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_013\",\n",
        "        \"dialogue\": \"A: Ech hu meng SchlÃ«sselen vergiess.\\nB: Wou bass du?\\nA: Virun der Dier.\\nB: Ech kommen, ginn mer fÃ«nnef Minutten.\",\n",
        "        \"reference_summary_en\": \"A forgot their keys and is locked out, and B will come in five minutes.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_014\",\n",
        "        \"dialogue\": \"A: De Bus kÃ«nnt net.\\nB: Hues du d'App gekuckt?\\nA: Jo, et steet 'retard'.\\nB: Dann huele mir en Taxi.\",\n",
        "        \"reference_summary_en\": \"The bus is delayed, so they decide to take a taxi.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_015\",\n",
        "        \"dialogue\": \"A: Ech muss nach d'Fichieren eroplueden.\\nB: Wou?\\nA: Op Hugging Face.\\nB: Ok, vergiss net d'Lizens an d'Readme.\",\n",
        "        \"reference_summary_en\": \"A needs to upload files to Hugging Face, and B reminds them to include a license and README.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_016\",\n",
        "        \"dialogue\": \"A: D'GPU ass frÃ¤i.\\nB: Super, dann starte mir den Training.\\nA: Ech setzen batch size op 4.\\nB: Ok, da maache mir gradient accumulation.\",\n",
        "        \"reference_summary_en\": \"They have GPU availability and will start training with a small batch size and gradient accumulation.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_017\",\n",
        "        \"dialogue\": \"A: Kanns du mir den Deadline soen?\\nB: Et ass Freideg um 18:00.\\nA: Merci, ech maachen et haut nach.\\nB: Gutt Iddi.\",\n",
        "        \"reference_summary_en\": \"The deadline is Friday at 18:00, and A plans to finish today.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_018\",\n",
        "        \"dialogue\": \"A: Ech hunn d'DonnÃ©eÃ«n gereinegt.\\nB: Super. Hues du och d'Nummeren normalisÃ©iert?\\nA: Jo, ech hunn se an Wierder Ã«mgewandelt.\\nB: Perfekt.\",\n",
        "        \"reference_summary_en\": \"A cleaned the data and normalized numbers by converting them into words.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_019\",\n",
        "        \"dialogue\": \"A: Ech verstinn d'Resultater net.\\nB: Wat ass komesch?\\nA: D'Accuracy ass hÃ©ich, mÃ¤ d'F1 ass niddreg.\\nB: Dann ass et wahrscheinlech Klassen-Imbalance.\",\n",
        "        \"reference_summary_en\": \"Accuracy is high but F1 is low, suggesting class imbalance.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_020\",\n",
        "        \"dialogue\": \"A: Tu peux me rappeler le plan?\\nB: Oui. D'abord on teste en anglais, aprÃ¨s on passe au luxembourgeois.\\nA: An de Prompt bleift Ã¤hnlech.\\nB: Genau.\",\n",
        "        \"reference_summary_en\": \"They will test in English first, then switch to Luxembourgish while keeping a similar prompt.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_021\",\n",
        "        \"dialogue\": \"A: Ech sinn net sÃ©cher ob 'Zentrum' richteg ass.\\nB: Et hÃ¤nkt vum Dialektgebiet of.\\nA: Ok, ech kontrollÃ©ieren d'Metadata.\\nB: Gutt, d'Labels mussen konsistent sinn.\",\n",
        "        \"reference_summary_en\": \"They will verify the metadata because dialect labels must be consistent.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_022\",\n",
        "        \"dialogue\": \"A: D'Audio ass ze laang.\\nB: WÃ©i laang?\\nA: 25 Sekonnen.\\nB: Dann schneiden mir et op 10 Sekonnen fir d'Training.\",\n",
        "        \"reference_summary_en\": \"The audio is 25 seconds long, so they will trim it to 10 seconds for training.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_023\",\n",
        "        \"dialogue\": \"A: Ech hu keng Internet um Laptop.\\nB: ProbÃ©ier d'WLAN nei.\\nA: Ok, ech maachen restart.\\nB: Wann et net geet, huele mir en Hotspot.\",\n",
        "        \"reference_summary_en\": \"A has no internet, B suggests restarting Wi-Fi, and they may use a hotspot if needed.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_024\",\n",
        "        \"dialogue\": \"A: D'Zesummefaassung ass ze laang.\\nB: Setz eng Limit.\\nA: WÃ©i vill?\\nB: ProbÃ©ier 2 SÃ¤tz an maximal 60 Wierder.\",\n",
        "        \"reference_summary_en\": \"They will constrain the summary length to two sentences and at most 60 words.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_025\",\n",
        "        \"dialogue\": \"A: Ech wÃ«ll eng neutral Zesummefaassung.\\nB: Da schreiwe mir am Prompt: 'neutral, factual, no opinion'.\\nA: Ok, ech testen dat.\\nB: Gutt, a kuck ob Bias kÃ«nnt.\",\n",
        "        \"reference_summary_en\": \"They want a neutral factual summary and will encode that in the prompt and then test for bias.\"\n",
        "    },\n",
        "]\n",
        "\n",
        "def sample_and_summarize(dialogue_set, k=1, seed=7, prompt=ZERO_SHOT_PROMPT):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    items = random.sample(dialogue_set, k=k)\n",
        "    for ex in items:\n",
        "        print(\"ID:\", ex[\"id\"])\n",
        "        print(\"\\nDIALOGUE:\\n\", ex[\"dialogue\"])\n",
        "        pred = generate_summary_t5(ex[\"dialogue\"], prompt=prompt, max_new_tokens=80, temperature=0.0)\n",
        "        print(\"\\nMODEL SUMMARY:\\n\", pred)\n",
        "        print(\"\\nREFERENCE (EN):\\n\", ex[\"reference_summary_en\"])\n",
        "        print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
        "\n",
        "sample_and_summarize(mini_lux, k=2)\n",
        "\n",
        "# Option 2. Pull a tiny real low-resource sample from Hugging Face\n",
        "# This uses XL-Sum (multilingual news summarization). Not a dialogue dataset.\n",
        "# For the workshop, we convert each article into a \"pseudo-dialogue\" so we can reuse the same pipeline.\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "def article_to_pseudo_dialogue(article_text: str, max_turns: int = 6) -> str:\n",
        "    # Lightweight sentence split. Good enough for teaching.\n",
        "    sentences = [s.strip() for s in article_text.replace(\"\\n\", \" \").split(\".\") if s.strip()]\n",
        "    sentences = sentences[:max_turns]\n",
        "    turns = []\n",
        "    for i, s in enumerate(sentences):\n",
        "        speaker = \"ANCHOR\" if i % 2 == 0 else \"REPORTER\"\n",
        "        turns.append(f\"{speaker}: {s}.\")\n",
        "    return \"\\n\".join(turns)\n",
        "\n",
        "def load_low_resource_hf_sample(language_subset: str = \"yoruba\", n: int = 20):\n",
        "    ds = load_dataset(\"csebuetnlp/xlsum\", language_subset, split=f\"train[:{n}]\")\n",
        "    # XL-Sum fields are typically: \"text\" and \"summary\"\n",
        "    out = []\n",
        "    for i, ex in enumerate(ds):\n",
        "        dialogue = article_to_pseudo_dialogue(ex[\"text\"], max_turns=8)\n",
        "        out.append(\n",
        "            {\n",
        "                \"id\": f\"xlsum_{language_subset}_{i:03d}\",\n",
        "                \"dialogue\": dialogue,\n",
        "                \"reference_summary\": ex[\"summary\"],\n",
        "            }\n",
        "        )\n",
        "    return out\n",
        "\n",
        "xlsum_yoruba = load_low_resource_hf_sample(language_subset=\"yoruba\", n=5)\n",
        "print(\"Example pseudo-dialogue from XL-Sum (yoruba subset):\")\n",
        "print(xlsum_yoruba[0][\"dialogue\"])\n",
        "print(\"\\nReference summary (yoruba):\")\n",
        "print(xlsum_yoruba[0][\"reference_summary\"])\n",
        "\n",
        "print(\"\\nNow run the same English prompt on the pseudo-dialogue. It will usually struggle, and that is the point.\")\n",
        "pred = generate_summary_t5(xlsum_yoruba[0][\"dialogue\"], prompt=ZERO_SHOT_PROMPT, max_new_tokens=80, temperature=0.0)\n",
        "print(\"\\nMODEL SUMMARY:\\n\", pred)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "33ca8b91"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Challenge. Adapt to your own low resource language\n",
        "\n",
        "Now you have an English pipeline. The next step is to replace the English dialogue with data from your target language.\n",
        "\n",
        "If you work on a language with limited resources, use the same structure.\n",
        "1) Create turns with speaker labels.\n",
        "2) Normalize and segment.\n",
        "3) Start with an extractive baseline.\n",
        "4) Add a multilingual model or a translation pivot only if you need it.\n",
        "5) Evaluate with a small set of human references.\n",
        "\n",
        "The next cell includes a ready to use template. It runs as is. Replace `MY_DIALOGUE` with your own data.\n"
      ],
      "id": "b94d5ff6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MY_DIALOGUE = \"\"\"SPEAKER1: Replace this with your own dialogue in any language.\n",
        "SPEAKER2: Keep speaker labels. Keep short lines if possible.\n",
        "SPEAKER1: Then rerun the cells below.\"\"\"\n",
        "\n",
        "clean = normalize_dialogue(MY_DIALOGUE)\n",
        "summary_baseline = turn_level_summarize(clean, max_turns=3)\n",
        "print(\"Baseline summary:\\n\", summary_baseline)\n",
        "\n",
        "if tokenizer is not None and model is not None:\n",
        "    prompt = build_prompt(style=\"neutral\", focus=\"actions\", max_sentences=2)\n",
        "    summary_llm = generate_summary_t5(clean, prompt, max_new_tokens=80, temperature=0.0)\n",
        "    print(\"\\nLLM summary:\\n\", summary_llm)\n",
        "else:\n",
        "    print(\"\\nLLM not available. Baseline is your default.\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8d0897db"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Wrap up\n",
        "\n",
        "You now have a reproducible dialogue summarization pipeline that is usable with.\n",
        "- No LLM, via TextRank and turn level extraction.\n",
        "- A small instruction model, via prompt engineering.\n",
        "- Low resource conditions, via normalization and constraints.\n",
        "\n",
        "If you want to push further for true low resource languages.\n",
        "- Swap English stopwords for a custom list, or disable stopwords.\n",
        "- Use character n gram TF IDF for languages without whitespace.\n",
        "- Add a small glossary and a retrieval step, then feed only the relevant turns to the model.\n",
        "- Build a tiny evaluation set, 50 to 200 dialogues with one reference summary each.\n"
      ],
      "id": "848f06d1"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}