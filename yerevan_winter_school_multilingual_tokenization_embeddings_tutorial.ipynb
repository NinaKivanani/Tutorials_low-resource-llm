{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33d5751",
   "metadata": {},
   "source": [
    "# Yerevan Winter School Tutorial 3\n",
    "## Multilingual Tokenization and Sentence Embeddings for Low Resource Languages\n",
    "\n",
    "**Context.**  \n",
    "In this hands on session, you will explore how multilingual models represent different languages at two levels.  \n",
    "First, you will inspect how tokenizers split sentences in English and in at least one low resource language of interest.  \n",
    "Second, you will generate sentence embeddings for short sentences in multiple languages and visualize them in two dimensions.\n",
    "\n",
    "**What you will do.**\n",
    "\n",
    "1. Use a multilingual tokenizer to inspect how sentences are split into tokens in English and another language.  \n",
    "2. Compare tokenization granularity and discuss how this might affect model performance.  \n",
    "3. Generate sentence embeddings for short sentences in multiple languages using a multilingual sentence encoder.  \n",
    "4. Visualize the embeddings in two dimensions and inspect clustering patterns.  \n",
    "5. Briefly document your observations about representation quality and typical failure cases for your language of interest.\n",
    "\n",
    "**Important note.**  \n",
    "The goal is to build intuition about how multilingual models handle your language, not to reach definitive scientific conclusions.  \n",
    "We work with very small examples so that everything fits into a short tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b317ba47",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Run the following cells to install and import the required libraries.  \n",
    "This notebook is designed for Google Colab, but it will also work in a local Jupyter environment with internet access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652223d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers sentence-transformers datasets umap-learn matplotlib pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc4cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b530d8fa",
   "metadata": {},
   "source": [
    "## 1. Configuration. choose your languages and example sentences\n",
    "\n",
    "In this section, you define:\n",
    "\n",
    "- At least one low resource language of interest.  \n",
    "- A small set of short sentences in each language.  \n",
    "- The multilingual models you want to inspect.\n",
    "\n",
    "By default, we include English (`en`) and Luxembourgish (`lb`) as an example low resource language.  \n",
    "You can replace `lb` with another language that is relevant to you, for example Armenian (`hy`), Kurdish (`ku`), or any language supported by your chosen models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the languages you will work with.\n",
    "# You must include \"en\" for English and at least one additional language code.\n",
    "\n",
    "languages = [\n",
    "    {\"code\": \"en\", \"name\": \"English\"},\n",
    "    {\"code\": \"lb\", \"name\": \"Luxembourgish\"},  # change this to your low resource language if you prefer\n",
    "]\n",
    "\n",
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55106bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example sentences for each language.\n",
    "# You can modify these or replace them with your own small set.\n",
    "\n",
    "example_sentences = {\n",
    "    \"en\": [\n",
    "        \"The doctor arrived late to the hospital.\",\n",
    "        \"Children are playing outside in the snow.\",\n",
    "        \"This research project focuses on low resource languages.\",\n",
    "        \"The bus was very crowded this morning.\",\n",
    "        \"I enjoy reading books in different languages.\",\n",
    "    ],\n",
    "    \"lb\": [\n",
    "        \"Den Dokter ass spéit am Spidol ukomm.\",\n",
    "        \"Kanner spillen dobaussen am Schnéi.\",\n",
    "        \"Dëse Fuerschungsprojet konzentréiert sech op Sproochen mat wéineg Ressourcen.\",\n",
    "        \"De Bus war haut de Moien ganz voll.\",\n",
    "        \"Ech liese gär Bicher an ënnerschiddleche Sproochen.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Quick sanity check\n",
    "for lang in languages:\n",
    "    code_ = lang[\"code\"]\n",
    "    print(f\"Language {code_} has {len(example_sentences.get(code_, []))} sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07017a",
   "metadata": {},
   "source": [
    "If your low resource language is different from Luxembourgish, update:\n",
    "\n",
    "- The `languages` list above.  \n",
    "- The corresponding entry in `example_sentences`.  \n",
    "\n",
    "Make sure that each language code in `languages` has a matching key in `example_sentences` with at least three sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ccedcc",
   "metadata": {},
   "source": [
    "## 2. Choose multilingual models for tokenization and embeddings\n",
    "\n",
    "We will typically use two related but not identical models.\n",
    "\n",
    "- A multilingual tokenizer, for example from `xlm-roberta-base` or a similar encoder only model.  \n",
    "- A multilingual sentence transformer, for example `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`.\n",
    "\n",
    "You can modify the defaults below if you have a specific model you want to inspect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model used to inspect tokenization behaviour.\n",
    "tokenizer_model_name = \"xlm-roberta-base\"\n",
    "\n",
    "# Model used to generate sentence embeddings.\n",
    "embedding_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n",
    "print(\"Tokenizer loaded from:\", tokenizer_model_name)\n",
    "\n",
    "embedder = SentenceTransformer(embedding_model_name)\n",
    "print(\"Sentence embedding model loaded from:\", embedding_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e87ef",
   "metadata": {},
   "source": [
    "## 3. Inspecting tokenization behaviour\n",
    "\n",
    "In this section, you will:\n",
    "\n",
    "1. Tokenize each sentence using the chosen multilingual tokenizer.  \n",
    "2. Inspect the tokens, including special characters that indicate subword splits.  \n",
    "3. Compare token counts and average token length across languages.\n",
    "\n",
    "Tokenization granularity matters because:\n",
    "\n",
    "- Shorter tokens mean longer sequences for the same sentence, which increases computation cost.  \n",
    "- Very fragmented tokenization may harm performance for your language if the model sees many rare subword patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a43ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence: str) -> Dict:\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"token_ids\": token_ids,\n",
    "        \"num_tokens\": len(tokens),\n",
    "    }\n",
    "\n",
    "# Build a table with tokenization details for all sentences.\n",
    "rows = []\n",
    "for lang in languages:\n",
    "    code_ = lang[\"code\"]\n",
    "    name_ = lang[\"name\"]\n",
    "    for sent in example_sentences.get(code_, []):\n",
    "        tok_info = tokenize_sentence(sent)\n",
    "        rows.append({\n",
    "            \"language_code\": code_,\n",
    "            \"language_name\": name_,\n",
    "            \"sentence\": sent,\n",
    "            \"tokens\": tok_info[\"tokens\"],\n",
    "            \"num_tokens\": tok_info[\"num_tokens\"],\n",
    "            \"avg_chars_per_token\": len(sent) / max(tok_info[\"num_tokens\"], 1),\n",
    "        })\n",
    "\n",
    "token_df = pd.DataFrame(rows)\n",
    "token_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba26390",
   "metadata": {},
   "source": [
    "You can scroll through the table above to see how sentences in each language are split into tokens.\n",
    "\n",
    "The `avg_chars_per_token` column gives a rough indication of granularity.  \n",
    "Lower values mean that the tokenizer splits the sentence into many short pieces.  \n",
    "Higher values mean that tokens are longer on average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef54e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by language.\n",
    "summary_token_stats = token_df.groupby([\"language_code\", \"language_name\"])[\n",
    "    [\"num_tokens\", \"avg_chars_per_token\"]\n",
    "].agg([\"mean\", \"min\", \"max\"]).round(2)\n",
    "\n",
    "summary_token_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e15ab",
   "metadata": {},
   "source": [
    "### 3.1 Visualizing token counts\n",
    "\n",
    "Run the cell below to visualize average token counts per language in a simple bar chart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0accdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average number of tokens per sentence by language.\n",
    "avg_tokens = token_df.groupby(\"language_code\")[\"num_tokens\"].mean()\n",
    "\n",
    "plt.figure()\n",
    "avg_tokens.plot(kind=\"bar\")\n",
    "plt.xlabel(\"Language code\")\n",
    "plt.ylabel(\"Average number of tokens per sentence\")\n",
    "plt.title(\"Average tokenization length by language\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5618800d",
   "metadata": {},
   "source": [
    "### 3.2 Inspect specific tokenizations\n",
    "\n",
    "If you see interesting or surprising patterns, you can inspect them more closely by printing tokens for selected sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c925aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these indices or conditions to inspect specific examples.\n",
    "\n",
    "for idx, row in token_df.head(10).iterrows():\n",
    "    print(\"Language:\", row[\"language_code\"])\n",
    "    print(\"Sentence:\", row[\"sentence\"])\n",
    "    print(\"Tokens:\", row[\"tokens\"])\n",
    "    print(\"Number of tokens:\", row[\"num_tokens\"])\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d8c2d",
   "metadata": {},
   "source": [
    "You can adapt the loop above to filter for a specific language or a specific sentence.  \n",
    "For example, you can look at the longest or shortest tokenizations per language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9878ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example. inspect the three sentences with the highest number of tokens.\n",
    "\n",
    "top_long = token_df.sort_values(\"num_tokens\", ascending=False).head(3)\n",
    "for idx, row in top_long.iterrows():\n",
    "    print(\"Language:\", row[\"language_code\"])\n",
    "    print(\"Sentence:\", row[\"sentence\"])\n",
    "    print(\"Tokens:\", row[\"tokens\"])\n",
    "    print(\"Number of tokens:\", row[\"num_tokens\"])\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa02f3f",
   "metadata": {},
   "source": [
    "## 4. Generating multilingual sentence embeddings\n",
    "\n",
    "Next, you will use a multilingual sentence transformer to obtain vector representations for all sentences.\n",
    "\n",
    "Steps.\n",
    "\n",
    "1. Create a list of all `(language, sentence)` pairs.  \n",
    "2. Encode each sentence into a high dimensional vector.  \n",
    "3. Reduce the vectors to two dimensions using PCA.  \n",
    "4. Visualize them in a scatter plot, colored by language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a unified dataset of sentences with language labels.\n",
    "all_rows = []\n",
    "for lang in languages:\n",
    "    code_ = lang[\"code\"]\n",
    "    name_ = lang[\"name\"]\n",
    "    for sent in example_sentences.get(code_, []):\n",
    "        all_rows.append({\n",
    "            \"language_code\": code_,\n",
    "            \"language_name\": name_,\n",
    "            \"sentence\": sent,\n",
    "        })\n",
    "\n",
    "sentence_df = pd.DataFrame(all_rows)\n",
    "sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60fc828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sentences into embeddings.\n",
    "\n",
    "sentences = sentence_df[\"sentence\"].tolist()\n",
    "embeddings = embedder.encode(sentences, convert_to_numpy=True)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to two dimensions using PCA for visualization.\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "sentence_df[\"dim1\"] = embeddings_2d[:, 0]\n",
    "sentence_df[\"dim2\"] = embeddings_2d[:, 1]\n",
    "\n",
    "sentence_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f98f6b",
   "metadata": {},
   "source": [
    "## 5. Visualizing sentence embeddings\n",
    "\n",
    "Run the cell below to see a scatter plot of sentences in the two dimensional PCA space, colored by language.\n",
    "\n",
    "You should look for patterns such as.\n",
    "\n",
    "- Do sentences from the same language cluster together.  \n",
    "- Are clusters for different languages clearly separated or overlapping.  \n",
    "- Does your low resource language behave similarly to high resource languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple scatter plot of sentence embeddings by language.\n",
    "\n",
    "plt.figure()\n",
    "for lang in languages:\n",
    "    code_ = lang[\"code\"]\n",
    "    subset = sentence_df[sentence_df[\"language_code\"] == code_]\n",
    "    plt.scatter(subset[\"dim1\"], subset[\"dim2\"], label=code_)\n",
    "\n",
    "for _, row in sentence_df.iterrows():\n",
    "    plt.text(row[\"dim1\"], row[\"dim2\"], row[\"language_code\"], fontsize=8)\n",
    "\n",
    "plt.xlabel(\"PCA dimension 1\")\n",
    "plt.ylabel(\"PCA dimension 2\")\n",
    "plt.title(\"Sentence embeddings projected to 2D\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef159b4c",
   "metadata": {},
   "source": [
    "If you have enough sentences, you can add more languages or replace PCA with UMAP for potentially clearer clusters.  \n",
    "For this short tutorial, PCA is usually sufficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba14b4",
   "metadata": {},
   "source": [
    "## 6. Observations and typical failure cases\n",
    "\n",
    "Use the questions below as a guide when you look at your tokenization and embedding results.\n",
    "\n",
    "### 6.1 Tokenization\n",
    "\n",
    "- Does your low resource language have many more tokens per sentence compared to English.  \n",
    "- Are there specific characters or letter combinations that are split in unexpected ways.  \n",
    "- Do named entities, technical terms, or local words get fragmented into many sub tokens.\n",
    "\n",
    "### 6.2 Embeddings\n",
    "\n",
    "- Do sentences in your language cluster tightly or are they scattered among other languages.  \n",
    "- If you use parallel or similar sentences across languages, do their embeddings appear close to each other.  \n",
    "- Are there sentences that you expect to be similar but that appear far apart in the embedding space.\n",
    "\n",
    "### 6.3 Typical failure cases\n",
    "\n",
    "- Words or phrases that are unknown, heavily fragmented, or mapped to unexpected parts of the embedding space.  \n",
    "- Mixing of scripts or code switching that confuses the tokenizer or encoder.  \n",
    "- Systematically worse behaviour for your language compared to high resource languages.\n",
    "\n",
    "You can use the markdown cell below to write down your notes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aea649",
   "metadata": {},
   "source": [
    "### 6.4 Notes\n",
    "\n",
    "Use this space to document your observations.  \n",
    "You can work in English or in your own language.\n",
    "\n",
    "- Tokenization observations.  \n",
    "- Embedding observations.  \n",
    "- Examples of typical failure cases.  \n",
    "- Ideas for how these issues might affect downstream tasks in your language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcff32a",
   "metadata": {},
   "source": [
    "## 7. Optional extensions\n",
    "\n",
    "If you have extra time, you can try one or more of the following extensions.\n",
    "\n",
    "1. **Add more models.**  \n",
    "   Compare tokenization and embeddings across different multilingual models, for example `xlm-roberta-large` versus a smaller model.\n",
    "\n",
    "2. **Parallel sentences.**  \n",
    "   Construct a small set of parallel sentences across languages and check whether embeddings for translations are close in the 2D plot.\n",
    "\n",
    "3. **Longer sentences.**  \n",
    "   Add longer and more complex sentences, for example from news or Wikipedia, and inspect how token counts and embeddings change.\n",
    "\n",
    "4. **Task specific prompts.**  \n",
    "   Use task oriented sentences, for example questions, instructions, or domain specific text, to see whether certain genres are better represented than others.\n",
    "\n",
    "These small experiments can help you decide which models and tokenizers might be more suitable for low resource applications in your own projects.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
