{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b48c569d",
      "metadata": {},
      "source": [
        "# Session 1: Foundations of Large Language Models ü§ñ\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/Session1_Foundations_of_Large_Language_Models.ipynb)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
        "\n",
        "**üìö Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "**Core Concepts:**\n",
        "- **LLM Architecture** - Understand transformer models and attention mechanisms\n",
        "- **Tokenization** - How models process and understand text across languages\n",
        "- **Text Representation** - Embeddings, vectors, and semantic similarity\n",
        "- **Model Comparison** - Analyze different LLM architectures and capabilities\n",
        "- **Low-Resource Considerations** - Challenges with underrepresented languages\n",
        "\n",
        "**Practical Skills:**\n",
        "- Compare tokenization across different models\n",
        "- Analyze model behavior with multilingual text\n",
        "- Implement basic text processing pipelines\n",
        "- Evaluate model performance on various languages\n",
        "- Build foundation for advanced NLP applications\n",
        "\n",
        "**Why This Matters:** Understanding LLM fundamentals is crucial for effective use in real-world applications, especially when working with diverse languages and limited computational resources.\n",
        "\n",
        "\n",
        "## Course Context\n",
        "\n",
        "| Session | Focus | Techniques | Prerequisites |\n",
        "|---------|-------|------------|---------------|\n",
        "| **Session 0** | Setup & Orientation | Environment, Basic Concepts | None |\n",
        "| **‚Üí This Session** | **LLM Foundations** | **Tokenization, Embeddings, Model Analysis** | **Session 0** |\n",
        "| **Session 2** | Prompt Engineering | Advanced Prompting, Chain-of-Thought | Sessions 0-1 |\n",
        "| **Session 3** | Fine-tuning | LoRA, QLoRA, Custom Training | Sessions 0-2 |\n",
        "| **Session 4** | Bias & Ethics | Fairness, Evaluation, Mitigation | Sessions 0-3 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup_section",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Environment Setup\n",
        "\n",
        "### What This Section Does\n",
        "This section prepares your coding environment with all necessary libraries for exploring Large Language Model foundations. We'll install packages optimized for **interactive learning** - educational, efficient, and GPU-optional!\n",
        "\n",
        "### Why These Specific Packages?\n",
        "\n",
        "**Core Dependencies:**\n",
        "- `numpy` + `pandas`: Essential for data manipulation and analysis\n",
        "- `scikit-learn`: Similarity metrics and basic ML utilities\n",
        "- `matplotlib`: Visualization of model behaviors and comparisons\n",
        "\n",
        "**LLM Ecosystem:**\n",
        "- `transformers`: Access to pretrained models and tokenizers\n",
        "- `sentence-transformers`: Semantic embeddings and similarity\n",
        "- `torch`: PyTorch backend for model operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install_packages",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick setup for this session\n",
        "!pip install -q transformers sentence-transformers scikit-learn matplotlib pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports_setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports for LLM foundations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"‚úÖ Environment ready for LLM foundations exploration!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tokenization_chapter",
      "metadata": {},
      "source": [
        "# Chapter 1: Understanding Tokenization\n",
        "\n",
        "## What We'll Explore\n",
        "\n",
        "Tokenization is how models convert text into numbers they can process. Let's see how this works with different languages and models.\n",
        "\n",
        "### Step 1: Prepare Test Sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6f525b6",
      "metadata": {},
      "source": [
        "**Model Selection:** We'll compare two popular multilingual models from [Hugging Face Hub](https://huggingface.co/models):\n",
        "\n",
        "- **BERT** (Google): Bidirectional Encoder Representations from Transformers - one of the first successful transformer models\n",
        "- **XLM-RoBERTa** (Facebook): Cross-lingual Language Model based on RoBERTa - specifically designed for multilingual tasks\n",
        "\n",
        "These model names are the official identifiers used to download them from Hugging Face's model repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test_sentences",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Multilingual Test Corpus Definition\n",
        "\n",
        "This corpus contains semantically equivalent sentences across three languages \n",
        "representing different language families and resource levels:\n",
        "- English: Germanic, high-resource language\n",
        "- Luxembourgish: Germanic, low-resource language\n",
        "- French: Romance, high-resource language\n",
        "\n",
        "Domain: Medical/Healthcare (to test domain-specific tokenization)\n",
        "Semantic equivalence: All sentences convey the same meaning\n",
        "\n",
        "Research Question: \n",
        "    How do multilingual models handle typologically similar vs. different \n",
        "    languages with varying resource availability?\n",
        "\n",
        "Expected Findings (Hypothesis):\n",
        "    1. Resource Availability Effect:\n",
        "       - English & French (high-resource) ‚Üí Lower tokens-per-word ratio\n",
        "       - Luxembourgish (low-resource) ‚Üí Higher tokens-per-word ratio\n",
        "       - Reason: Models trained predominantly on high-resource languages learn\n",
        "                 better subword representations for those languages\n",
        "    \n",
        "    2. Typological Similarity:\n",
        "       - English ‚Üî Luxembourgish (both Germanic): May show some overlap in \n",
        "         tokenization patterns despite resource difference\n",
        "       - French (Romance) vs. Germanic languages: Different morphological \n",
        "         patterns may lead to different tokenization strategies\n",
        "    \n",
        "    3. Model Architecture Differences:\n",
        "       - BERT: Trained on fewer languages, may show stronger resource bias\n",
        "       - XLM-RoBERTa: Trained on 100 languages, may handle low-resource \n",
        "         languages more efficiently\n",
        "\n",
        "Practical Implications:\n",
        "    If Luxembourgish requires 2-3x more tokens than English:\n",
        "    ‚Üí Processing costs increase proportionally\n",
        "    ‚Üí Context window fills up faster (fewer words fit in same token budget)\n",
        "    ‚Üí Inference latency increases\n",
        "    ‚Üí This quantifies the \"low-resource penalty\" in production systems\n",
        "\n",
        "Note: You may substitute these examples with sentences from your target language\n",
        "      and domain for comparative analysis.\n",
        "\"\"\"\n",
        "\n",
        "# Multilingual test corpus\n",
        "test_sentences = {\n",
        "    \"English\": \"The doctor explains the diagnosis carefully to the patient.\",\n",
        "    \"Luxembourgish\": \"Den Dokter erkl√§ert d'Diagnos ganz roueg dem Patient.\",\n",
        "    \"French\": \"Le m√©decin explique le diagnostic avec soin au patient.\"\n",
        "}\n",
        "\n",
        "# Display corpus for verification\n",
        "print(\"=\" * 70)\n",
        "print(\"MULTILINGUAL TEST CORPUS\")\n",
        "print(\"=\" * 70)\n",
        "for language, sentence in test_sentences.items():\n",
        "    word_count = len(sentence.split())\n",
        "    char_count = len(sentence)\n",
        "    print(f\"\\n{language:15} | Words: {word_count:2d} | Characters: {char_count:3d}\")\n",
        "    print(f\"{'':15} | {sentence}\")\n",
        "print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tokenization_analysis",
      "metadata": {},
      "source": [
        "### Step 2: Compare Tokenization Across Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "compare_tokenization",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMPREHENSIVE TOKENIZATION COMPARISON ACROSS MODEL ARCHITECTURES\n",
        "# ============================================================================\n",
        "# These models represent different tokenization algorithms and training approaches:\n",
        "\n",
        "models_to_compare = [\n",
        "    \"bert-base-multilingual-cased\",        # WordPiece tokenization, multilingual\n",
        "    \"xlm-roberta-base\",                    # SentencePiece tokenization, multilingual  \n",
        "    \"google/mt5-small\",                    # SentencePiece, multilingual encoder-decoder\n",
        "    \"gpt2\",                                # BPE (Byte-Pair Encoding), English-focused\n",
        "]\n",
        "\n",
        "# Test sentences from our corpus\n",
        "text_en = \"Students are learning about large language models.\"\n",
        "text_lr = \"D'Studenten l√©ieren iwwer grouss Sproochmodeller.\"  # Luxembourgish\n",
        "\n",
        "def show_tokenization(model_name, text):\n",
        "    \"\"\"\n",
        "    Display detailed tokenization analysis for a given model and text.\n",
        "    \n",
        "    Args:\n",
        "        model_name (str): HuggingFace model identifier\n",
        "        text (str): Input text to tokenize\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        \n",
        "        # Get tokenization algorithm info\n",
        "        tokenizer_type = type(tokenizer).__name__\n",
        "        \n",
        "        print(f\"\\nModel: {model_name}\")\n",
        "        print(f\"Algorithm: {tokenizer_type}\")\n",
        "        print(f\"Text: {text}\")\n",
        "        print(f\"Tokens: {tokens}\")\n",
        "        print(f\"Token count: {len(tokens)} | Word count: {len(text.split())} | Ratio: {len(tokens)/len(text.split()):.2f}\")\n",
        "        \n",
        "        return {\n",
        "            'model': model_name,\n",
        "            'tokenizer_type': tokenizer_type,\n",
        "            'text': text,\n",
        "            'token_count': len(tokens),\n",
        "            'word_count': len(text.split()),\n",
        "            'ratio': len(tokens)/len(text.split()),\n",
        "            'tokens': tokens\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"\\nModel: {model_name}\")\n",
        "        print(f\"‚ùå Error loading model: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "print(\"üîç COMPREHENSIVE TOKENIZATION ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nüî§ ENGLISH TEXT ANALYSIS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "english_results = []\n",
        "for model_name in models_to_compare:\n",
        "    result = show_tokenization(model_name, text_en)\n",
        "    if result:\n",
        "        english_results.append(result)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üåç LOW-RESOURCE LANGUAGE ANALYSIS (Luxembourgish)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "lr_results = []\n",
        "for model_name in models_to_compare:\n",
        "    result = show_tokenization(model_name, text_lr)\n",
        "    if result:\n",
        "        lr_results.append(result)\n",
        "\n",
        "# Summary comparison\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä EFFICIENCY COMPARISON SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if english_results and lr_results:\n",
        "    print(f\"{'Model':<25} | {'English Ratio':<14} | {'Luxembourgish Ratio':<18} | {'Efficiency Gap':<12}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for en_result in english_results:\n",
        "        # Find corresponding LR result\n",
        "        lr_result = next((lr for lr in lr_results if lr['model'] == en_result['model']), None)\n",
        "        if lr_result:\n",
        "            efficiency_gap = lr_result['ratio'] / en_result['ratio']\n",
        "            model_short = en_result['model'].split('/')[-1][:24]\n",
        "            print(f\"{model_short:<25} | {en_result['ratio']:<14.2f} | {lr_result['ratio']:<18.2f} | {efficiency_gap:<12.2f}x\")\n",
        "\n",
        "print(f\"\\nüí° Key Insight: Higher ratios indicate less efficient tokenization\")\n",
        "print(f\"   ‚Üí Low-resource languages often require 2-3x more tokens than English\")\n",
        "print(f\"   ‚Üí This directly impacts inference costs and processing speed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tokenization_reflection",
      "metadata": {},
      "source": [
        "### ü§î Reflection Questions\n",
        "\n",
        "Look at the results above and consider:\n",
        "\n",
        "- Which language uses more tokens per word?\n",
        "- How might more tokens affect inference cost and speed?\n",
        "- Do you see any unusual token splits (broken words, weird subwords)?\n",
        "\n",
        "**Key Insight:** Languages with fewer training examples often get split into more subword tokens, increasing computational costs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "embeddings_chapter",
      "metadata": {},
      "source": [
        "# üìä Chapter 2: Text Embeddings & Semantic Similarity\n",
        "\n",
        "## Understanding Vector Representations\n",
        "\n",
        "**What are embeddings?** Numbers that capture the meaning of text in high-dimensional space.\n",
        "\n",
        "Let's see how different models create these representations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sentence_embeddings",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a multilingual sentence embedding model\n",
        "embedder_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "embedder = SentenceTransformer(embedder_name, device=device)\n",
        "\n",
        "print(f\"üìä Loaded embedding model: {embedder_name}\")\n",
        "\n",
        "# Get embeddings for our test sentences\n",
        "sentences = list(test_sentences.values())\n",
        "languages = list(test_sentences.keys())\n",
        "\n",
        "embeddings = embedder.encode(sentences, convert_to_numpy=True)\n",
        "print(f\"‚úÖ Created embeddings with shape: {embeddings.shape}\")\n",
        "print(f\"   Each sentence ‚Üí {embeddings.shape[1]} dimensional vector\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "visualize_embeddings",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce to 2D for visualization\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "coords_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Create visualization\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "for i, (lang, sentence) in enumerate(test_sentences.items()):\n",
        "    plt.scatter(coords_2d[i, 0], coords_2d[i, 1], \n",
        "               c=colors[i], s=200, alpha=0.7, label=lang)\n",
        "    plt.annotate(lang, (coords_2d[i, 0], coords_2d[i, 1]), \n",
        "                xytext=(10, 10), textcoords='offset points', fontsize=12)\n",
        "\n",
        "plt.title(\"Sentence Embeddings in 2D Space\\n(All sentences have similar meaning)\", fontsize=14)\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üí° Key Observation: Similar-meaning sentences in different languages should cluster together!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "similarity_analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate semantic similarities\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "print(\"üîç SEMANTIC SIMILARITY ANALYSIS\")\n",
        "print(\"\\nSimilarity Matrix (1.0 = identical, 0.0 = unrelated):\")\n",
        "print()\n",
        "\n",
        "# Create a nice formatted table\n",
        "lang_names = list(test_sentences.keys())\n",
        "print(f\"{'Language':<12} \", end=\"\")\n",
        "for lang in lang_names:\n",
        "    print(f\"{lang:<10}\", end=\"\")\n",
        "print()\n",
        "\n",
        "for i, lang1 in enumerate(lang_names):\n",
        "    print(f\"{lang1:<12} \", end=\"\")\n",
        "    for j, lang2 in enumerate(lang_names):\n",
        "        sim = similarity_matrix[i, j]\n",
        "        print(f\"{sim:.3f}     \", end=\"\")\n",
        "    print()\n",
        "\n",
        "print(f\"\\nüí° Cross-lingual similarities (excluding self-comparisons):\")\n",
        "for i, lang1 in enumerate(lang_names):\n",
        "    for j, lang2 in enumerate(lang_names):\n",
        "        if i < j:  # Avoid duplicates\n",
        "            sim = similarity_matrix[i, j]\n",
        "            print(f\"   {lang1} ‚Üî {lang2}: {sim:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model_comparison",
      "metadata": {},
      "source": [
        "# Chapter 3: Model Comparison Summary\n",
        "\n",
        "Let's summarize what we've learned about different models and languages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create_summary",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a summary of our analysis\n",
        "summary_df = df_results.pivot_table(\n",
        "    index='language', \n",
        "    columns='model', \n",
        "    values=['tokens_per_word', 'num_tokens'], \n",
        "    aggfunc='mean'\n",
        ").round(2)\n",
        "\n",
        "print(\"üìä TOKENIZATION EFFICIENCY SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\nTokens per word (lower = more efficient):\")\n",
        "print(summary_df['tokens_per_word'])\n",
        "\n",
        "print(\"\\nTotal tokens per sentence:\")\n",
        "print(summary_df['num_tokens'])\n",
        "\n",
        "# Find the most efficient model for each language\n",
        "print(\"\\nüèÜ RECOMMENDATIONS:\")\n",
        "for lang in test_sentences.keys():\n",
        "    lang_data = df_results[df_results['language'] == lang]\n",
        "    best_model = lang_data.loc[lang_data['tokens_per_word'].idxmin(), 'model']\n",
        "    best_ratio = lang_data['tokens_per_word'].min()\n",
        "    print(f\"   {lang:12}: Best model is {best_model} (ratio: {best_ratio:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
        "# üéì Session 1 Complete: LLM Foundations Mastered!\n",
        "\n",
        "## üéØ What You've Learned\n",
        "\n",
        "Congratulations! You've explored the core foundations of Large Language Models:\n",
        "\n",
        "- ‚úÖ **Tokenization**: How models convert text into processable tokens\n",
        "- ‚úÖ **Cross-lingual Analysis**: Understanding language differences in model processing  \n",
        "- ‚úÖ **Text Embeddings**: Converting text to meaningful vector representations\n",
        "- ‚úÖ **Model Comparison**: Evaluating different architectures for your needs\n",
        "- ‚úÖ **Practical Skills**: Analyzing tokenization quality and embedding behavior\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Optional: Try It Yourself - Dialogue Summarization\n",
        "\n",
        "*Want to apply these concepts? Try creating your own dialogue summarization system using the foundations you've learned:*\n",
        "\n",
        "1. **Choose your own dialogue data** (conversations, meetings, chat logs)\n",
        "2. **Apply tokenization analysis** to understand processing costs\n",
        "3. **Use embeddings** to find similar conversation segments  \n",
        "4. **Compare models** for your specific language/domain\n",
        "5. **Implement TextRank** for extractive summarization (research the algorithm!)\n",
        "\n",
        "*This makes great homework or project work to deepen your understanding!*\n",
        "\n",
        "### üéØ Your Toolkit for Future Projects\n",
        "\n",
        "```python\n",
        "# Core functions you can reuse:\n",
        "analyze_tokenization(text, model_name)    # Compare tokenization efficiency\n",
        "embedder.encode(sentences)                # Create semantic embeddings\n",
        "cosine_similarity(embeddings)            # Measure text similarity\n",
        "```\n",
        "\n",
        "**üåü Achievement Unlocked: LLM Foundations Expert! üíé**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
