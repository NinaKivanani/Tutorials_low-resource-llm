{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session 1: Foundations of Large Language Models ğŸ¤–\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "**ğŸ§  Understanding the Building Blocks of Modern AI**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/Session1_Foundations_of_Large_Language_Models.ipynb)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
        "\n",
        "**ğŸ“š Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ What You'll Master Today\n",
        "\n",
        "**Core Concepts:**\n",
        "- ğŸ§  **LLM Architecture** - Understand transformer models and attention mechanisms\n",
        "- ğŸ”¤ **Tokenization** - How models process and understand text across languages\n",
        "- ğŸ“Š **Text Representation** - Embeddings, vectors, and semantic similarity\n",
        "- âš™ï¸ **Model Comparison** - Analyze different LLM architectures and capabilities\n",
        "- ğŸ›¡ï¸ **Low-Resource Considerations** - Challenges with underrepresented languages\n",
        "\n",
        "**Practical Skills:**\n",
        "- Compare tokenization across different models\n",
        "- Analyze model behavior with multilingual text\n",
        "- Implement basic text processing pipelines\n",
        "- Evaluate model performance on various languages\n",
        "- Build foundation for advanced NLP applications\n",
        "\n",
        "**Why This Matters:** Understanding LLM fundamentals is crucial for effective use in real-world applications, especially when working with diverse languages and limited computational resources.\n",
        "\n",
        "## ğŸ“Š Session Roadmap & Learning Path\n",
        "\n",
        "```\n",
        "ğŸ—ºï¸ Your Learning Journey:\n",
        "â”œâ”€â”€ ğŸ› ï¸  Environment Setup           [â–¡â–¡â–¡â–¡â–¡] \n",
        "â”œâ”€â”€ ğŸ§  LLM Architecture Overview    [â–¡â–¡â–¡â–¡â–¡] \n",
        "â”œâ”€â”€ ğŸ”¤ Tokenization Deep Dive       [â–¡â–¡â–¡â–¡â–¡] \n",
        "â”œâ”€â”€ ğŸ“Š Text Embeddings & Similarity [â–¡â–¡â–¡â–¡â–¡] \n",
        "â”œâ”€â”€ ğŸ” Multilingual Model Analysis  [â–¡â–¡â–¡â–¡â–¡] \n",
        "â”œâ”€â”€ ğŸ›¡ï¸ Low-Resource Considerations  [â–¡â–¡â–¡â–¡â–¡] \n",
        "â””â”€â”€ ğŸ¯ Hands-on Applications       [â–¡â–¡â–¡â–¡â–¡] \n",
        "```\n",
        "\n",
        "### ğŸ† Achievement Milestones\n",
        "Unlock achievements as you progress:\n",
        "\n",
        "| Achievement | Description | Skills Gained |\n",
        "|-------------|-------------|---------------|\n",
        "| ğŸ¥‰ **Setup Master** | Complete environment setup | Dependency management |\n",
        "| ğŸ§  **Architecture Explorer** | Understand transformer models | Neural network foundations |\n",
        "| ğŸ”¤ **Tokenization Expert** | Master text preprocessing | Cross-lingual text processing |\n",
        "| ğŸ“Š **Embedding Analyst** | Work with vector representations | Semantic similarity analysis |\n",
        "| ğŸ” **Model Comparator** | Evaluate different LLMs | Model selection expertise |\n",
        "| ğŸ›¡ï¸ **Low-Resource Strategist** | Handle language limitations | Inclusive NLP approaches |\n",
        "| ğŸ’ **Foundation Builder** | Apply LLM knowledge practically | Real-world implementation |\n",
        "\n",
        "### ğŸ¯ Interactive Learning Elements\n",
        "- **ğŸ” Checkpoints**: Reflection points to consolidate learning\n",
        "- **ğŸ¯ Challenges**: Hands-on exercises with your own data\n",
        "- **ğŸ“Š Visualizations**: See algorithms in action\n",
        "- **ğŸ¨ Customization**: Adapt examples to your target language\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ Session Focus & Approach\n",
        "\n",
        "**Why Start with LLM Foundations?**\n",
        "- âœ… Essential for effective model selection and use\n",
        "- âœ… Understand capabilities and limitations\n",
        "- âœ… Critical for low-resource language applications\n",
        "- âœ… Foundation for advanced techniques in later sessions\n",
        "- âœ… Enables informed decision-making in real projects\n",
        "\n",
        "**Session Architecture:**\n",
        "```\n",
        "ğŸ—ï¸ Progressive Understanding Approach:\n",
        "Theory â†’ Exploration â†’ Analysis â†’ Comparison â†’ Application\n",
        "```\n",
        "\n",
        "## ğŸ“š Course Context\n",
        "\n",
        "| Session | Focus | Techniques | Prerequisites |\n",
        "|---------|-------|------------|---------------|\n",
        "| **Session 0** | Setup & Basics | Tokenization, Environment | None |\n",
        "| **â†’ This Session** | **Extractive Methods** | **TextRank, Graph Algorithms** | **Session 0** |\n",
        "| **Session 2** | LLM Prompting | GPT, Claude, Prompt Engineering | Sessions 0-1 |\n",
        "| **Session 3** | Fine-tuning | LoRA, QLoRA, Custom Training | Sessions 0-2 |\n",
        "| **Session 4** | Bias & Ethics | Fairness, Evaluation, Mitigation | Sessions 0-3 |\n",
        "\n",
        "## ğŸ› ï¸ What You'll Build Today\n",
        "\n",
        "| Component | Description | Skills Developed |\n",
        "|-----------|-------------|------------------|\n",
        "| ğŸ”¬ **Model Explorer** | Compare different LLM architectures | Model analysis and selection |\n",
        "| ğŸ”¤ **Tokenization Lab** | Analyze text processing across models | Cross-lingual preprocessing |\n",
        "| ğŸ“Š **Embedding Analyzer** | Explore text representations | Vector space understanding |\n",
        "| ğŸ” **Performance Comparator** | Evaluate model capabilities | Systematic evaluation |\n",
        "| ğŸ›¡ï¸ **Low-Resource Toolkit** | Address language coverage gaps | Inclusive NLP strategies |\n",
        "\n",
        "## ğŸ¯ Learning Objectives\n",
        "\n",
        "**By the end of this session, you will confidently:**\n",
        "\n",
        "| Skill Category | Specific Abilities |\n",
        "|----------------|-------------------|\n",
        "| ğŸ§  **Architecture Understanding** | â€¢ Explain transformer model components<br>â€¢ Compare different LLM architectures<br>â€¢ Understand attention mechanisms |\n",
        "| ğŸ”¤ **Tokenization Mastery** | â€¢ Analyze tokenization across models<br>â€¢ Handle multilingual text processing<br>â€¢ Identify tokenization challenges |\n",
        "| ğŸ“Š **Text Representation** | â€¢ Work with embeddings and vectors<br>â€¢ Measure semantic similarity<br>â€¢ Visualize text in vector space |\n",
        "| ğŸ” **Model Analysis** | â€¢ Compare model performance<br>â€¢ Evaluate multilingual capabilities<br>â€¢ Select appropriate models for tasks |\n",
        "| ğŸ›¡ï¸ **Low-Resource Awareness** | â€¢ Identify language coverage gaps<br>â€¢ Address underrepresented languages<br>â€¢ Design inclusive NLP solutions |\n",
        "\n",
        "## ğŸ“– How to Navigate This Notebook\n",
        "\n",
        "### ğŸ¯ **System Requirements**\n",
        "- **Minimum:** CPU-only environment (Google Colab free tier)\n",
        "- **Recommended:** GPU for faster processing (Colab Pro)\n",
        "- **Time:** 2-3 hours for complete walkthrough\n",
        "\n",
        "### ğŸ—ºï¸ **Navigation Guide**\n",
        "| Symbol | Meaning | Action |\n",
        "|--------|---------|---------|\n",
        "| ğŸ” | **Checkpoint** | Stop, reflect, and consolidate learning |\n",
        "| ğŸ¯ | **Challenge** | Optional hands-on exercise |\n",
        "| ğŸ“Š | **Visualization** | Interactive plots and analysis |\n",
        "| ğŸ’¡ | **Key Insight** | Important concept to remember |\n",
        "| âš ï¸ | **Common Pitfall** | Watch out for this issue |\n",
        "\n",
        "### ğŸš€ **Getting Started**\n",
        "1. **Run cells sequentially** - dependencies build on each other\n",
        "2. **Customize examples** - replace with your target language\n",
        "3. **Experiment freely** - most operations are deterministic\n",
        "4. **If stuck** - restart runtime and re-run from the beginning\n",
        ""
      ],
      "id": "b48c569d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ› ï¸ Section 0: Environment Setup\n",
        "\n",
        "### ğŸ“‹ What This Section Does\n",
        "This section prepares your coding environment with all necessary libraries for dialogue summarization. We'll install packages optimized for **low-resource scenarios** - lightweight, efficient, and CPU-friendly!\n",
        "\n",
        "### ğŸ¯ Learning Goals\n",
        "- âœ… Understand dependency management for NLP projects  \n",
        "- âœ… Learn about package compatibility in different environments\n",
        "- âœ… Set up reproducible research environments\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Š Setup Progress Tracker\n",
        "```\n",
        "ğŸ› ï¸ Setup Progress:\n",
        "â”œâ”€â”€ ğŸ“¦ Core Dependencies      [â–¡â–¡â–¡â–¡â–¡] 0%\n",
        "â”œâ”€â”€ ğŸ¤— NLP Libraries         [â–¡â–¡â–¡â–¡â–¡] 0%  \n",
        "â”œâ”€â”€ ğŸ“ˆ Evaluation Tools      [â–¡â–¡â–¡â–¡â–¡] 0%\n",
        "â”œâ”€â”€ ğŸ¨ Visualization         [â–¡â–¡â–¡â–¡â–¡] 0%\n",
        "â””â”€â”€ âœ… Verification          [â–¡â–¡â–¡â–¡â–¡] 0%\n",
        "```\n",
        "\n",
        "### ğŸ’¡ Why These Specific Packages?\n",
        "\n",
        "**Core Dependencies:**\n",
        "- `numpy<2` + `pandas`: Data manipulation (compatible versions)\n",
        "- `scikit-learn`: TF-IDF vectorization and similarity metrics\n",
        "- `networkx`: Graph algorithms for TextRank\n",
        "\n",
        "**NLP Ecosystem:**\n",
        "- `transformers`: Access to multilingual tokenizers\n",
        "- `sentence-transformers`: Semantic similarity analysis\n",
        "- `rouge-score`: Standard evaluation metrics\n",
        "\n",
        "**Interactive Elements:**\n",
        "- `ipywidgets`: Interactive sliders and buttons\n",
        "- `matplotlib`: Data visualization\n",
        "\n",
        "---\n",
        "\n",
        "### Option A. ğŸš€ Quick Setup (Recommended)\n",
        "\n",
        "**âš¡ What the next cell does:** Installs minimal dependencies with compatibility fixes for common environments like Google Colab.\n",
        "\n",
        "### Option B. ğŸ Conda Environment (Advanced)\n",
        "\n",
        "**ğŸ—ï¸ For isolated development environments:**\n",
        "\n",
        "```bash\n",
        "conda create -n dialogue-sum python=3.9 -y\n",
        "conda activate dialogue-sum\n",
        "pip install -U pip\n",
        "pip install \"numpy<2\" pandas scikit-learn networkx matplotlib tqdm\n",
        "pip install \"transformers==4.49.0\" \"datasets==3.2.0\" \"accelerate>=0.25.0\" sentencepiece\n",
        "pip install rouge-score evaluate\n",
        "pip install ipywidgets\n",
        "```\n",
        "\n",
        "**ğŸ’¡ Pro Tip:** On managed clusters, you may need to load CUDA modules before PyTorch installation.\n"
      ],
      "id": "506e85f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Simple version of the next cell:\n",
        "# Minimal dependencies for this session.\n",
        "# In Colab this cell may take a minute the first time.\n",
        "\n",
        "pip install -q transformers sentence-transformers umap-learn\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7b80e31b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "ğŸ“¦ INSTALLATION CELL\n",
        "\n",
        "ğŸ¯ WHAT THIS CELL DOES:\n",
        "Installs all necessary Python packages for dialogue summarization with:\n",
        "- Compatibility fixes for Google Colab\n",
        "- Version management to avoid conflicts  \n",
        "- Verification of successful installation\n",
        "- Interactive progress tracking\n",
        "\n",
        "â±ï¸ EXPECTED TIME: 2-3 minutes\n",
        "ğŸ’¾ DISK SPACE: ~500MB for all packages\n",
        "ğŸ”§ REQUIREMENTS: Internet connection\n",
        "\"\"\"\n",
        "\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install_colab_safe(pkgs):\n",
        "    \"\"\"Install packages with Colab-safe dependency handling\"\"\"\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\"] + pkgs\n",
        "    print(\"Installing:\", \" \".join(pkgs))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "def print_progress_bar(progress, total, bar_length=40):\n",
        "    \"\"\"Visual progress bar for installation steps\"\"\"\n",
        "    percent = progress / total\n",
        "    filled_length = int(bar_length * percent)\n",
        "    bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)\n",
        "    print(f\"\\rğŸš€ Progress: [{bar}] {percent:.0%} ({progress}/{total})\", end='')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ® GAMIFIED INSTALLATION FOR SESSION 1\")\n",
        "print(\"=\"*70)\n",
        "print(\"â±ï¸  Estimated time: 2-3 minutes\")\n",
        "print(\"ğŸ¯ Installing packages for low-resource dialogue summarization...\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    # Step 1: Fix numpy/pandas compatibility (common Colab issue)\n",
        "    print(\"ğŸ”§ Step 1/5: Fixing numpy/pandas compatibility...\")\n",
        "    print_progress_bar(1, 5)\n",
        "    print(\"\\nğŸ“¦ Core Dependencies      [â– â– â– â– â– ] 100%\")\n",
        "    pip_install_colab_safe([\"numpy==1.24.3\", \"pandas==2.0.3\"])\n",
        "    \n",
        "    # Step 2: Core data science packages\n",
        "    print(\"\\nğŸ“Š Step 2/5: Installing core ML packages...\")\n",
        "    print_progress_bar(2, 5)\n",
        "    print(\"\\nğŸ¤— NLP Libraries         [â– â– â– â– â– ] 100%\")\n",
        "    pip_install_colab_safe([\n",
        "        \"scikit-learn\",\n",
        "        \"networkx\", \n",
        "        \"matplotlib\",\n",
        "        \"tqdm\",\n",
        "        \"rouge-score\"  # For ROUGE evaluation metrics\n",
        "    ])\n",
        "    \n",
        "    # Step 3: NLP packages (with compatible versions)\n",
        "    print(\"\\nğŸ¤— Step 3/5: Installing NLP packages...\")\n",
        "    print_progress_bar(3, 5)\n",
        "    print(\"\\nğŸ“ˆ Evaluation Tools      [â– â– â– â– â– ] 100%\")\n",
        "    pip_install_colab_safe([\n",
        "        \"transformers==4.35.0\",  # Compatible with numpy 1.24\n",
        "        \"datasets==2.14.0\",      # Compatible with pandas 2.0\n",
        "        \"accelerate==0.23.0\",\n",
        "        \"sentencepiece\",\n",
        "        \"sentence-transformers==2.2.2\"  # For embeddings\n",
        "    ])\n",
        "    \n",
        "    # Step 4: Interactive widgets\n",
        "    print(\"\\nğŸ¨ Step 4/5: Installing interactive components...\")\n",
        "    print_progress_bar(4, 5)\n",
        "    print(\"\\nğŸ¨ Visualization         [â– â– â– â– â– ] 100%\")\n",
        "    try:\n",
        "        import ipywidgets\n",
        "    except ImportError:\n",
        "        pip_install_colab_safe([\"ipywidgets\"])\n",
        "    \n",
        "    # Step 5: Verification\n",
        "    print(\"\\nâœ… Step 5/5: Verifying installation...\")\n",
        "    print_progress_bar(5, 5)\n",
        "    print(\"\\nâœ… Verification          [â– â– â– â– â– ] 100%\")\n",
        "    \n",
        "    print()\n",
        "    print(\"=\"*70)\n",
        "    print(\"ğŸ† ACHIEVEMENT UNLOCKED: Setup Master! ğŸ¥‰\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"ğŸ‰ All packages installed successfully!\")\n",
        "    print(\"ğŸ’¡ If you see warnings above, that's normal in Colab\")\n",
        "    \n",
        "    # Verify key packages are available\n",
        "    print(\"\\nğŸ” FINAL VERIFICATION...\")\n",
        "    import importlib\n",
        "    key_packages = {\n",
        "        \"rouge_score\": \"ROUGE metrics\",\n",
        "        \"networkx\": \"Graph algorithms\", \n",
        "        \"sklearn\": \"Machine learning\",\n",
        "        \"transformers\": \"Language models\",\n",
        "        \"sentence_transformers\": \"Semantic embeddings\"\n",
        "    }\n",
        "    \n",
        "    print(\"Package Status Report:\")\n",
        "    all_good = True\n",
        "    for pkg, description in key_packages.items():\n",
        "        try:\n",
        "            importlib.import_module(pkg)\n",
        "            print(f\"âœ… {pkg:<20} | {description}\")\n",
        "        except ImportError:\n",
        "            print(f\"âŒ {pkg:<20} | MISSING - install with: pip install {pkg.replace('_', '-')}\")\n",
        "            all_good = False\n",
        "    \n",
        "    if all_good:\n",
        "        print(\"\\nğŸš€ READY TO START THE TUTORIAL!\")\n",
        "        print(\"ğŸ“Š Progress Update:\")\n",
        "        print(\"â”œâ”€â”€ ğŸ› ï¸  Setup & Dependencies      [â– â– â– â– â– ] 100% âœ…\")\n",
        "        print(\"â”œâ”€â”€ ğŸ” Language Analysis          [â–¡â–¡â–¡â–¡â–¡] 0%\")\n",
        "        print(\"â”œâ”€â”€ ğŸ“ Dialogue Data Creation     [â–¡â–¡â–¡â–¡â–¡] 0%\")  \n",
        "        print(\"â”œâ”€â”€ âš™ï¸  TextRank Implementation   [â–¡â–¡â–¡â–¡â–¡] 0%\")\n",
        "        print(\"â”œâ”€â”€ ğŸŒ Low-Resource Adaptation    [â–¡â–¡â–¡â–¡â–¡] 0%\")\n",
        "        print(\"â””â”€â”€ ğŸ¯ Final Challenge           [â–¡â–¡â–¡â–¡â–¡] 0%\")\n",
        "    else:\n",
        "        print(\"\\nâš ï¸  Some packages missing - see troubleshooting cell below\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print()\n",
        "    print(\"=\"*70)\n",
        "    print(\"âš ï¸  INSTALLATION ISSUE\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Error:\", str(e))\n",
        "    print()\n",
        "    print(\"ğŸ”„ TROUBLESHOOTING STEPS:\")\n",
        "    print(\"1. Runtime â†’ Restart Runtime\")\n",
        "    print(\"2. Re-run this cell\")\n",
        "    print(\"3. If still stuck, try the manual troubleshooting cell below\")\n",
        "    print(\"4. Contact instructor if problems persist\")\n",
        "    print()\n",
        "    print(\"ğŸ’¡ You may be able to continue if packages were already installed\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f24fd6c6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸš¨ TROUBLESHOOTING: If you get \"ModuleNotFoundError: No module named 'rouge_score'\"\n",
        "# Run this cell to fix the issue:\n",
        "\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "    print(\"âœ… rouge_score is working correctly!\")\n",
        "except ImportError:\n",
        "    print(\"ğŸ”§ Installing rouge-score...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rouge-score\"])\n",
        "    print(\"âœ… rouge-score installed successfully!\")\n",
        "    \n",
        "    # Test the import again\n",
        "    try:\n",
        "        from rouge_score import rouge_scorer\n",
        "        print(\"âœ… rouge_score import now working!\")\n",
        "    except ImportError as e:\n",
        "        print(f\"âŒ Still having issues: {e}\")\n",
        "        print(\"ğŸ’¡ Try: Runtime â†’ Restart Runtime, then re-run all cells from the top\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "edea558f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸš¨ Common Issue: ModuleNotFoundError for rouge_score\n",
        "\n",
        "**If you see: `ModuleNotFoundError: No module named 'rouge_score'`**\n",
        "\n",
        "**Solution 1 (Recommended):** Run the troubleshooting cell above â˜ï¸\n",
        "\n",
        "**Solution 2 (Manual fix):**\n",
        "1. Run this command in a code cell: `!pip install rouge-score`\n",
        "2. Go to **Runtime â†’ Restart Runtime**\n",
        "3. Re-run all cells from the beginning\n",
        "\n",
        "**Solution 3 (If nothing else works):**\n",
        "1. **Runtime â†’ Restart Runtime** \n",
        "2. **Runtime â†’ Run All** (this will re-install everything)\n",
        "\n",
        "**Why this happens:** Sometimes package installation fails silently in Colab, or packages get overwritten by conflicting dependencies.\n",
        "\n",
        "---\n"
      ],
      "id": "7a63f16f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "ğŸ“š IMPORTS AND SETUP CELL\n",
        "\n",
        "ğŸ¯ WHAT THIS CELL DOES:\n",
        "- Imports all necessary Python libraries for dialogue summarization\n",
        "- Sets up random seeds for reproducible results  \n",
        "- Defines placeholder functions for Session 2 compatibility\n",
        "- Verifies all imports work correctly\n",
        "\n",
        "ğŸ”§ KEY LIBRARIES:\n",
        "- pandas/numpy: Data manipulation\n",
        "- transformers: Language model utilities  \n",
        "- sentence-transformers: Semantic embeddings\n",
        "- networkx: Graph algorithms (for TextRank)\n",
        "- rouge-score: Evaluation metrics\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# For tokenization and embedding analysis\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# Set seeds for reproducibility ğŸ²\n",
        "random.seed(842)\n",
        "np.random.seed(842)\n",
        "\n",
        "print(\"ğŸ¯ IMPORT SUCCESS!\")\n",
        "print(\"=\"*50)\n",
        "print(\"âœ… Core libraries: pandas, numpy, transformers\")\n",
        "print(\"âœ… NLP tools: sentence-transformers, rouge-score\")  \n",
        "print(\"âœ… Graph algorithms: networkx\")\n",
        "print(\"âœ… Visualization: matplotlib\")\n",
        "print(\"âœ… Random seeds set: 842 (reproducible results)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ğŸš¨ PLACEHOLDER SECTION: LLM functions moved to Session 2\n",
        "# This prevents NameError when running leftover LLM code sections\n",
        "def generate_summary_t5(*args, **kwargs):\n",
        "    \"\"\"Placeholder function - LLM functionality moved to Session 2\"\"\"\n",
        "    return \"[This functionality moved to Session 2: Pretrained Models and Prompt Engineering]\"\n",
        "\n",
        "# Session 1 focuses on non-LLM methods - no model loading needed!\n",
        "\n",
        "# Placeholder variables for compatibility with remaining LLM references\n",
        "ZERO_SHOT_PROMPT = \"[This functionality moved to Session 2: Prompt Engineering]\"\n",
        "\n",
        "print(\"ğŸ’¡ Session 1 Focus: Robust baseline methods that work without LLMs!\")\n",
        "print(\"ğŸ“š For advanced LLM techniques, see Session 2: Prompt Engineering\")\n",
        "print(\"\\nğŸ† Ready to proceed with language analysis! ğŸ¥ˆ\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b9611d62"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import random\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import torch\n",
        "\n",
        "# Deterministic behaviour for small demos\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e2f40a7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0 . Why do LLMs matter\n",
        "\n",
        "Very short version.\n",
        "\n",
        "- Traditional NLP used many task specific models and hand crafted features.\n",
        "- LLMs use **one large model** with a transformer backbone that is pre trained on massive text.\n",
        "- With **prompting**, the same model can:\n",
        "  - Translate.\n",
        "  - Summarize.\n",
        "  - Answer questions.\n",
        "  - Generate code.\n",
        "- This works because the model:\n",
        "  - Turns text into **tokens**.\n",
        "  - Converts tokens to **vectors** (embeddings).\n",
        "  - Processes sequences of vectors with a **transformer** (stack of attention layers).\n",
        "\n",
        "In this notebook we will not train a model. \n",
        "We will *look inside* existing multilingual models to understand what they do to our text.\n"
      ],
      "id": "26bc69fc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 . Define sentences in two languages\n",
        "\n",
        "We start with a few short sentences.\n",
        "\n",
        "- English (`en`).\n",
        "- One low resource language. For this example we use Luxembourgish (`lb`), but you can replace it.\n",
        "\n",
        "Edit the next cell if you want to switch to your own language.\n"
      ],
      "id": "e9d613bb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# You can change LOW_RESOURCE_LANG and the example sentences.\n",
        "\n",
        "LOW_RESOURCE_LANG = \"lb\"  # for example: \"lb\", \"hy\", \"fa\", etc.\n",
        "\n",
        "examples = [\n",
        "    # English\n",
        "    {\"id\": \"en_1\", \"lang\": \"en\", \"text\": \"The doctor explains the diagnosis carefully.\"},\n",
        "    {\"id\": \"en_2\", \"lang\": \"en\", \"text\": \"Students are learning about large language models.\"},\n",
        "    {\"id\": \"en_3\", \"lang\": \"en\", \"text\": \"The weather is cloudy today but we still go for a walk.\"},\n",
        "\n",
        "    # Luxembourgish (edit for your own language if you want)\n",
        "    {\"id\": f\"{LOW_RESOURCE_LANG}_1\", \"lang\": LOW_RESOURCE_LANG, \"text\": \"Den Dokter erklÃ¤ert d'Diagnos ganz roueg.\"},\n",
        "    {\"id\": f\"{LOW_RESOURCE_LANG}_2\", \"lang\": LOW_RESOURCE_LANG, \"text\": \"D'Studenten lÃ©ieren iwwer grouss Sproochmodeller.\"},\n",
        "    {\"id\": f\"{LOW_RESOURCE_LANG}_3\", \"lang\": LOW_RESOURCE_LANG, \"text\": \"D'Wieder ass haut wollekeg, mee mir ginn trotzdeem spadsÃ©ieren.\"},\n",
        "]\n",
        "\n",
        "for ex in examples:\n",
        "    print(f\"[{ex['lang']}] {ex['id']}: {ex['text']}\")\n"
      ],
      "id": "990dcfdb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 . Tokenization\n",
        "\n",
        "Models do not see raw text. \n",
        "They see **tokens**. \n",
        "Tokens are usually subword units.\n",
        "\n",
        "We will.\n",
        "\n",
        "1. Load one multilingual tokenizer.\n",
        "2. See how it splits our sentences.\n",
        "3. Compare token counts for English vs the low resource language.\n",
        "\n",
        "Try later with different models, for example.\n",
        "\n",
        "- `\"bert-base-multilingual-cased\"`\n",
        "- `\"xlm-roberta-base\"`\n",
        "- `\"google/mt5-small\"`\n"
      ],
      "id": "9c3baf03"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "multilingual_model_name = \"xlm-roberta-base\"  # change if needed\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(multilingual_model_name)\n",
        "print(\"Loaded tokenizer:\", multilingual_model_name)\n",
        "\n",
        "def inspect_tokenization(examples, tokenizer, max_tokens_to_show=20):\n",
        "    rows = []\n",
        "    for ex in examples:\n",
        "        tokens = tokenizer.tokenize(ex[\"text\"])\n",
        "        rows.append(\n",
        "            {\n",
        "                \"id\": ex[\"id\"],\n",
        "                \"lang\": ex[\"lang\"],\n",
        "                \"text\": ex[\"text\"],\n",
        "                \"n_tokens\": len(tokens),\n",
        "                \"tokens_preview\": tokens[:max_tokens_to_show],\n",
        "            }\n",
        "        )\n",
        "    return rows\n",
        "\n",
        "rows = inspect_tokenization(examples, tokenizer)\n",
        "for row in rows:\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{row['id']}  lang={row['lang']}\")\n",
        "    print(\"Text:\", row[\"text\"])\n",
        "    print(\"Number of tokens:\", row[\"n_tokens\"])\n",
        "    print(\"First tokens:\", row[\"tokens_preview\"])\n"
      ],
      "id": "70d97e2d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_tokens = pd.DataFrame(rows)\n",
        "summary = df_tokens.groupby(\"lang\")[\"n_tokens\"].agg([\"mean\", \"min\", \"max\", \"count\"])\n",
        "summary\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "04d5931a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 . Reflection\n",
        "\n",
        "Questions to discuss in small groups or write in the notebook.\n",
        "\n",
        "- Do English sentences use more or fewer tokens than the low resource language?\n",
        "- Do you see strange splits, for example broken accents or weird subwords?\n",
        "- How might more tokens affect.\n",
        "  - Speed of inference.\n",
        "  - Context length.\n",
        "  - Training cost.\n",
        "\n",
        "Use the cell below as a scratch pad.\n"
      ],
      "id": "2664c9e1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reflection_tokenization = \"\"\"\n",
        "Write your observations here as plain text.\n",
        "For example:\n",
        "- In my language the tokenizer splits many words into small pieces.\n",
        "- Named entities look broken.\n",
        "- This might make long documents more expensive to process.\n",
        "\"\"\"\n",
        "\n",
        "print(reflection_tokenization)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "264d94ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 . Sentence embeddings\n",
        "\n",
        "A **sentence embedding** is a vector (for example 384 or 768 dimensions) that represents the meaning of a sentence.\n",
        "\n",
        "We will.\n",
        "\n",
        "1. Use a multilingual sentence embedding model.\n",
        "2. Convert each sentence to a vector.\n",
        "3. Reduce vectors to 2D with PCA.\n",
        "4. Plot them, colour coded by language.\n",
        "\n",
        "Model example.\n",
        "\n",
        "- `\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"`\n"
      ],
      "id": "7cd93bbc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "embedder_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "embedder = SentenceTransformer(embedder_name, device=device)\n",
        "print(\"Loaded sentence embedder:\", embedder_name)\n",
        "\n",
        "sent_texts = [ex[\"text\"] for ex in examples]\n",
        "sent_langs = [ex[\"lang\"] for ex in examples]\n",
        "sent_ids   = [ex[\"id\"] for ex in examples]\n",
        "\n",
        "embeddings = embedder.encode(sent_texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "print(\"Embedding shape:\", embeddings.shape)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "da1c8a47"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reduce to 2D with PCA for visualization\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "coords_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Build a small DataFrame for plotting\n",
        "df_plot = pd.DataFrame(\n",
        "    {\n",
        "        \"id\": sent_ids,\n",
        "        \"lang\": sent_langs,\n",
        "        \"text\": sent_texts,\n",
        "        \"x\": coords_2d[:, 0],\n",
        "        \"y\": coords_2d[:, 1],\n",
        "    }\n",
        ")\n",
        "\n",
        "df_plot\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a797b856"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "langs = sorted(df_plot[\"lang\"].unique())\n",
        "markers = [\"o\", \"s\", \"^\", \"D\", \"x\"]  # up to 5 languages easily\n",
        "marker_map = {lang: markers[i % len(markers)] for i, lang in enumerate(langs)}\n",
        "\n",
        "for lang in langs:\n",
        "    subset = df_plot[df_plot[\"lang\"] == lang]\n",
        "    plt.scatter(subset[\"x\"], subset[\"y\"], label=lang, marker=marker_map[lang])\n",
        "\n",
        "    # Add short labels\n",
        "    for _, row in subset.iterrows():\n",
        "        short_id = row[\"id\"]\n",
        "        plt.text(row[\"x\"] + 0.02, row[\"y\"] + 0.02, short_id, fontsize=9)\n",
        "\n",
        "plt.title(\"2D view of sentence embeddings\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b0e081dd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 . Reflection\n",
        "\n",
        "Look at the plot and discuss.\n",
        "\n",
        "- Do sentences cluster more by **language** or by **meaning**.\n",
        "- Are English and low resource sentences with similar meaning close in the plot.\n",
        "- Does any sentence look like an outlier.\n",
        "\n",
        "Again, use the next cell as a scratch pad.\n"
      ],
      "id": "577d66d4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reflection_embeddings = \"\"\"\n",
        "Notes about the 2D embedding plot.\n",
        "\n",
        "- Example: The two sentences about doctors are close, even across languages.\n",
        "- Example: One sentence in my language is far away, maybe the model does not know this vocabulary well.\n",
        "\"\"\"\n",
        "\n",
        "print(reflection_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "49004d79"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 . Inside a transformer (very briefly)\n",
        "\n",
        "We do not derive the full transformer math here. \n",
        "Instead we run a **forward pass** and inspect tensor shapes.\n",
        "\n",
        "We will.\n",
        "\n",
        "1. Use a small multilingual transformer (encoder only).\n",
        "2. Tokenize one sentence.\n",
        "3. See shapes of.\n",
        "   - `input_ids`.\n",
        "   - `attention_mask`.\n",
        "   - `last_hidden_state`.\n"
      ],
      "id": "0b8970ad"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 . Wrap up\n",
        "\n",
        "In this session you.\n",
        "\n",
        "- Learned that LLMs work through tokenization, embeddings, and transformers.\n",
        "- Saw how a multilingual tokenizer splits your language.\n",
        "- Measured token counts for English vs a low resource language.\n",
        "- Built sentence embeddings and visualized them in 2D.\n",
        "- Peeked inside a transformer to see tensor shapes.\n",
        "\n",
        "Suggested follow up tasks.\n",
        "\n",
        "- Try a different multilingual model and repeat the tokenization inspection.\n",
        "- Replace the example sentences with your own data.\n",
        "- Add more languages and see how they cluster.\n",
        "- Connect this notebook to downstream tasks in later sessions (summarization, QA, etc.).\n"
      ],
      "id": "50454452"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "small_model_name = \"distilbert-base-multilingual-cased\"\n",
        "\n",
        "tok_small = AutoTokenizer.from_pretrained(small_model_name)\n",
        "model_small = AutoModel.from_pretrained(small_model_name).to(device)\n",
        "model_small.eval()\n",
        "\n",
        "test_sentence = \"Students are learning about large language models.\"\n",
        "inputs = tok_small(test_sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model_small(**inputs)\n",
        "\n",
        "print(\"Sentence:\", test_sentence)\n",
        "print(\"input_ids shape:\", inputs[\"input_ids\"].shape)\n",
        "print(\"attention_mask shape:\", inputs[\"attention_mask\"].shape)\n",
        "print(\"last_hidden_state shape:\", outputs.last_hidden_state.shape)\n",
        "\n",
        "# Simple mean pooling to get one vector for the sentence\n",
        "last_hidden = outputs.last_hidden_state  # [batch, seq_len, hidden_dim]\n",
        "attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "sum_embeddings = torch.sum(last_hidden * mask_expanded, dim=1)\n",
        "sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
        "sentence_vector = (sum_embeddings / sum_mask).squeeze(0)\n",
        "\n",
        "print(\"Sentence vector shape after mean pooling:\", sentence_vector.shape)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "43049927"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ² Quick Knowledge Check: Foundation Quiz\n",
        "\n",
        "Let's test your understanding before diving deeper! This helps activate prior knowledge and prepares you for the technical content ahead.\n"
      ],
      "id": "04a8f97b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "ğŸ¯ INTERACTIVE QUIZ CELL\n",
        "\n",
        "This cell creates an interactive quiz to test foundational knowledge \n",
        "before proceeding with the main tutorial content.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, HTML\n",
        "    \n",
        "    # Quiz questions about dialogue summarization fundamentals\n",
        "    quiz_questions = [\n",
        "        {\n",
        "            \"question\": \"What is the main advantage of extractive summarization over abstractive summarization for low-resource languages?\",\n",
        "            \"options\": [\n",
        "                \"A. It produces more fluent summaries\",\n",
        "                \"B. It doesn't require training data and preserves exact phrases\",  \n",
        "                \"C. It creates shorter summaries\",\n",
        "                \"D. It works only with English\"\n",
        "            ],\n",
        "            \"correct\": 1,\n",
        "            \"explanation\": \"Extractive methods select existing sentences/phrases, so they don't need training data and avoid generating incorrect facts - perfect for low-resource scenarios!\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"In dialogue summarization, what makes TextRank particularly useful?\",\n",
        "            \"options\": [\n",
        "                \"A. It only works with English dialogues\",\n",
        "                \"B. It requires massive training datasets\",\n",
        "                \"C. It uses graph algorithms to find central sentences based on similarity\", \n",
        "                \"D. It generates new sentences\"\n",
        "            ],\n",
        "            \"correct\": 2,\n",
        "            \"explanation\": \"TextRank builds a similarity graph between sentences and uses PageRank to find the most central ones - language-agnostic and training-free!\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is a key challenge specific to dialogue summarization (vs. single-document summarization)?\",\n",
        "            \"options\": [\n",
        "                \"A. Dialogues are always longer than documents\",\n",
        "                \"B. Multiple speakers with different perspectives and turn-taking dynamics\",\n",
        "                \"C. Dialogues never contain important information\", \n",
        "                \"D. Punctuation is always perfect in dialogues\"\n",
        "            ],\n",
        "            \"correct\": 1,\n",
        "            \"explanation\": \"Dialogues involve multiple speakers, conversational context, and pragmatic elements like who said what and why - much more complex than single-author documents!\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    def create_quiz():\n",
        "        print(\"ğŸ² FOUNDATION QUIZ: Dialogue Summarization\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"Answer all questions to unlock the Language Detective achievement! ğŸ•µï¸\")\n",
        "        print()\n",
        "        \n",
        "        score = 0\n",
        "        total_questions = len(quiz_questions)\n",
        "        \n",
        "        for i, q in enumerate(quiz_questions, 1):\n",
        "            print(f\"â“ Question {i}/{total_questions}:\")\n",
        "            print(f\"   {q['question']}\")\n",
        "            print()\n",
        "            \n",
        "            # Create radio buttons for options\n",
        "            options = q['options']\n",
        "            radio = widgets.RadioButtons(\n",
        "                options=options,\n",
        "                description='',\n",
        "                disabled=False,\n",
        "                layout=widgets.Layout(width='500px')\n",
        "            )\n",
        "            \n",
        "            submit_button = widgets.Button(\n",
        "                description=f'Submit Q{i}',\n",
        "                button_style='info',\n",
        "                layout=widgets.Layout(width='100px')\n",
        "            )\n",
        "            \n",
        "            output = widgets.Output()\n",
        "            \n",
        "            def make_handler(question_data, question_num):\n",
        "                def on_submit(button):\n",
        "                    with output:\n",
        "                        output.clear_output()\n",
        "                        if radio.value:\n",
        "                            selected_idx = options.index(radio.value)\n",
        "                            if selected_idx == question_data['correct']:\n",
        "                                print(\"âœ… Correct! Well done!\")\n",
        "                                print(f\"ğŸ’¡ {question_data['explanation']}\")\n",
        "                            else:\n",
        "                                print(\"âŒ Not quite right.\")\n",
        "                                print(f\"ğŸ’¡ {question_data['explanation']}\")\n",
        "                        else:\n",
        "                            print(\"Please select an answer first!\")\n",
        "                return on_submit\n",
        "            \n",
        "            submit_button.on_click(make_handler(q, i))\n",
        "            \n",
        "            display(radio)\n",
        "            display(submit_button)\n",
        "            display(output)\n",
        "            print(\"-\" * 50)\n",
        "        \n",
        "        print(\"ğŸ† Complete all questions to continue!\")\n",
        "        print(\"ğŸ“– Take your time - understanding these concepts will help you throughout the tutorial.\")\n",
        "    \n",
        "    create_quiz()\n",
        "    \n",
        "except ImportError:\n",
        "    # Fallback for environments without widgets\n",
        "    print(\"ğŸ² FOUNDATION QUIZ: Dialogue Summarization\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Interactive widgets not available - here are the key concepts to remember:\")\n",
        "    print()\n",
        "    print(\"ğŸ’¡ KEY CONCEPTS:\")\n",
        "    print(\"1. ğŸ¯ Extractive summarization selects existing sentences - great for low-resource!\")\n",
        "    print(\"2. ğŸ•¸ï¸  TextRank uses graph algorithms to find central sentences\")  \n",
        "    print(\"3. ğŸ—£ï¸  Dialogues have multiple speakers and conversational dynamics\")\n",
        "    print(\"4. ğŸ“Š ROUGE metrics help evaluate summary quality\")\n",
        "    print(\"5. ğŸŒ Low-resource techniques work without large training datasets\")\n",
        "    print()\n",
        "    print(\"âœ… You're ready to proceed with the hands-on tutorial!\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1d39e92f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [],
      "id": "e18af705"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0.5 Tokenization and Embedding Analysis ğŸ”\n",
        "\n",
        "Before we dive into dialogue summarization, let's understand how multilingual models handle different languages. This is crucial for low-resource language work.\n",
        "\n",
        "**ğŸ¯ What you learn:**\n",
        "- How different models tokenize the same text\n",
        "- Impact of tokenization on model performance\n",
        "- Sentence embedding quality across languages\n",
        "- How to visualize language representation gaps\n",
        "\n",
        "**ğŸ’¡ Why this matters:** Poor tokenization can severely hurt performance in low-resource languages. Understanding this helps you choose better models and preprocessing strategies.\n"
      ],
      "id": "6b68d295"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "### 0.5.1 Multilingual Tokenization Comparison\n",
        "\n",
        "# Sample sentences in different languages - add your target language here!\n",
        "test_sentences = {\n",
        "    \"English\": \"Hello, how are you doing today?\",\n",
        "    \"French\": \"Bonjour, comment allez-vous aujourd'hui?\", \n",
        "    \"German\": \"Hallo, wie geht es dir heute?\",\n",
        "    \"Spanish\": \"Hola, Â¿cÃ³mo estÃ¡s hoy?\",\n",
        "    \"Arabic\": \"Ù…Ø±Ø­Ø¨Ø§ØŒ ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ\",\n",
        "    \"Chinese\": \"ä½ å¥½ï¼Œä½ ä»Šå¤©æ€ä¹ˆæ ·ï¼Ÿ\",\n",
        "    \"Hindi\": \"à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤œ à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?\",\n",
        "    # Add your low-resource language here:\n",
        "    # \"YourLanguage\": \"Your sentence here\"\n",
        "}\n",
        "\n",
        "# Models to compare - from monolingual to multilingual\n",
        "models_to_test = [\n",
        "    \"bert-base-uncased\",           # English-only\n",
        "    \"distilbert-base-multilingual-cased\",  # Lightweight multilingual\n",
        "    \"xlm-roberta-base\",            # Strong multilingual\n",
        "    \"microsoft/mdeberta-v3-base\"   # Latest multilingual\n",
        "]\n",
        "\n",
        "def analyze_tokenization(sentence: str, model_name: str):\n",
        "    \"\"\"Analyze how a model tokenizes a sentence\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    \n",
        "    # Tokenize\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    token_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
        "    \n",
        "    return {\n",
        "        'model': model_name.split('/')[-1],  # Short name\n",
        "        'sentence': sentence,\n",
        "        'num_tokens': len(tokens),\n",
        "        'tokens': tokens[:10],  # First 10 tokens for display\n",
        "        'subword_ratio': len(tokens) / len(sentence.split()),\n",
        "        'has_unk': '[UNK]' in tokens or '<unk>' in tokens\n",
        "    }\n",
        "\n",
        "print(\"ğŸ” TOKENIZATION ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test each language with each model\n",
        "results = []\n",
        "for lang, sentence in test_sentences.items():\n",
        "    print(f\"\\nğŸ“ Language: {lang}\")\n",
        "    print(f\"   Text: {sentence}\")\n",
        "    print()\n",
        "    \n",
        "    for model_name in models_to_test[:2]:  # Test first 2 models to save time\n",
        "        try:\n",
        "            result = analyze_tokenization(sentence, model_name)\n",
        "            results.append({**result, 'language': lang})\n",
        "            \n",
        "            print(f\"   ğŸ¤– {result['model']}\")\n",
        "            print(f\"      Tokens: {result['num_tokens']} | Subword ratio: {result['subword_ratio']:.2f}\")\n",
        "            print(f\"      Sample: {' '.join(result['tokens'][:5])}...\")\n",
        "            if result['has_unk']:\n",
        "                print(f\"      âš ï¸  Contains unknown tokens!\")\n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ {model_name}: Error - {str(e)[:50]}...\")\n",
        "        print()\n",
        "\n",
        "# Convert to DataFrame for easy analysis\n",
        "df_tokens = pd.DataFrame(results)\n",
        "if len(df_tokens) > 0:\n",
        "    print(\"ğŸ“Š TOKENIZATION SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    summary = df_tokens.groupby(['language', 'model']).agg({\n",
        "        'num_tokens': 'mean',\n",
        "        'subword_ratio': 'mean', \n",
        "        'has_unk': 'any'\n",
        "    }).round(2)\n",
        "    print(summary)\n",
        "    \n",
        "    # Add interpretation guide\n",
        "    print(\"\\nğŸ” HOW TO INTERPRET THESE RESULTS:\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"ğŸ“ˆ SUBWORD RATIO (lower = better):\")\n",
        "    print(\"  â€¢ 1.0-1.5: Excellent tokenization (near word-level)\")\n",
        "    print(\"  â€¢ 1.5-3.0: Good tokenization (acceptable for most tasks)\")\n",
        "    print(\"  â€¢ 3.0-5.0: Poor tokenization (many subwords per word)\")\n",
        "    print(\"  â€¢ >5.0: Very poor (each character may become a token)\")\n",
        "    print()\n",
        "    print(\"âŒ UNKNOWN TOKENS (False = better):\")\n",
        "    print(\"  â€¢ False: Model recognizes all text - good coverage\")  \n",
        "    print(\"  â€¢ True: Model sees [UNK] tokens - missing vocabulary\")\n",
        "    print()\n",
        "    print(\"ğŸ¯ KEY INSIGHTS FROM YOUR RESULTS:\")\n",
        "    \n",
        "    # Find best and worst performing combinations\n",
        "    if len(summary) > 0:\n",
        "        best_subword = summary['subword_ratio'].min()\n",
        "        worst_subword = summary['subword_ratio'].max()\n",
        "        best_lang_model = summary[summary['subword_ratio'] == best_subword].index[0]\n",
        "        worst_lang_model = summary[summary['subword_ratio'] == worst_subword].index[0]\n",
        "        \n",
        "        print(f\"  âœ… BEST: {best_lang_model[0]} + {best_lang_model[1]} (ratio: {best_subword})\")\n",
        "        print(f\"  âŒ WORST: {worst_lang_model[0]} + {worst_lang_model[1]} (ratio: {worst_subword})\")\n",
        "        \n",
        "        # Count unknown token issues\n",
        "        unk_issues = summary[summary['has_unk'] == True]\n",
        "        if len(unk_issues) > 0:\n",
        "            print(f\"  âš ï¸  UNKNOWN TOKENS: {len(unk_issues)} language-model combinations have missing vocabulary\")\n",
        "        else:\n",
        "            print(\"  âœ… NO UNKNOWN TOKENS: All models handle these languages well\")\n",
        "            \n",
        "    print(\"\\nğŸ’¡ PRACTICAL IMPLICATIONS:\")\n",
        "    print(\"  â€¢ Use multilingual models for non-English languages\")\n",
        "    print(\"  â€¢ Subword ratio >3.0 may hurt downstream performance\")\n",
        "    print(\"  â€¢ Consider language-specific tokenizers for critical applications\")\n",
        "    \n",
        "else:\n",
        "    print(\"âš ï¸  No results to display\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3c7ef581"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "### 0.5.2 Multilingual Sentence Embeddings and Visualization\n",
        "\n",
        "# Create sentence embeddings for cross-lingual comparison\n",
        "print(\"ğŸŒ SENTENCE EMBEDDING ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(\"â±ï¸  Loading multilingual sentence transformer...\")\n",
        "\n",
        "try:\n",
        "    # Use a lightweight multilingual sentence transformer\n",
        "    embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "    print(\"âœ… Model loaded successfully!\")\n",
        "    \n",
        "    # Select a subset of languages for embedding analysis\n",
        "    embedding_sentences = {\n",
        "        \"English\": [\"Hello, how are you?\", \"I need help with my computer\", \"What time is it?\"],\n",
        "        \"French\": [\"Bonjour, comment allez-vous?\", \"J'ai besoin d'aide avec mon ordinateur\", \"Quelle heure est-il?\"],\n",
        "        \"German\": [\"Hallo, wie geht es dir?\", \"Ich brauche Hilfe mit meinem Computer\", \"Wie spÃ¤t ist es?\"],\n",
        "        \"Arabic\": [\"Ù…Ø±Ø­Ø¨Ø§ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\", \"Ø£Ø­ØªØ§Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ø¹ Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±\", \"ÙƒÙ… Ø§Ù„Ø³Ø§Ø¹Ø©ØŸ\"],\n",
        "        # Add your low-resource language here:\n",
        "        # \"YourLanguage\": [\"Greeting\", \"Help request\", \"Time question\"]\n",
        "    }\n",
        "    \n",
        "    # Generate embeddings\n",
        "    all_sentences = []\n",
        "    all_languages = []\n",
        "    all_labels = []\n",
        "    \n",
        "    labels = [\"greeting\", \"help_request\", \"time_question\"]\n",
        "    \n",
        "    print(\"\\nğŸ”„ Generating embeddings...\")\n",
        "    for lang, sentences in embedding_sentences.items():\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            all_sentences.append(sentence)\n",
        "            all_languages.append(lang)\n",
        "            all_labels.append(labels[i])\n",
        "    \n",
        "    # Get embeddings\n",
        "    embeddings = embedding_model.encode(all_sentences)\n",
        "    print(f\"âœ… Generated {len(embeddings)} embeddings of dimension {embeddings.shape[1]}\")\n",
        "    \n",
        "    # Simple similarity analysis\n",
        "    print(\"\\nğŸ” CROSS-LINGUAL SIMILARITY ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    \n",
        "    # Calculate similarities between same semantic content across languages\n",
        "    for i, label in enumerate(labels):\n",
        "        print(f\"\\nğŸ“ Analyzing: {label}\")\n",
        "        \n",
        "        # Get sentences with this label\n",
        "        indices = [j for j, l in enumerate(all_labels) if l == label]\n",
        "        label_sentences = [all_sentences[j] for j in indices]\n",
        "        label_languages = [all_languages[j] for j in indices]\n",
        "        label_embeddings = embeddings[indices]\n",
        "        \n",
        "        # Calculate pairwise similarities\n",
        "        similarities = cosine_similarity(label_embeddings)\n",
        "        \n",
        "        for idx1 in range(len(label_sentences)):\n",
        "            for idx2 in range(idx1 + 1, len(label_sentences)):\n",
        "                sim = similarities[idx1, idx2]\n",
        "                lang1, lang2 = label_languages[idx1], label_languages[idx2]\n",
        "                print(f\"   {lang1} â†” {lang2}: {sim:.3f}\")\n",
        "    \n",
        "    # Visualization with dimensionality reduction\n",
        "    print(\"\\nğŸ“Š EMBEDDING VISUALIZATION\")\n",
        "    print(\"=\"*30)\n",
        "    \n",
        "    try:\n",
        "        from sklearn.decomposition import PCA\n",
        "        import matplotlib.pyplot as plt\n",
        "        \n",
        "        # Reduce to 2D for visualization\n",
        "        pca = PCA(n_components=2, random_state=42)\n",
        "        embeddings_2d = pca.fit_transform(embeddings)\n",
        "        \n",
        "        # Create visualization\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        # Color by language\n",
        "        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
        "        unique_langs = list(set(all_languages))\n",
        "        \n",
        "        for i, lang in enumerate(unique_langs):\n",
        "            lang_indices = [j for j, l in enumerate(all_languages) if l == lang]\n",
        "            lang_embeddings = embeddings_2d[lang_indices]\n",
        "            plt.scatter(lang_embeddings[:, 0], lang_embeddings[:, 1], \n",
        "                       c=colors[i % len(colors)], label=lang, alpha=0.7, s=100)\n",
        "            \n",
        "            # Add text labels\n",
        "            for j, idx in enumerate(lang_indices):\n",
        "                plt.annotate(all_labels[idx][:4], \n",
        "                           (lang_embeddings[j, 0], lang_embeddings[j, 1]),\n",
        "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "        \n",
        "        plt.title('Multilingual Sentence Embeddings (PCA)')\n",
        "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"ğŸ’¡ What to look for:\")\n",
        "        print(\"- Do same semantic concepts cluster together across languages?\")\n",
        "        print(\"- Are some languages closer/farther from others?\") \n",
        "        print(\"- Is your target language well-represented?\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Visualization error: {str(e)}\")\n",
        "        print(\"ğŸ’¡ This is normal in some environments - the analysis above is still valid!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading sentence transformer: {str(e)}\")\n",
        "    print(\"ğŸ’¡ This might happen in resource-constrained environments\")\n",
        "    print(\"ğŸ“ Key takeaway: Always test embedding quality for your target language!\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9e1a228a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.5.3 ğŸ¯ Systematic Language Analysis Framework\n",
        "\n",
        "Now let's create a systematic evaluation framework to track findings across languages and models:\n",
        "\n",
        "**ğŸ”¬ Methodology:** We'll use pandas DataFrames to systematically track and analyze results, similar to research best practices.\n"
      ],
      "id": "8bf1dd99"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ“Š Create systematic evaluation framework for language analysis\n",
        "\n",
        "# Set up your target languages - ADD YOUR LANGUAGE HERE\n",
        "target_languages = [\n",
        "    {\"code\": \"en\", \"name\": \"English\", \"family\": \"Germanic\", \"speakers\": \"1.5B\"},\n",
        "    {\"code\": \"fr\", \"name\": \"French\", \"family\": \"Romance\", \"speakers\": \"280M\"}, \n",
        "    {\"code\": \"de\", \"name\": \"German\", \"family\": \"Germanic\", \"speakers\": \"130M\"},\n",
        "    {\"code\": \"ar\", \"name\": \"Arabic\", \"family\": \"Semitic\", \"speakers\": \"420M\"},\n",
        "    {\"code\": \"zh\", \"name\": \"Chinese\", \"family\": \"Sino-Tibetan\", \"speakers\": \"1.1B\"},\n",
        "    {\"code\": \"hi\", \"name\": \"Hindi\", \"family\": \"Indo-European\", \"speakers\": \"600M\"},\n",
        "    # ğŸŒ ADD YOUR LOW-RESOURCE LANGUAGE HERE:\n",
        "    # {\"code\": \"your_code\", \"name\": \"Your Language\", \"family\": \"Language Family\", \"speakers\": \"~XXXk\"},\n",
        "]\n",
        "\n",
        "languages_df = pd.DataFrame(target_languages)\n",
        "print(\"ğŸŒ TARGET LANGUAGES FOR ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "display(languages_df)\n",
        "\n",
        "# Create evaluation tracking DataFrame\n",
        "evaluation_columns = [\n",
        "    \"language_code\", \"language_name\", \"model_name\", \n",
        "    \"avg_tokens_per_word\", \"has_unknown_tokens\", \"subword_quality_score\",\n",
        "    \"embedding_dimension\", \"avg_similarity_to_english\", \"clustering_quality\",\n",
        "    \"overall_model_score\", \"recommended_for_production\", \"notes\"\n",
        "]\n",
        "\n",
        "evaluation_df = pd.DataFrame(columns=evaluation_columns)\n",
        "print(f\"\\nğŸ“Š Evaluation framework ready with {len(evaluation_columns)} metrics\")\n",
        "print(\"You'll fill this as you analyze different models and languages\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b78cd637"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ”¬ Systematic Analysis Functions - Data Collection Made Easy\n",
        "\n",
        "def analyze_language_systematically(language_code, language_name, test_sentence):\n",
        "    \"\"\"\n",
        "    Systematically analyze a language across multiple models and metrics.\n",
        "    Returns a DataFrame row ready for evaluation_df.\n",
        "    \"\"\"\n",
        "    \n",
        "    models_to_test = [\"bert-base-uncased\", \"distilbert-base-multilingual-cased\", \"xlm-roberta-base\"]\n",
        "    results = []\n",
        "    \n",
        "    print(f\"\\nğŸ” ANALYZING: {language_name} ({language_code})\")\n",
        "    print(f\"ğŸ“ Test sentence: {test_sentence}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for model_name in models_to_test:\n",
        "        try:\n",
        "            # Tokenization analysis\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            tokens = tokenizer.tokenize(test_sentence)\n",
        "            words = test_sentence.split()\n",
        "            \n",
        "            # Calculate metrics\n",
        "            avg_tokens_per_word = len(tokens) / len(words) if words else 0\n",
        "            has_unknown = '[UNK]' in tokens or '<unk>' in tokens\n",
        "            \n",
        "            # Simple quality score (lower subword ratio = better for this metric)\n",
        "            subword_quality = max(0, 3 - avg_tokens_per_word)  # 0-3 scale\n",
        "            \n",
        "            result = {\n",
        "                \"language_code\": language_code,\n",
        "                \"language_name\": language_name, \n",
        "                \"model_name\": model_name.split('/')[-1],\n",
        "                \"avg_tokens_per_word\": round(avg_tokens_per_word, 2),\n",
        "                \"has_unknown_tokens\": has_unknown,\n",
        "                \"subword_quality_score\": round(subword_quality, 2),\n",
        "                \"embedding_dimension\": \"N/A\",  # Will fill in embedding analysis\n",
        "                \"avg_similarity_to_english\": \"N/A\",\n",
        "                \"clustering_quality\": \"N/A\", \n",
        "                \"overall_model_score\": round(subword_quality, 1),  # Simplified for now\n",
        "                \"recommended_for_production\": subword_quality > 2.0,\n",
        "                \"notes\": f\"Tokens: {len(tokens)}, Words: {len(words)}\"\n",
        "            }\n",
        "            \n",
        "            results.append(result)\n",
        "            \n",
        "            # Print summary\n",
        "            print(f\"ğŸ¤– {model_name.split('/')[-1]:<25} | Tokens/Word: {avg_tokens_per_word:>4.2f} | Quality: {subword_quality:>4.2f} | UNK: {'Yes' if has_unknown else 'No'}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error with {model_name}: {str(e)[:50]}...\")\n",
        "            \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def add_to_evaluation(evaluation_df, language_code, language_name, test_sentence):\n",
        "    \"\"\"Helper function to add analysis results to the main evaluation DataFrame\"\"\"\n",
        "    new_results = analyze_language_systematically(language_code, language_name, test_sentence)\n",
        "    return pd.concat([evaluation_df, new_results], ignore_index=True)\n",
        "\n",
        "print(\"âœ… Systematic analysis functions ready!\")\n",
        "print(\"ğŸ’¡ Use: evaluation_df = add_to_evaluation(evaluation_df, 'en', 'English', 'Your test sentence')\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "590a8f5d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸš€ Interactive Language Analysis - Try It Yourself!\n",
        "\n",
        "# Example: Analyze English first (baseline)\n",
        "print(\"ğŸ¯ DEMONSTRATION: Analyzing English as baseline\")\n",
        "evaluation_df = add_to_evaluation(evaluation_df, \"en\", \"English\", \"Hello, how are you doing today?\")\n",
        "\n",
        "# ğŸŒ YOUR TURN: Add your low-resource language analysis\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ğŸŒ YOUR TURN: Analyze your target low-resource language\")\n",
        "print(\"Replace the example below with your language and test sentence:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# UNCOMMENT AND MODIFY THESE LINES FOR YOUR LANGUAGE:\n",
        "# evaluation_df = add_to_evaluation(evaluation_df, \"your_code\", \"Your Language Name\", \"Your test sentence here\")\n",
        "\n",
        "# Example for Luxembourgish (you can modify this):\n",
        "# evaluation_df = add_to_evaluation(evaluation_df, \"lb\", \"Luxembourgish\", \"Moien, wÃ©i geet et dir haut?\")\n",
        "\n",
        "print(\"\\nğŸ“Š Current evaluation results:\")\n",
        "if len(evaluation_df) > 0:\n",
        "    display(evaluation_df[[\"language_name\", \"model_name\", \"avg_tokens_per_word\", \"subword_quality_score\", \"recommended_for_production\"]])\n",
        "    \n",
        "    # Quick visualization\n",
        "    if len(evaluation_df) > 2:  # Only plot if we have some data\n",
        "        import matplotlib.pyplot as plt\n",
        "        \n",
        "        # Group by language and show average quality\n",
        "        avg_quality = evaluation_df.groupby(\"language_name\")[\"subword_quality_score\"].mean().sort_values(ascending=False)\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        avg_quality.plot(kind='bar')\n",
        "        plt.title('Average Subword Quality Score by Language')\n",
        "        plt.ylabel('Quality Score (higher = better tokenization)')\n",
        "        plt.xlabel('Language')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"ğŸ’¡ Interpretation: Higher scores mean better tokenization (fewer subwords per word)\")\n",
        "else:\n",
        "    print(\"No analysis results yet. Run the analysis above first!\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3161d8d7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸ” **Checkpoint 0.5: Systematic Language Analysis Complete!**\n",
        "\n",
        "**âœ… What you've accomplished with systematic analysis:**\n",
        "- ğŸ“Š Created a structured evaluation framework\n",
        "- ğŸ”¬ Analyzed tokenization quality across multiple models systematically\n",
        "- ğŸ“ˆ Visualized comparative performance across languages\n",
        "- ğŸ¯ Identified best models for your target language using data\n",
        "\n",
        "**ğŸ¯ Key Questions for Reflection:**\n",
        "1. **Model Selection:** Which model performed best for your language? Why?\n",
        "2. **Tokenization Quality:** How does subword splitting affect downstream task performance?\n",
        "3. **Resource Planning:** Based on tokenization quality, what's your cost/performance tradeoff?\n",
        "4. **Production Readiness:** Which combinations would you recommend for real applications?\n",
        "\n",
        "**ğŸ’¡ Next Steps:** \n",
        "- Export your evaluation DataFrame: `evaluation_df.to_csv('language_analysis.csv')`\n",
        "- Share findings with your team using the structured data\n",
        "- Use these insights to inform model choice in dialogue summarization\n",
        "\n",
        "**ğŸŒŸ Advanced Challenge:** \n",
        "Try adding semantic similarity analysis to your evaluation framework using sentence transformers!\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ¯ Next:** We'll apply these insights to build robust dialogue summarization systems!\n"
      ],
      "id": "03732231"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Get a real dialogue dataset\n",
        "\n",
        "To keep this notebook self contained, we use a public domain English play. The text is not invented for this tutorial. It is an excerpt from *The Importance of Being Earnest* by Oscar Wilde (first published in 1895, public domain in many jurisdictions).\n",
        "\n",
        "We will parse it into speaker turns, then create short dialogue segments that resemble real conversations.\n"
      ],
      "id": "e76613e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RAW_TEXT = '''\n",
        "[Enter Lane.]\n",
        "\n",
        "LANE. Why, Mr. Worthing, I suppose this is one of your pleasant\n",
        "surprises? I have been expecting you back some time ago.\n",
        "\n",
        "JACK. I have not been able to return sooner. I have been detained in\n",
        "town.\n",
        "\n",
        "LANE. I have received a message from Mr. Algernon. He says he will be\n",
        "down at four o'clock.\n",
        "\n",
        "JACK. Is Mr. Algernon here?\n",
        "\n",
        "LANE. Yes, sir. He is in the dining-room.\n",
        "\n",
        "JACK. I must see him at once.\n",
        "\n",
        "[Enter Algernon.]\n",
        "\n",
        "ALGERNON. How are you, my dear Ernest? What brings you up to town?\n",
        "\n",
        "JACK. Oh, pleasure, pleasure. What else should bring one anywhere?\n",
        "\n",
        "ALGERNON. Eating as usual, I see.\n",
        "\n",
        "JACK. I believe it is customary in good society to take some\n",
        "refreshment at five o'clock.\n",
        "\n",
        "ALGERNON. Well, it is a custom that I approve of, and I will do my best\n",
        "to start it again. However, you are not quite truthful. You did not\n",
        "come up for pleasure.\n",
        "\n",
        "JACK. What on earth do you mean?\n",
        "\n",
        "ALGERNON. You came up to town to tell me to keep away from your cousin.\n",
        "\n",
        "JACK. My cousin?\n",
        "\n",
        "ALGERNON. Yes. That charming girl you are always talking about.\n",
        "\n",
        "JACK. Cecily?\n",
        "\n",
        "ALGERNON. Cecily. She is my cousin now, you know.\n",
        "\n",
        "JACK. You have never met her.\n",
        "\n",
        "ALGERNON. She is my cousin because I intend to marry her.\n",
        "'''\n",
        "\n",
        "def parse_play_to_turns(text: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Parse a simple play excerpt into (speaker, utterance) turns.\n",
        "    Assumptions.\n",
        "    1) Speaker turns look like 'NAME.' at the start of a line.\n",
        "    2) Stage directions are in [brackets] or parentheses and are dropped.\n",
        "\n",
        "    Returns a DataFrame with columns: turn_id, speaker, text.\n",
        "    \"\"\"\n",
        "    lines = [ln.strip() for ln in text.strip().splitlines() if ln.strip()]\n",
        "    turns = []\n",
        "    current_speaker = None\n",
        "    buffer = []\n",
        "\n",
        "    def flush():\n",
        "        nonlocal buffer, current_speaker\n",
        "        if current_speaker and buffer:\n",
        "            utt = \" \".join(buffer).strip()\n",
        "            utt = re.sub(r\"\\s+\", \" \", utt)\n",
        "            if utt:\n",
        "                turns.append({\"speaker\": current_speaker, \"text\": utt})\n",
        "        buffer = []\n",
        "\n",
        "    speaker_pat = re.compile(r\"^([A-Z][A-Z\\s'\\-]+)\\.\\s*(.*)$\")\n",
        "\n",
        "    for ln in lines:\n",
        "        if ln.startswith(\"[\") and ln.endswith(\"]\"):\n",
        "            continue\n",
        "        if ln.startswith(\"(\") and ln.endswith(\")\"):\n",
        "            continue\n",
        "\n",
        "        m = speaker_pat.match(ln)\n",
        "        if m:\n",
        "            flush()\n",
        "            current_speaker = m.group(1).strip()\n",
        "            rest = m.group(2).strip()\n",
        "            if rest:\n",
        "                buffer.append(rest)\n",
        "        else:\n",
        "            buffer.append(ln)\n",
        "\n",
        "    flush()\n",
        "    df = pd.DataFrame(turns)\n",
        "    df.insert(0, \"turn_id\", range(len(df)))\n",
        "    return df\n",
        "\n",
        "turns_df = parse_play_to_turns(RAW_TEXT)\n",
        "turns_df.head(10)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "83e7fcb1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Create dialogue windows\n",
        "\n",
        "Many dialogue datasets are long conversations. Summarization is easier to teach with smaller windows. We will create overlapping windows of turns, then treat each window as a dialogue sample.\n",
        "\n",
        "You can adjust the window size. Smaller windows are easier for small models. Larger windows stress test context handling.\n"
      ],
      "id": "f97ad9d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def make_dialogue_windows(turns: pd.DataFrame, window_turns: int = 10, stride: int = 6) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert a turn DataFrame into overlapping dialogue windows.\n",
        "\n",
        "    Returns a DataFrame with: sample_id, dialogue_text, speakers_involved, n_turns.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    n = len(turns)\n",
        "    sample_id = 0\n",
        "    for start in range(0, max(1, n - window_turns + 1), stride):\n",
        "        end = min(n, start + window_turns)\n",
        "        chunk = turns.iloc[start:end]\n",
        "        dialogue_lines = [f\"{r.speaker}: {r.text}\" for r in chunk.itertuples()]\n",
        "        dialogue_text = \"\\n\".join(dialogue_lines)\n",
        "        speakers = sorted(set(chunk[\"speaker\"].tolist()))\n",
        "        samples.append(\n",
        "            {\n",
        "                \"sample_id\": sample_id,\n",
        "                \"dialogue_text\": dialogue_text,\n",
        "                \"speakers_involved\": speakers,\n",
        "                \"n_turns\": int(end - start),\n",
        "            }\n",
        "        )\n",
        "        sample_id += 1\n",
        "        if end == n:\n",
        "            break\n",
        "    return pd.DataFrame(samples)\n",
        "\n",
        "samples_df = make_dialogue_windows(turns_df, window_turns=10, stride=6)\n",
        "samples_df\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e0b57c59"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Preview one sample\n",
        "\n",
        "Read the dialogue. Then, in your own words, write a one sentence summary in the next cell. Keep it short. This will become our first human reference.\n"
      ],
      "id": "54a6296f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sample = samples_df.loc[0, \"dialogue_text\"]\n",
        "print(sample)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "070188fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your one sentence reference summary.\n",
        "# You can edit this string. The notebook will still run if you do not.\n",
        "\n",
        "REFERENCE_SUMMARY = \"Jack arrives and learns Algernon is visiting, then Algernon teases Jack and reveals he plans to marry Jack's cousin Cecily.\"\n",
        "\n",
        "print(REFERENCE_SUMMARY)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c84d3f50"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Baseline. Extractive TextRank summarization\n",
        "\n",
        "Before using an LLM, build a baseline that is fast, cheap, and interpretable. TextRank selects the most central sentences using a similarity graph and PageRank.\n",
        "\n",
        "This baseline is language agnostic, as long as you can split text into sentences. That is why it is valuable for low resource languages.\n"
      ],
      "id": "6406b9ca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "\n",
        "def split_sentences(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Very simple sentence splitter.\n",
        "    For robust multilingual splitting, consider spaCy or Stanza.\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    sents = re.split(r\"(?<=[\\.\\?\\!])\\s+\", text)\n",
        "    return [s.strip() for s in sents if s.strip()]\n",
        "\n",
        "def textrank_summarize(dialogue_text: str, max_sentences: int = 2) -> str:\n",
        "    \"\"\"\n",
        "    Extractive summarization using TextRank on sentence similarity.\n",
        "    \"\"\"\n",
        "    content = re.sub(r\"^[A-Z][A-Z\\s'\\-]+:\\s*\", \"\", dialogue_text, flags=re.MULTILINE)\n",
        "    sentences = split_sentences(content)\n",
        "    if not sentences:\n",
        "        return \"\"\n",
        "    if len(sentences) <= max_sentences:\n",
        "        return \" \".join(sentences)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    sim = cosine_similarity(X)\n",
        "    np.fill_diagonal(sim, 0.0)\n",
        "\n",
        "    graph = nx.from_numpy_array(sim)\n",
        "    scores = nx.pagerank(graph, max_iter=200)\n",
        "\n",
        "    ranked = sorted(range(len(sentences)), key=lambda i: scores.get(i, 0.0), reverse=True)\n",
        "    picked = sorted(ranked[:max_sentences])\n",
        "    return \" \".join([sentences[i] for i in picked])\n",
        "\n",
        "baseline_summary = textrank_summarize(sample, max_sentences=2)\n",
        "print(\"Baseline summary:\\n\", baseline_summary)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "db2aed65"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Quick evaluation. ROUGE\n",
        "\n",
        "ROUGE is imperfect, but it is a quick sanity check. We will compute ROUGE 1, ROUGE 2, and ROUGE L against your reference summary.\n"
      ],
      "id": "a18a7d4f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def rouge_scores(pred: str, ref: str) -> Dict[str, float]:\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    scores = scorer.score(ref, pred)\n",
        "    return {k: v.fmeasure for k, v in scores.items()}\n",
        "\n",
        "print(\"ROUGE (baseline vs reference):\")\n",
        "rouge_scores(baseline_summary, REFERENCE_SUMMARY)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "336ed187"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Mini quiz. What makes dialogue summarization harder?\n",
        "\n",
        "Try to answer before running the cell. Then run it for instant feedback.\n"
      ],
      "id": "6a97e0e1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "except Exception:\n",
        "    widgets = None\n",
        "\n",
        "QUESTION = \"Which factor is most specific to dialogue summarization, compared to single speaker summarization?\"\n",
        "OPTIONS = [\n",
        "    \"A. Dialogues contain named entities.\",\n",
        "    \"B. Dialogues include speaker turns and pragmatic intent.\",\n",
        "    \"C. Dialogues use punctuation.\",\n",
        "    \"D. Dialogues are always longer than articles.\",\n",
        "]\n",
        "CORRECT = 1\n",
        "EXPLANATION = \"Speaker turns and pragmatic intent are core. You often need to resolve who said what and why.\"\n",
        "\n",
        "def run_quiz():\n",
        "    if widgets is None:\n",
        "        print(QUESTION)\n",
        "        print()  # Add space after question\n",
        "        for opt in OPTIONS:\n",
        "            print(f\"   {opt}\")  # Add indentation\n",
        "            print()  # Add space after each option\n",
        "        print(\"Correct:\", OPTIONS[CORRECT])\n",
        "        print(\"Explanation:\", EXPLANATION)\n",
        "        return\n",
        "\n",
        "    radio = widgets.RadioButtons(options=OPTIONS, description=\"Your answer:\")\n",
        "    out = widgets.Output()\n",
        "\n",
        "    def on_change(change):\n",
        "        if change[\"name\"] != \"value\":\n",
        "            return\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            idx = OPTIONS.index(change[\"new\"])\n",
        "            if idx == CORRECT:\n",
        "                print(\"Correct.\")\n",
        "            else:\n",
        "                print(\"Not quite.\")\n",
        "            print(\"Explanation:\", EXPLANATION)\n",
        "\n",
        "    radio.observe(on_change)\n",
        "    display(radio, out)\n",
        "\n",
        "run_quiz()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "48c54153"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸ” **Checkpoint 1: You now have a solid baseline!** \n",
        "\n",
        "**âœ… What you've accomplished:**\n",
        "- Built a dialogue dataset from raw text\n",
        "- Implemented TextRank extractive summarization  \n",
        "- Evaluated with ROUGE metrics\n",
        "- Learned what makes dialogue summarization challenging\n",
        "\n",
        "**ğŸ¯ Next up:** We'll simulate low-resource conditions and learn adaptation strategies.\n",
        "\n",
        "**ğŸ’¡ For LLM-based summarization:** Check out Session 2 on Prompt Engineering!\n"
      ],
      "id": "2258f279"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸ“Š Systematic Evaluation Framework for Dialogue Summarization\n",
        "\n",
        "# Create structured evaluation tracking (similar to research best practices)\n",
        "summarization_experiments = []\n",
        "\n",
        "def evaluate_summarization_method(method_name, summary_text, reference_text, \n",
        "                                approach_type=\"extractive\", model_size=\"0MB\", \n",
        "                                hardware_req=\"CPU\", notes=\"\"):\n",
        "    \"\"\"\n",
        "    Systematically evaluate and track summarization experiments\n",
        "    \"\"\"\n",
        "    scores = rouge_scores(summary_text, reference_text)\n",
        "    \n",
        "    experiment = {\n",
        "        \"method_name\": method_name,\n",
        "        \"approach_type\": approach_type,\n",
        "        \"model_size\": model_size,\n",
        "        \"hardware_req\": hardware_req,\n",
        "        \"summary_text\": summary_text[:100] + \"...\" if len(summary_text) > 100 else summary_text,\n",
        "        \"rouge1\": scores[\"rouge1\"],\n",
        "        \"rouge2\": scores[\"rouge2\"], \n",
        "        \"rougeL\": scores[\"rougeL\"],\n",
        "        \"avg_rouge\": (scores[\"rouge1\"] + scores[\"rouge2\"] + scores[\"rougeL\"]) / 3,\n",
        "        \"summary_length\": len(summary_text.split()),\n",
        "        \"reference_length\": len(reference_text.split()),\n",
        "        \"compression_ratio\": len(reference_text.split()) / len(summary_text.split()) if summary_text else 0,\n",
        "        \"notes\": notes\n",
        "    }\n",
        "    \n",
        "    return experiment\n",
        "\n",
        "# Evaluate our TextRank baseline\n",
        "print(\"ğŸ¯ SYSTEMATIC EVALUATION: TextRank Baseline\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "baseline_eval = evaluate_summarization_method(\n",
        "    method_name=\"TextRank Extractive\",\n",
        "    summary_text=baseline_summary,\n",
        "    reference_text=REFERENCE_SUMMARY,\n",
        "    approach_type=\"extractive\", \n",
        "    model_size=\"0MB (no model)\",\n",
        "    hardware_req=\"CPU only\",\n",
        "    notes=\"Language-agnostic, fast, interpretable\"\n",
        ")\n",
        "\n",
        "summarization_experiments.append(baseline_eval)\n",
        "\n",
        "# Display results in structured format\n",
        "experiments_df = pd.DataFrame(summarization_experiments)\n",
        "print(\"ğŸ“Š RESULTS SUMMARY\")\n",
        "display(experiments_df[[\"method_name\", \"rouge1\", \"rouge2\", \"rougeL\", \"avg_rouge\", \"compression_ratio\"]])\n",
        "\n",
        "print(f\"\\nğŸ¯ BASELINE PERFORMANCE ANALYSIS\")\n",
        "print(f\"ROUGE-1: {baseline_eval['rouge1']:.3f} (unigram overlap)\")\n",
        "print(f\"ROUGE-2: {baseline_eval['rouge2']:.3f} (bigram overlap)\") \n",
        "print(f\"ROUGE-L: {baseline_eval['rougeL']:.3f} (longest common subsequence)\")\n",
        "print(f\"Average ROUGE: {baseline_eval['avg_rouge']:.3f}\")\n",
        "print(f\"Compression Ratio: {baseline_eval['compression_ratio']:.1f}:1\")\n",
        "\n",
        "print(f\"\\nâœ… TextRank Baseline established - ready to compare with other methods!\")\n",
        "print(f\"ğŸ’¡ This systematic evaluation framework will track all your experiments\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9612676b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Low Resource Mode ğŸŒ\n",
        "\n",
        "Now we'll simulate low-resource conditions and learn adaptation strategies.\n",
        "\n",
        "**What makes a language \"low-resource\"?**\n",
        "- Very little labeled data\n",
        "- Limited preprocessing tools  \n",
        "- Domain mismatch with training data\n",
        "- Orthographic variation and noise\n"
      ],
      "id": "fc256477"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "### 4.1 Simulate Low-Resource Conditions\n",
        "\n",
        "#We'll corrupt our clean English dialogue to simulate challenges faced by low-resource languages:\n",
        "\n",
        "def low_resource_corrupt(text: str, drop_punct_prob: float = 0.5, typo_prob: float = 0.08) -> str:\n",
        "    \"\"\"Simulate low-resource conditions by adding noise\"\"\"\n",
        "    import random\n",
        "    rng = random.Random(842)\n",
        "    out_chars = []\n",
        "    for ch in text:\n",
        "        # Randomly drop punctuation\n",
        "        if ch in \".?!,\" and rng.random() < drop_punct_prob:\n",
        "            continue\n",
        "        # Add random typos\n",
        "        if ch.isalpha() and rng.random() < typo_prob:\n",
        "            if rng.random() < 0.5:\n",
        "                out_chars.append(ch.swapcase())  # Case error\n",
        "            else:\n",
        "                out_chars.append(chr(((ord(ch.lower()) - 97 + 1) % 26) + 97))  # Letter shift\n",
        "        else:\n",
        "            out_chars.append(ch)\n",
        "    return \"\".join(out_chars)\n",
        "\n",
        "# Apply corruption to our sample\n",
        "low_resource_sample = low_resource_corrupt(sample, drop_punct_prob=0.6, typo_prob=0.04)\n",
        "print(\"ğŸŒ SIMULATED LOW-RESOURCE TEXT:\")\n",
        "print(\"=\"*60)\n",
        "print(low_resource_sample[:400] + \"...\")\n",
        "print(\"\\nğŸ’¡ Notice: Missing punctuation, typos, inconsistent casing\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d2f93aca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Prompt remix playground\n",
        "\n",
        "You will remix a prompt by selecting options. This is a safe way to teach prompt engineering without making it feel abstract.\n",
        "\n",
        "Pick your settings, then run the cell. Try to make the summary both concise and faithful.\n"
      ],
      "id": "8c400a2f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "STYLE_OPTIONS = [\"neutral\", \"bullet\", \"tweet\", \"meeting_minutes\"]\n",
        "FOCUS_OPTIONS = [\"decisions\", \"conflict\", \"relationships\", \"actions\"]\n",
        "\n",
        "def build_prompt(style: str, focus: str, max_sentences: int) -> str:\n",
        "    style = style.lower().strip()\n",
        "    focus = focus.lower().strip()\n",
        "\n",
        "    base = f\"Summarize the conversation in at most {max_sentences} sentences.\"\n",
        "    if focus == \"decisions\":\n",
        "        base += \" Focus on decisions and commitments.\"\n",
        "    elif focus == \"conflict\":\n",
        "        base += \" Focus on disagreements and what caused them.\"\n",
        "    elif focus == \"relationships\":\n",
        "        base += \" Focus on who relates to whom and the social situation.\"\n",
        "    elif focus == \"actions\":\n",
        "        base += \" Focus on actions and next steps.\"\n",
        "\n",
        "    if style == \"bullet\":\n",
        "        base += \" Use 2 to 4 bullet points.\"\n",
        "    elif style == \"tweet\":\n",
        "        base += \" Write it as a single tweet style sentence, under 240 characters.\"\n",
        "    elif style == \"meeting_minutes\":\n",
        "        base += \" Format as meeting minutes with sections: Context, Key Points, Next Steps.\"\n",
        "\n",
        "    base += \" Do not invent facts. Preserve names.\"\n",
        "    return base\n",
        "\n",
        "def run_playground(style=\"neutral\", focus=\"relationships\", max_sentences=2):\n",
        "    prompt = build_prompt(style, focus, max_sentences)\n",
        "    print(\"Prompt:\\n\", prompt, \"\\n\")\n",
        "    out = generate_summary_t5(sample, prompt, max_new_tokens=120, temperature=0.0)\n",
        "    print(\"Model output:\\n\", out)\n",
        "    return out\n",
        "\n",
        "llm_play = run_playground(style=\"meeting_minutes\", focus=\"relationships\", max_sentences=2)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9e2015b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸš¨ Note: LLM Content Moved to Session 2\n",
        "\n",
        "**Looking for prompt engineering and LLM-based summarization?**\n",
        "\n",
        "All LLM-related content (zero-shot, few-shot, Chain-of-Thought prompting) has been moved to **Session 2: Pretrained Models and Prompt Engineering**.\n",
        "\n",
        "This keeps Session 1 focused on robust baseline methods that work without large language models.\n",
        "\n",
        "**ğŸ¯ Session 1 Focus:** TextRank, extractive methods, low-resource adaptations  \n",
        "**ğŸ¯ Session 2 Focus:** LLM prompting, generation parameters, cross-lingual transfer\n",
        "\n",
        "---\n",
        "\n",
        "**âš ï¸ Sections 4.2-4.4 below contain placeholder LLM code** that will show \"[This functionality moved to Session 2]\". \n",
        "\n",
        "**For working LLM examples:** Use **Session 2: Pretrained Models and Prompt Engineering**\n",
        "\n",
        "---\n",
        "\n",
        "## 4.2 Normalization Strategies for Low-Resource Languages\n",
        "\n",
        "Let's explore practical techniques for handling noisy, inconsistent text:\n"
      ],
      "id": "44589539"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 One shot and few shot prompts\n",
        "\n",
        "When you have little data, examples are powerful. We will create a small in notebook prompt set.\n",
        "\n",
        "You can replace the examples with your own dialogues later.\n"
      ],
      "id": "c375adbb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "EXAMPLE_DIALOGUE = \"\"\"ALICE: Are we still meeting at 3?\n",
        "BOB: Yes, but I will be 10 minutes late.\n",
        "ALICE: Ok. Please bring the slides.\n",
        "BOB: Will do.\"\"\"\n",
        "\n",
        "EXAMPLE_SUMMARY = \"Alice and Bob confirm a 3 pm meeting. Bob will arrive 10 minutes late and will bring the slides.\"\n",
        "\n",
        "ONE_SHOT_PROMPT = f\"\"\"Summarize the conversation in 1 to 2 sentences. Do not invent facts.\n",
        "\n",
        "Example.\n",
        "DIALOGUE:\n",
        "{EXAMPLE_DIALOGUE}\n",
        "\n",
        "SUMMARY:\n",
        "{EXAMPLE_SUMMARY}\n",
        "\n",
        "Now summarize this dialogue.\n",
        "\"\"\"\n",
        "\n",
        "llm_one = generate_summary_t5(sample, ONE_SHOT_PROMPT, max_new_tokens=120, temperature=0.0)\n",
        "print(llm_one)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d9939ace"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Generation parameters. Temperature and length\n",
        "\n",
        "Temperature can change factuality. Length controls how much detail you get.\n",
        "\n",
        "Use the sliders if available. Otherwise, edit the numbers and rerun.\n"
      ],
      "id": "c395098e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def demo_generation_controls(temperature: float = 0.0, max_new_tokens: int = 80):\n",
        "    prompt = build_prompt(style=\"neutral\", focus=\"actions\", max_sentences=2)\n",
        "    out = generate_summary_t5(sample, prompt, max_new_tokens=max_new_tokens, temperature=temperature, top_p=0.95)\n",
        "    print(\"temperature:\", temperature, \"max_new_tokens:\", max_new_tokens)\n",
        "    print(out)\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "    if tokenizer is None or model is None:\n",
        "        raise RuntimeError(\"Model not available, skipping widgets.\")\n",
        "    ui = widgets.interactive(\n",
        "        demo_generation_controls,\n",
        "        temperature=widgets.FloatSlider(min=0.0, max=1.0, step=0.1, value=0.0),\n",
        "        max_new_tokens=widgets.IntSlider(min=30, max=200, step=10, value=80),\n",
        "    )\n",
        "    display(ui)\n",
        "except Exception:\n",
        "    demo_generation_controls(temperature=0.0, max_new_tokens=80)\n",
        "    demo_generation_controls(temperature=0.7, max_new_tokens=120)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a552b490"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary and Evaluation Framework\n",
        "\n",
        "In this session, we've built a complete evaluation framework using ROUGE metrics and systematic comparison methods.\n",
        "\n",
        "In production applications, you should also perform human evaluation including factuality checks, completeness assessment, and speaker attribution accuracy.\n"
      ],
      "id": "ed7860c7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"ğŸ“Š SESSION 1 EVALUATION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Reference Summary: \\\"{REFERENCE_SUMMARY}\\\"\")\n",
        "print(f\"TextRank Summary: \\\"{baseline_summary}\\\"\")\n",
        "print()\n",
        "\n",
        "# Evaluate our TextRank baseline\n",
        "baseline_scores = rouge_scores(baseline_summary, REFERENCE_SUMMARY)\n",
        "print(\"ğŸ¯ TEXTRANK PERFORMANCE:\")\n",
        "for metric, score in baseline_scores.items():\n",
        "    print(f\"  {metric.upper()}: {score:.3f}\")\n",
        "\n",
        "print()\n",
        "print(\"âœ… SESSION 1 COMPLETE!\")\n",
        "print(\"ğŸ¯ You've built a robust baseline system that:\")\n",
        "print(\"  â€¢ Works without large language models\")\n",
        "print(\"  â€¢ Handles multiple languages\") \n",
        "print(\"  â€¢ Processes noisy, real-world text\")\n",
        "print(\"  â€¢ Provides systematic evaluation\")\n",
        "print()\n",
        "print(\"ğŸ“š NEXT: Session 2 covers LLM-based approaches!\")\n",
        "print(\"  â€¢ Zero-shot and few-shot prompting\")\n",
        "print(\"  â€¢ Generation parameter tuning\")\n",
        "print(\"  â€¢ Advanced prompt engineering\")\n",
        "print(\"  â€¢ LLM vs baseline comparisons\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a176f348"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Low resource mode. Make English behave like a low resource language\n",
        "\n",
        "Low resource usually means one or more of the following.\n",
        "- Very little labeled data.\n",
        "- Limited tools for tokenization, sentence splitting, and normalization.\n",
        "- Domain mismatch. Your data looks different from what models saw during pre training.\n",
        "- Orthography variation and borrowing, including code switching.\n",
        "\n",
        "We will simulate these constraints in English by.\n",
        "1) Reducing the available context.\n",
        "2) Corrupting the text with noise and inconsistent spelling.\n",
        "3) Removing punctuation, which hurts naive sentence splitting.\n",
        "\n",
        "Then we apply strategies that transfer to true low resource settings.\n"
      ],
      "id": "d90e6fb1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def low_resource_corrupt(text: str, drop_punct_prob: float = 0.5, typo_prob: float = 0.08) -> str:\n",
        "    rng = random.Random(842)\n",
        "    out_chars = []\n",
        "    for ch in text:\n",
        "        if ch in \".?!,\" and rng.random() < drop_punct_prob:\n",
        "            continue\n",
        "        if ch.isalpha() and rng.random() < typo_prob:\n",
        "            if rng.random() < 0.5:\n",
        "                out_chars.append(ch.swapcase())\n",
        "            else:\n",
        "                out_chars.append(chr(((ord(ch.lower()) - 97 + 1) % 26) + 97))\n",
        "        else:\n",
        "            out_chars.append(ch)\n",
        "    return \"\".join(out_chars)\n",
        "\n",
        "low_text = low_resource_corrupt(sample, drop_punct_prob=0.8, typo_prob=0.05)\n",
        "print(low_text[:600])\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f19d5077"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Baseline on clean text:\")\n",
        "print(textrank_summarize(sample, max_sentences=2))\n",
        "print(\"\\nBaseline on low resource corrupted text:\")\n",
        "print(textrank_summarize(low_text, max_sentences=2))\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "60dc3964"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Strategy toolkit\n",
        "\n",
        "Here are practical tactics that often help in low resource dialogue summarization.\n",
        "\n",
        "1. Normalize input.\n",
        "   - Fix common punctuation issues.\n",
        "   - Normalize whitespace.\n",
        "   - Normalize speaker labels.\n",
        "\n",
        "2. Use robust segmentation.\n",
        "   - If sentence splitting fails, summarize at turn level.\n",
        "\n",
        "3. Constrain generation.\n",
        "   - Use explicit length limits.\n",
        "   - Instruct the model to preserve names, numbers, and decisions.\n",
        "\n",
        "4. Add lightweight context.\n",
        "   - Provide a glossary of names and places.\n",
        "   - Provide a domain hint, such as \"family conversation\" or \"customer support\".\n",
        "\n",
        "5. Evaluate with targeted checks.\n",
        "   - Did we preserve who wants to marry whom.\n",
        "   - Did we hallucinate actions that never happened.\n",
        "\n",
        "We will implement 1 and 2 now.\n"
      ],
      "id": "df866059"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def normalize_dialogue(text: str) -> str:\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"([A-Z][A-Z\\s'\\-]+:)\\s*\", r\"\\n\\1 \", text)\n",
        "    return text.strip()\n",
        "\n",
        "def turn_level_summarize(dialogue_text: str, max_turns: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Extractive turn level summarization, more robust than sentence splitting.\n",
        "    \"\"\"\n",
        "    lines = [ln.strip() for ln in dialogue_text.splitlines() if ln.strip()]\n",
        "    lines = [ln for ln in lines if len(ln) > 10]\n",
        "    if not lines:\n",
        "        return \"\"\n",
        "    if len(lines) <= max_turns:\n",
        "        return \" \".join(lines)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "    X = vectorizer.fit_transform(lines)\n",
        "    sim = cosine_similarity(X)\n",
        "    np.fill_diagonal(sim, 0.0)\n",
        "    graph = nx.from_numpy_array(sim)\n",
        "    scores = nx.pagerank(graph, max_iter=200)\n",
        "    ranked = sorted(range(len(lines)), key=lambda i: scores.get(i, 0.0), reverse=True)\n",
        "    picked = sorted(ranked[:max_turns])\n",
        "    return \" \".join([lines[i] for i in picked])\n",
        "\n",
        "print(\"Before normalization:\\n\", low_text[:250], \"\\n\")\n",
        "norm_low = normalize_dialogue(low_text)\n",
        "print(\"After normalization:\\n\", norm_low[:250])\n",
        "print(\"\\nTurn level summary on corrupted text:\\n\", turn_level_summarize(norm_low, max_turns=3))\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0a203e30"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Low resource prompting\n",
        "\n",
        "If you can use an instruction model, you can push it to behave better on noisy input. The key is to add constraints.\n",
        "\n",
        "We will.\n",
        "- Ask for short output.\n",
        "- Ask it to avoid inventing facts.\n",
        "- Ask it to preserve names.\n"
      ],
      "id": "0a8837fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "LOW_RESOURCE_PROMPT = \"\"\"Summarize the conversation in 1 sentence.\n",
        "Rules.\n",
        "1) Do not invent facts.\n",
        "2) Preserve names exactly as they appear.\n",
        "3) If the text is noisy, infer only what is obvious.\"\"\"\n",
        "\n",
        "if tokenizer is None or model is None:\n",
        "    print(\"Model not available, skipping.\")\n",
        "else:\n",
        "    print(generate_summary_t5(norm_low, LOW_RESOURCE_PROMPT, max_new_tokens=60, temperature=0.0))\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e8bd8227"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional mini dataset hook. Try a real non-English case in two minutes\n",
        "\n",
        "This workshop is designed to start in English, then transfer the same workflow to a low-resource language.\n",
        "\n",
        "Below are two quick options.\n",
        "\n",
        "1. **MiniLux micro-set (synthetic)**. A small set of short Luxembourgish and LU, FR mixed snippets created for teaching. It is intentionally tiny and imperfect, so that you can iterate fast and discuss typical issues, like code-switching, named entities, and spelling variation.\n",
        "\n",
        "2. **Hugging Face low-resource sample (real text)**. Pull 20 examples from a multilingual summarization dataset and run the same prompt, plus the same evaluation, to see how performance changes outside English.\n"
      ],
      "id": "57f285af"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Option 1. MiniLux micro-set (synthetic)\n",
        "# This is only for the workshop. You can replace it with your own low-resource dialogues later.\n",
        "\n",
        "mini_lux = [\n",
        "    {\n",
        "        \"id\": \"lux_001\",\n",
        "        \"dialogue\": \"A: Moien. Hues du ZÃ¤it fir e Kaffi?\\nB: Jo, mÃ¤ just zÃ©ng Minutten. Ech muss glÃ¤ich op d'Aarbecht.\\nA: Ok. Mir treffen eis beim Gare.\\nB: Super, ech kommen direkt.\",\n",
        "        \"reference_summary_en\": \"They agree to meet for a quick coffee at the station before B goes to work.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_002\",\n",
        "        \"dialogue\": \"A: WÃ©i war d'Reunioun haut?\\nB: Ganz laang. Mir hu just d'Agenda diskutÃ©iert.\\nA: An hu mir eng Decisioun?\\nB: Nee, mir maachen et nÃ¤chste Woch nach eng KÃ©ier.\",\n",
        "        \"reference_summary_en\": \"The meeting was long, they only discussed the agenda, and no decision was made.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_003\",\n",
        "        \"dialogue\": \"A: Kanns du mer de Rapport schÃ©cken?\\nB: Jo. Ech schÃ©cken en elo per Mail.\\nA: Merci. Ech muss en nach haut ofginn.\\nB: Kloer, ech maachen et direkt.\",\n",
        "        \"reference_summary_en\": \"B will email A the report immediately because A must submit it today.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_004\",\n",
        "        \"dialogue\": \"A: Ech sinn am Stau op der A6.\\nB: Ok, dann fÃ¤nke mir ouni dech un.\\nA: Gitt mir zÃ©ng Minutten.\\nB: Passt. Mir halen dir e SÃ«tz frÃ¤i.\",\n",
        "        \"reference_summary_en\": \"A is stuck in traffic but will arrive in about ten minutes, and the others will start and save a seat.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_005\",\n",
        "        \"dialogue\": \"A: Ech hu muer en rendez-vous chez le mÃ©decin.\\nB: Bass du ok?\\nA: Jo, just e Check-up.\\nB: Ok, soen mer dono wÃ©i et gaangen ass.\",\n",
        "        \"reference_summary_en\": \"A has a doctor appointment tomorrow for a check-up and will update B afterward.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_006\",\n",
        "        \"dialogue\": \"A: Wou si mir mam Projet?\\nB: Mir hu 80 Prozent fÃ¤erdeg.\\nA: Wat feelt nach?\\nB: D'Dokumentatioun an d'Tester.\",\n",
        "        \"reference_summary_en\": \"The project is about 80 percent done, but documentation and testing are still missing.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_007\",\n",
        "        \"dialogue\": \"A: Ech krÃ©ien Ã«mmer eng Fehlermeldung.\\nB: WÃ©i eng?\\nA: 'Permission denied'.\\nB: Dann hues du wahrscheinlech keng Rechter. ProbÃ©ier et mat sudo oder fro den Admin.\",\n",
        "        \"reference_summary_en\": \"A gets a permission error, and B suggests using sudo or asking the admin for access.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_008\",\n",
        "        \"dialogue\": \"A: Mir treffen eis um 14:00.\\nB: Ech sinn um 14:15 do.\\nA: Ok, ech waarden am CafÃ©.\\nB: Merci. Bis glÃ¤ich.\",\n",
        "        \"reference_summary_en\": \"They planned to meet at 14:00, but B will arrive at 14:15 and A will wait at a cafÃ©.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_009\",\n",
        "        \"dialogue\": \"A: Hues du d'Presentatioun gesinn?\\nB: Jo, si ass gutt, mÃ¤ d'Grafike sinn ze kleng.\\nA: Ok, ech maachen se mÃ©i grouss.\\nB: Super, dann ass et perfekt.\",\n",
        "        \"reference_summary_en\": \"B thinks the presentation is good but the charts are too small, so A will enlarge them.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_010\",\n",
        "        \"dialogue\": \"A: Ech sinn haut am Homeoffice.\\nB: Ok, kÃ«nns du trotzdem an de Call?\\nA: Jo, ech sinn do um 10:00.\\nB: Top, ech schÃ©cken de Link.\",\n",
        "        \"reference_summary_en\": \"A works from home but will join the 10:00 call, and B will send the link.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_011\",\n",
        "        \"dialogue\": \"A: Mir brauche nach e Beispill fir d'Course.\\nB: Wat fir ee Beispill?\\nA: E klengt Dialog-Set fir Zesummefaassung.\\nB: Ok, ech schreiwen 20 kuerz Dialogen.\",\n",
        "        \"reference_summary_en\": \"They need a small dialogue dataset for a summarization course, and B will write 20 short dialogues.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_012\",\n",
        "        \"dialogue\": \"A: Kanns du den Text nach eng KÃ©ier kontrollÃ©ieren?\\nB: Jo, ech kucken no Tippfeeler.\\nA: An och Punktuatioun.\\nB: Maachen ech.\",\n",
        "        \"reference_summary_en\": \"B will proofread the text for typos and punctuation.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_013\",\n",
        "        \"dialogue\": \"A: Ech hu meng SchlÃ«sselen vergiess.\\nB: Wou bass du?\\nA: Virun der Dier.\\nB: Ech kommen, ginn mer fÃ«nnef Minutten.\",\n",
        "        \"reference_summary_en\": \"A forgot their keys and is locked out, and B will come in five minutes.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_014\",\n",
        "        \"dialogue\": \"A: De Bus kÃ«nnt net.\\nB: Hues du d'App gekuckt?\\nA: Jo, et steet 'retard'.\\nB: Dann huele mir en Taxi.\",\n",
        "        \"reference_summary_en\": \"The bus is delayed, so they decide to take a taxi.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_015\",\n",
        "        \"dialogue\": \"A: Ech muss nach d'Fichieren eroplueden.\\nB: Wou?\\nA: Op Hugging Face.\\nB: Ok, vergiss net d'Lizens an d'Readme.\",\n",
        "        \"reference_summary_en\": \"A needs to upload files to Hugging Face, and B reminds them to include a license and README.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_016\",\n",
        "        \"dialogue\": \"A: D'GPU ass frÃ¤i.\\nB: Super, dann starte mir den Training.\\nA: Ech setzen batch size op 4.\\nB: Ok, da maache mir gradient accumulation.\",\n",
        "        \"reference_summary_en\": \"They have GPU availability and will start training with a small batch size and gradient accumulation.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_017\",\n",
        "        \"dialogue\": \"A: Kanns du mir den Deadline soen?\\nB: Et ass Freideg um 18:00.\\nA: Merci, ech maachen et haut nach.\\nB: Gutt Iddi.\",\n",
        "        \"reference_summary_en\": \"The deadline is Friday at 18:00, and A plans to finish today.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_018\",\n",
        "        \"dialogue\": \"A: Ech hunn d'DonnÃ©eÃ«n gereinegt.\\nB: Super. Hues du och d'Nummeren normalisÃ©iert?\\nA: Jo, ech hunn se an Wierder Ã«mgewandelt.\\nB: Perfekt.\",\n",
        "        \"reference_summary_en\": \"A cleaned the data and normalized numbers by converting them into words.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_019\",\n",
        "        \"dialogue\": \"A: Ech verstinn d'Resultater net.\\nB: Wat ass komesch?\\nA: D'Accuracy ass hÃ©ich, mÃ¤ d'F1 ass niddreg.\\nB: Dann ass et wahrscheinlech Klassen-Imbalance.\",\n",
        "        \"reference_summary_en\": \"Accuracy is high but F1 is low, suggesting class imbalance.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_020\",\n",
        "        \"dialogue\": \"A: Tu peux me rappeler le plan?\\nB: Oui. D'abord on teste en anglais, aprÃ¨s on passe au luxembourgeois.\\nA: An de Prompt bleift Ã¤hnlech.\\nB: Genau.\",\n",
        "        \"reference_summary_en\": \"They will test in English first, then switch to Luxembourgish while keeping a similar prompt.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_021\",\n",
        "        \"dialogue\": \"A: Ech sinn net sÃ©cher ob 'Zentrum' richteg ass.\\nB: Et hÃ¤nkt vum Dialektgebiet of.\\nA: Ok, ech kontrollÃ©ieren d'Metadata.\\nB: Gutt, d'Labels mussen konsistent sinn.\",\n",
        "        \"reference_summary_en\": \"They will verify the metadata because dialect labels must be consistent.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_022\",\n",
        "        \"dialogue\": \"A: D'Audio ass ze laang.\\nB: WÃ©i laang?\\nA: 25 Sekonnen.\\nB: Dann schneiden mir et op 10 Sekonnen fir d'Training.\",\n",
        "        \"reference_summary_en\": \"The audio is 25 seconds long, so they will trim it to 10 seconds for training.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_023\",\n",
        "        \"dialogue\": \"A: Ech hu keng Internet um Laptop.\\nB: ProbÃ©ier d'WLAN nei.\\nA: Ok, ech maachen restart.\\nB: Wann et net geet, huele mir en Hotspot.\",\n",
        "        \"reference_summary_en\": \"A has no internet, B suggests restarting Wi-Fi, and they may use a hotspot if needed.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_024\",\n",
        "        \"dialogue\": \"A: D'Zesummefaassung ass ze laang.\\nB: Setz eng Limit.\\nA: WÃ©i vill?\\nB: ProbÃ©ier 2 SÃ¤tz an maximal 60 Wierder.\",\n",
        "        \"reference_summary_en\": \"They will constrain the summary length to two sentences and at most 60 words.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"lux_025\",\n",
        "        \"dialogue\": \"A: Ech wÃ«ll eng neutral Zesummefaassung.\\nB: Da schreiwe mir am Prompt: 'neutral, factual, no opinion'.\\nA: Ok, ech testen dat.\\nB: Gutt, a kuck ob Bias kÃ«nnt.\",\n",
        "        \"reference_summary_en\": \"They want a neutral factual summary and will encode that in the prompt and then test for bias.\"\n",
        "    },\n",
        "]\n",
        "\n",
        "def sample_and_summarize(dialogue_set, k=1, seed=7, prompt=ZERO_SHOT_PROMPT):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    items = random.sample(dialogue_set, k=k)\n",
        "    for ex in items:\n",
        "        print(\"ID:\", ex[\"id\"])\n",
        "        print(\"\\nDIALOGUE:\\n\", ex[\"dialogue\"])\n",
        "        pred = generate_summary_t5(ex[\"dialogue\"], prompt=prompt, max_new_tokens=80, temperature=0.0)\n",
        "        print(\"\\nMODEL SUMMARY:\\n\", pred)\n",
        "        print(\"\\nREFERENCE (EN):\\n\", ex[\"reference_summary_en\"])\n",
        "        print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
        "\n",
        "sample_and_summarize(mini_lux, k=2)\n",
        "\n",
        "# Option 2. Pull a tiny real low-resource sample from Hugging Face\n",
        "# This uses XL-Sum (multilingual news summarization). Not a dialogue dataset.\n",
        "# For the workshop, we convert each article into a \"pseudo-dialogue\" so we can reuse the same pipeline.\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "def article_to_pseudo_dialogue(article_text: str, max_turns: int = 6) -> str:\n",
        "    # Lightweight sentence split. Good enough for teaching.\n",
        "    sentences = [s.strip() for s in article_text.replace(\"\\n\", \" \").split(\".\") if s.strip()]\n",
        "    sentences = sentences[:max_turns]\n",
        "    turns = []\n",
        "    for i, s in enumerate(sentences):\n",
        "        speaker = \"ANCHOR\" if i % 2 == 0 else \"REPORTER\"\n",
        "        turns.append(f\"{speaker}: {s}.\")\n",
        "    return \"\\n\".join(turns)\n",
        "\n",
        "def load_low_resource_hf_sample(language_subset: str = \"yoruba\", n: int = 20):\n",
        "    ds = load_dataset(\"csebuetnlp/xlsum\", language_subset, split=f\"train[:{n}]\")\n",
        "    # XL-Sum fields are typically: \"text\" and \"summary\"\n",
        "    out = []\n",
        "    for i, ex in enumerate(ds):\n",
        "        dialogue = article_to_pseudo_dialogue(ex[\"text\"], max_turns=8)\n",
        "        out.append(\n",
        "            {\n",
        "                \"id\": f\"xlsum_{language_subset}_{i:03d}\",\n",
        "                \"dialogue\": dialogue,\n",
        "                \"reference_summary\": ex[\"summary\"],\n",
        "            }\n",
        "        )\n",
        "    return out\n",
        "\n",
        "xlsum_yoruba = load_low_resource_hf_sample(language_subset=\"yoruba\", n=5)\n",
        "print(\"Example pseudo-dialogue from XL-Sum (yoruba subset):\")\n",
        "print(xlsum_yoruba[0][\"dialogue\"])\n",
        "print(\"\\nReference summary (yoruba):\")\n",
        "print(xlsum_yoruba[0][\"reference_summary\"])\n",
        "\n",
        "print(\"\\nNow run the same English prompt on the pseudo-dialogue. It will usually struggle, and that is the point.\")\n",
        "pred = generate_summary_t5(xlsum_yoruba[0][\"dialogue\"], prompt=ZERO_SHOT_PROMPT, max_new_tokens=80, temperature=0.0)\n",
        "print(\"\\nMODEL SUMMARY:\\n\", pred)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "33ca8b91"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¨ Enhanced Section: Visual TextRank Algorithm Explanation\n",
        "\n",
        "### ğŸ“Š TextRank Algorithm Visualization\n",
        "\n",
        "Let's understand how TextRank works with a **visual step-by-step** breakdown!\n"
      ],
      "id": "b01f2067"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "ğŸ¨ TEXTRANK VISUALIZATION CELL\n",
        "\n",
        "This cell creates interactive visualizations to help understand\n",
        "the TextRank algorithm step-by-step.\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from matplotlib.patches import FancyBboxPatch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def create_textrank_visualization():\n",
        "    \"\"\"Create comprehensive TextRank visualization\"\"\"\n",
        "    \n",
        "    print(\"ğŸ¨ TEXTRANK ALGORITHM: Step-by-Step Visualization\")\n",
        "    print(\"=\"*60)\n",
        "    print()\n",
        "    \n",
        "    # Sample dialogue for demonstration\n",
        "    sample_dialogue = [\n",
        "        \"Jack arrives and meets Lane at the house.\",\n",
        "        \"Lane tells Jack that Algernon is waiting in the dining room.\", \n",
        "        \"Jack says he must see Algernon immediately.\",\n",
        "        \"Algernon greets Jack and asks why he came to town.\",\n",
        "        \"Jack responds that he came for pleasure.\",\n",
        "        \"Algernon suggests Jack came to discuss his cousin.\"\n",
        "    ]\n",
        "    \n",
        "    print(\"ğŸ“ Step 1: Input Sentences\")\n",
        "    print(\"-\" * 30)\n",
        "    for i, sent in enumerate(sample_dialogue, 1):\n",
        "        print(f\"{i}. {sent}\")\n",
        "    \n",
        "    print(\"\\nğŸ“Š Step 2: Create TF-IDF Vectors\")\n",
        "    print(\"-\" * 35)\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(sample_dialogue)\n",
        "    print(f\"âœ… Created {tfidf_matrix.shape[0]} sentence vectors\")\n",
        "    print(f\"âœ… Vector dimension: {tfidf_matrix.shape[1]} features\")\n",
        "    \n",
        "    # Calculate similarity matrix\n",
        "    print(\"\\nğŸ”— Step 3: Calculate Similarity Matrix\")\n",
        "    print(\"-\" * 40)\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "    np.fill_diagonal(similarity_matrix, 0)  # Remove self-similarity\n",
        "    \n",
        "    print(\"Similarity Matrix (first 3x3):\")\n",
        "    print(similarity_matrix[:3, :3].round(3))\n",
        "    \n",
        "    # Create and visualize network graph\n",
        "    print(\"\\nğŸ•¸ï¸ Step 4: Build Sentence Similarity Graph\") \n",
        "    print(\"-\" * 45)\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('ğŸ¨ TextRank Algorithm Visualization', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Plot 1: Similarity Matrix Heatmap\n",
        "    im1 = ax1.imshow(similarity_matrix, cmap='Blues', aspect='auto')\n",
        "    ax1.set_title('ğŸ“Š Sentence Similarity Matrix', fontweight='bold')\n",
        "    ax1.set_xlabel('Sentence Index')\n",
        "    ax1.set_ylabel('Sentence Index')\n",
        "    \n",
        "    # Add text annotations\n",
        "    for i in range(len(sample_dialogue)):\n",
        "        for j in range(len(sample_dialogue)):\n",
        "            text = ax1.text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
        "                          ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
        "    \n",
        "    # Plot 2: Network Graph\n",
        "    G = nx.from_numpy_array(similarity_matrix)\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "    \n",
        "    # Draw network\n",
        "    nx.draw(G, pos, ax=ax2, with_labels=True, node_color='lightblue',\n",
        "           node_size=800, font_size=10, font_weight='bold',\n",
        "           edge_color='gray', width=2)\n",
        "    ax2.set_title('ğŸ•¸ï¸ Sentence Similarity Network', fontweight='bold')\n",
        "    \n",
        "    # Plot 3: PageRank Scores\n",
        "    pagerank_scores = nx.pagerank(G)\n",
        "    sentences = [f'S{i+1}' for i in range(len(sample_dialogue))]\n",
        "    scores = list(pagerank_scores.values())\n",
        "    \n",
        "    bars = ax3.bar(sentences, scores, color='orange', alpha=0.7)\n",
        "    ax3.set_title('ğŸ“ˆ PageRank Scores', fontweight='bold')\n",
        "    ax3.set_ylabel('PageRank Score')\n",
        "    ax3.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Highlight top sentences\n",
        "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:2]\n",
        "    for i in top_indices:\n",
        "        bars[i].set_color('red')\n",
        "        bars[i].set_alpha(0.9)\n",
        "    \n",
        "    # Plot 4: Final Summary Selection\n",
        "    ax4.axis('off')\n",
        "    ax4.text(0.1, 0.8, 'ğŸ¯ FINAL SUMMARY:', fontsize=14, fontweight='bold', transform=ax4.transAxes)\n",
        "    \n",
        "    # Show top sentences\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        sentence_text = sample_dialogue[idx][:50] + \"...\" if len(sample_dialogue[idx]) > 50 else sample_dialogue[idx]\n",
        "        ax4.text(0.1, 0.6 - i*0.15, f'â€¢ {sentence_text}', \n",
        "                fontsize=11, transform=ax4.transAxes, wrap=True)\n",
        "        ax4.text(0.05, 0.6 - i*0.15, f'#{idx+1}', fontsize=11, fontweight='bold',\n",
        "                color='red', transform=ax4.transAxes)\n",
        "    \n",
        "    ax4.text(0.1, 0.2, 'ğŸ’¡ These sentences have highest centrality!', \n",
        "             fontsize=12, fontweight='bold', color='green', transform=ax4.transAxes)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nğŸ¯ ALGORITHM SUMMARY:\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"1. ğŸ“ Split dialogue into sentences\")\n",
        "    print(\"2. ğŸ“Š Convert sentences to TF-IDF vectors\") \n",
        "    print(\"3. ğŸ”— Calculate cosine similarity between all pairs\")\n",
        "    print(\"4. ğŸ•¸ï¸ Build graph where edges = similarity scores\")\n",
        "    print(\"5. ğŸ“ˆ Apply PageRank to find most central sentences\")\n",
        "    print(\"6. ğŸ¯ Select top-ranked sentences as summary\")\n",
        "    \n",
        "    print(f\"\\nğŸ† ACHIEVEMENT UNLOCKED: TextRank Champion! ğŸ¥‡\")\n",
        "    print(\"You now understand the core algorithm behind extractive summarization!\")\n",
        "    \n",
        "    return pagerank_scores, top_indices\n",
        "\n",
        "try:\n",
        "    scores, top_sentences = create_textrank_visualization()\n",
        "    print(\"\\\\nâœ… Visualization complete!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Visualization error: {str(e)}\")\n",
        "    print(\"ğŸ’¡ This might happen in some environments - the algorithm explanation is still valid!\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d2173ec5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š Interactive ROUGE Evaluation Workshop\n",
        "\n",
        "### ğŸ¯ Understanding ROUGE Metrics with Visual Examples\n",
        "\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) can seem abstract. Let's make it concrete with **interactive examples**!\n"
      ],
      "id": "1785d92c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "ğŸ“Š ROUGE EVALUATION WORKSHOP CELL  \n",
        "\n",
        "This cell provides interactive examples to understand ROUGE metrics\n",
        "with visual explanations and hands-on calculation examples.\n",
        "\"\"\"\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def create_rouge_workshop():\n",
        "    \"\"\"Interactive ROUGE metrics workshop with visual explanations\"\"\"\n",
        "    \n",
        "    print(\"ğŸ“Š ROUGE METRICS WORKSHOP\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸ¯ Learn evaluation metrics through interactive examples!\")\n",
        "    print()\n",
        "    \n",
        "    # Example summaries for demonstration\n",
        "    reference = \"Jack arrives at the house and learns that Algernon is waiting for him.\"\n",
        "    \n",
        "    examples = {\n",
        "        \"Perfect Match\": \"Jack arrives at the house and learns that Algernon is waiting for him.\",\n",
        "        \"Good Summary\": \"Jack arrives and learns Algernon is waiting.\", \n",
        "        \"Partial Match\": \"Jack comes to the house where Algernon waits.\",\n",
        "        \"Poor Summary\": \"The weather is nice today and everyone is happy.\",\n",
        "        \"Different Words\": \"John reaches the building and discovers Albert is present.\"\n",
        "    }\n",
        "    \n",
        "    print(\"ğŸ“ REFERENCE SUMMARY:\")\n",
        "    print(f'\"{reference}\"')\n",
        "    print()\n",
        "    \n",
        "    # Create visual comparison\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    \n",
        "    # Calculate scores for all examples\n",
        "    results = []\n",
        "    for name, summary in examples.items():\n",
        "        scores = scorer.score(reference, summary)\n",
        "        results.append({\n",
        "            'name': name,\n",
        "            'summary': summary,\n",
        "            'rouge1': scores['rouge1'].fmeasure,\n",
        "            'rouge2': scores['rouge2'].fmeasure, \n",
        "            'rougeL': scores['rougeL'].fmeasure\n",
        "        })\n",
        "    \n",
        "    # Visual breakdown for each example\n",
        "    print(\"ğŸ” DETAILED ROUGE ANALYSIS:\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"\\nğŸ“‹ Example {i}: {result['name']}\")\n",
        "        print(f\"Summary: \\\"{result['summary']}\\\"\")\n",
        "        print(f\"ROUGE-1: {result['rouge1']:.3f} | ROUGE-2: {result['rouge2']:.3f} | ROUGE-L: {result['rougeL']:.3f}\")\n",
        "        \n",
        "        # Show token overlap for ROUGE-1\n",
        "        ref_tokens = reference.lower().split()\n",
        "        sum_tokens = result['summary'].lower().split()\n",
        "        \n",
        "        overlap = set(ref_tokens) & set(sum_tokens) \n",
        "        print(f\"Word overlap: {overlap}\")\n",
        "        print(\"-\" * 40)\n",
        "    \n",
        "    # Create visualization\n",
        "    print(\"\\\\nğŸ“Š VISUAL COMPARISON:\")\n",
        "    print(\"=\"*30)\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Bar chart of ROUGE scores\n",
        "    names = [r['name'] for r in results]\n",
        "    rouge1_scores = [r['rouge1'] for r in results]\n",
        "    rouge2_scores = [r['rouge2'] for r in results] \n",
        "    rougeL_scores = [r['rougeL'] for r in results]\n",
        "    \n",
        "    x = range(len(names))\n",
        "    width = 0.25\n",
        "    \n",
        "    ax1.bar([i - width for i in x], rouge1_scores, width, label='ROUGE-1', alpha=0.8, color='blue')\n",
        "    ax1.bar(x, rouge2_scores, width, label='ROUGE-2', alpha=0.8, color='orange') \n",
        "    ax1.bar([i + width for i in x], rougeL_scores, width, label='ROUGE-L', alpha=0.8, color='green')\n",
        "    \n",
        "    ax1.set_xlabel('Summary Examples')\n",
        "    ax1.set_ylabel('ROUGE Score')\n",
        "    ax1.set_title('ğŸ† ROUGE Scores Comparison', fontweight='bold')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(names, rotation=45, ha='right')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Token overlap visualization\n",
        "    ref_words = reference.lower().split()\n",
        "    good_summary = \"Jack arrives and learns Algernon is waiting.\"\n",
        "    good_words = good_summary.lower().split()\n",
        "    \n",
        "    # Word overlap diagram\n",
        "    ax2.axis('off')\n",
        "    ax2.set_title('ğŸ” Token Overlap Analysis', fontweight='bold', pad=20)\n",
        "    \n",
        "    # Reference words\n",
        "    ax2.text(0.05, 0.8, 'Reference:', fontweight='bold', fontsize=12, transform=ax2.transAxes)\n",
        "    ref_text = ' '.join(ref_words)\n",
        "    ax2.text(0.05, 0.75, ref_text, fontsize=10, transform=ax2.transAxes, \n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
        "    \n",
        "    # Summary words\n",
        "    ax2.text(0.05, 0.6, 'Summary:', fontweight='bold', fontsize=12, transform=ax2.transAxes)\n",
        "    sum_text = ' '.join(good_words)\n",
        "    ax2.text(0.05, 0.55, sum_text, fontsize=10, transform=ax2.transAxes,\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
        "    \n",
        "    # Overlap\n",
        "    overlap_words = set(ref_words) & set(good_words)\n",
        "    ax2.text(0.05, 0.4, 'Overlapping Words:', fontweight='bold', fontsize=12, transform=ax2.transAxes)\n",
        "    overlap_text = ' '.join(sorted(overlap_words))\n",
        "    ax2.text(0.05, 0.35, overlap_text, fontsize=10, transform=ax2.transAxes,\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\"))\n",
        "    \n",
        "    # Metrics explanation\n",
        "    ax2.text(0.05, 0.2, 'ğŸ“ˆ ROUGE Metrics Explained:', fontweight='bold', fontsize=12, transform=ax2.transAxes)\n",
        "    ax2.text(0.05, 0.15, 'ROUGE-1: Unigram (word) overlap', fontsize=10, transform=ax2.transAxes)\n",
        "    ax2.text(0.05, 0.12, 'ROUGE-2: Bigram (2-word phrase) overlap', fontsize=10, transform=ax2.transAxes)\n",
        "    ax2.text(0.05, 0.09, 'ROUGE-L: Longest common subsequence', fontsize=10, transform=ax2.transAxes)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\\\nğŸ¯ KEY INSIGHTS:\")\n",
        "    print(\"=\"*30)\n",
        "    print(\"âœ… ROUGE-1 measures word overlap (higher = more shared words)\")\n",
        "    print(\"âœ… ROUGE-2 measures phrase overlap (harder to achieve)\")  \n",
        "    print(\"âœ… ROUGE-L measures sequence overlap (order matters)\")\n",
        "    print(\"âœ… Perfect match = 1.0, No overlap = 0.0\")\n",
        "    print(\"âœ… Different words with same meaning = low ROUGE (limitation!)\")\n",
        "    \n",
        "    print(\"\\\\nğŸ† ACHIEVEMENT UNLOCKED: Evaluation Expert! ğŸ’\")\n",
        "    print(\"You now understand how to interpret ROUGE scores!\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "def rouge_quiz():\n",
        "    \"\"\"Quick quiz about ROUGE understanding\"\"\"\n",
        "    print(\"\\\\nğŸ² ROUGE KNOWLEDGE CHECK:\")\n",
        "    print(\"=\"*40)\n",
        "    \n",
        "    questions = [\n",
        "        {\n",
        "            \"q\": \"If a summary has ROUGE-1 = 0.8, what does this mean?\",\n",
        "            \"options\": [\"A) 80% of sentences match\", \"B) 80% of words are shared\", \"C) Summary is 80% correct\"],\n",
        "            \"correct\": 1,\n",
        "            \"explanation\": \"ROUGE-1 measures unigram (word) overlap between reference and summary.\"\n",
        "        },\n",
        "        {\n",
        "            \"q\": \"Why might a good summary have low ROUGE scores?\", \n",
        "            \"options\": [\"A) It's too short\", \"B) Uses different words with same meaning\", \"C) It's in wrong language\"],\n",
        "            \"correct\": 1,\n",
        "            \"explanation\": \"ROUGE only measures word overlap, not semantic similarity. Paraphrases can score low despite being good summaries.\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    for i, quiz_item in enumerate(questions, 1):\n",
        "        print(f\"\\\\nQuestion {i}: {quiz_item['q']}\")\n",
        "        print()  # Add space after question\n",
        "        for option in quiz_item['options']:\n",
        "            print(f\"      {option}\")  # More indentation and spacing\n",
        "            print()  # Add space after each option\n",
        "        print(f\"ğŸ’¡ Answer: {quiz_item['options'][quiz_item['correct']]}\")\n",
        "        print(f\"   Explanation: {quiz_item['explanation']}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Run the workshop\n",
        "try:\n",
        "    results = create_rouge_workshop()\n",
        "    rouge_quiz()\n",
        "    print(\"\\\\nâœ… ROUGE Workshop complete! You're now an evaluation expert! ğŸ“\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Workshop error: {str(e)}\")\n",
        "    print(\"ğŸ’¡ The conceptual explanations above are still valuable!\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "da9695b8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ® Low-Resource Challenges: Survival Game Mode\n",
        "\n",
        "### ğŸƒâ€â™€ï¸ The Challenge: Can Your System Handle Real-World Conditions?\n",
        "\n",
        "Time to test your skills! We'll simulate increasingly difficult **low-resource scenarios** and see how well your TextRank system performs.\n"
      ],
      "id": "514dc5c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "ğŸ® LOW-RESOURCE SURVIVAL GAME\n",
        "\n",
        "This interactive game simulates real-world challenges faced when \n",
        "working with low-resource languages and tests your adaptation strategies.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "\n",
        "def low_resource_survival_game():\n",
        "    \"\"\"Interactive survival game for low-resource dialogue summarization\"\"\"\n",
        "    \n",
        "    print(\"ğŸ® LOW-RESOURCE SURVIVAL GAME\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸ¯ Survive 5 increasingly difficult scenarios!\")\n",
        "    print(\"ğŸ’ª Adapt your TextRank system to handle real-world challenges\")\n",
        "    print()\n",
        "    \n",
        "    # Original clean dialogue\n",
        "    original_dialogue = \"\"\"\n",
        "    JACK: I have not been able to return sooner. I was detained in town.\n",
        "    LANE: I received a message from Mr. Algernon. He will be down at four.\n",
        "    JACK: Is Mr. Algernon here now?\n",
        "    LANE: Yes sir, he is in the dining room.\n",
        "    JACK: I must see him at once.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Challenge scenarios - progressively harder\n",
        "    scenarios = [\n",
        "        {\n",
        "            \"name\": \"ğŸ“± Level 1: Social Media Mode\",\n",
        "            \"description\": \"Informal text with abbreviations and emoji\",\n",
        "            \"corruption\": lambda x: x.lower().replace(\"mr. \", \"\").replace(\"sir\", \"\").replace(\".\", \" lol\"),\n",
        "            \"difficulty\": \"â­\",\n",
        "            \"tip\": \"Handle case variations and informal language\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"ğŸ”¤ Level 2: Missing Punctuation\", \n",
        "            \"description\": \"All punctuation removed (common in speech-to-text)\",\n",
        "            \"corruption\": lambda x: re.sub(r'[^\\w\\s]', '', x),\n",
        "            \"difficulty\": \"â­â­\", \n",
        "            \"tip\": \"Segment sentences without punctuation cues\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"âŒ Level 3: Typo Storm\",\n",
        "            \"description\": \"Random character substitutions simulate OCR errors\",\n",
        "            \"corruption\": lambda x: add_typos(x, error_rate=0.05),\n",
        "            \"difficulty\": \"â­â­â­\",\n",
        "            \"tip\": \"Robust to spelling variations and OCR errors\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"ğŸŒ€ Level 4: Word Order Chaos\",\n",
        "            \"description\": \"Scrambled word order within sentences\",\n",
        "            \"corruption\": lambda x: scramble_words(x),\n",
        "            \"difficulty\": \"â­â­â­â­\",\n",
        "            \"tip\": \"Handle non-standard syntax and word order\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"ğŸ† Level 5: Ultimate Challenge\",\n",
        "            \"description\": \"All corruptions combined - the real world!\",\n",
        "            \"corruption\": lambda x: scramble_words(add_typos(re.sub(r'[^\\w\\s]', '', x.lower()), 0.08)),\n",
        "            \"difficulty\": \"â­â­â­â­â­\",\n",
        "            \"tip\": \"Production-ready robustness test\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    def add_typos(text, error_rate=0.05):\n",
        "        \"\"\"Add random character substitutions\"\"\"\n",
        "        chars = list(text)\n",
        "        for i in range(len(chars)):\n",
        "            if random.random() < error_rate and chars[i].isalpha():\n",
        "                chars[i] = chr(ord(chars[i]) + random.choice([-1, 1]))\n",
        "        return ''.join(chars)\n",
        "    \n",
        "    def scramble_words(text):\n",
        "        \"\"\"Randomly scramble word order in sentences\"\"\"\n",
        "        sentences = text.split('.')\n",
        "        result = []\n",
        "        for sent in sentences:\n",
        "            words = sent.strip().split()\n",
        "            if len(words) > 3:\n",
        "                # Keep first word (often speaker), scramble rest\n",
        "                random.shuffle(words[1:])\n",
        "            result.append(' '.join(words))\n",
        "        return '. '.join(result)\n",
        "    \n",
        "    def robust_textrank(text, max_sentences=2):\n",
        "        \"\"\"Enhanced TextRank that handles corrupted input\"\"\"\n",
        "        try:\n",
        "            # Normalize whitespace\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "            \n",
        "            # Split by speaker or line breaks\n",
        "            lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
        "            if len(lines) < 2:\n",
        "                return text[:100] + \"...\" if len(text) > 100 else text\n",
        "                \n",
        "            # Use TF-IDF with character n-grams for robustness  \n",
        "            vectorizer = TfidfVectorizer(\n",
        "                stop_words='english',\n",
        "                ngram_range=(1, 2),\n",
        "                analyzer='word',\n",
        "                min_df=1,\n",
        "                lowercase=True\n",
        "            )\n",
        "            \n",
        "            try:\n",
        "                X = vectorizer.fit_transform(lines)\n",
        "                sim = cosine_similarity(X)\n",
        "                np.fill_diagonal(sim, 0.0)\n",
        "                \n",
        "                # Build graph and apply PageRank\n",
        "                graph = nx.from_numpy_array(sim)\n",
        "                scores = nx.pagerank(graph, max_iter=100)\n",
        "                \n",
        "                # Select top sentences\n",
        "                ranked = sorted(range(len(lines)), key=lambda i: scores.get(i, 0.0), reverse=True)\n",
        "                picked = sorted(ranked[:max_sentences])\n",
        "                return \" \".join([lines[i] for i in picked])\n",
        "                \n",
        "            except Exception:\n",
        "                # Fallback: return first few lines\n",
        "                return \" \".join(lines[:max_sentences])\n",
        "                \n",
        "        except Exception as e:\n",
        "            return f\"[Extraction failed: {str(e)[:30]}...]\"\n",
        "    \n",
        "    # Game state\n",
        "    score = 0\n",
        "    survival_rate = 100\n",
        "    \n",
        "    print(\"ğŸ¯ BASELINE PERFORMANCE:\")\n",
        "    print(\"Original dialogue:\")\n",
        "    print(original_dialogue.strip())\n",
        "    print()\n",
        "    baseline_summary = robust_textrank(original_dialogue)\n",
        "    print(\"Baseline TextRank summary:\")\n",
        "    print(f'\"{baseline_summary}\"')\n",
        "    print(\"=\"*60)\n",
        "    print()\n",
        "    \n",
        "    # Run challenge scenarios\n",
        "    for i, scenario in enumerate(scenarios, 1):\n",
        "        print(f\"ğŸ® CHALLENGE {i}/5: {scenario['name']} {scenario['difficulty']}\")\n",
        "        print(f\"ğŸ“‹ Task: {scenario['description']}\")\n",
        "        print(f\"ğŸ’¡ Tip: {scenario['tip']}\")\n",
        "        print()\n",
        "        \n",
        "        # Apply corruption\n",
        "        corrupted_text = scenario['corruption'](original_dialogue)\n",
        "        print(\"ğŸ”€ Corrupted input:\")\n",
        "        print(f'\"{corrupted_text[:200]}...\"' if len(corrupted_text) > 200 else f'\"{corrupted_text}\"')\n",
        "        print()\n",
        "        \n",
        "        # Test robust TextRank\n",
        "        try:\n",
        "            summary = robust_textrank(corrupted_text)\n",
        "            print(\"ğŸ¤– Robust TextRank output:\")\n",
        "            print(f'\"{summary}\"')\n",
        "            \n",
        "            # Simple scoring based on length and content preservation\n",
        "            baseline_words = set(baseline_summary.lower().split())\n",
        "            output_words = set(summary.lower().split()) \n",
        "            \n",
        "            if len(summary) < 10:\n",
        "                challenge_score = 0\n",
        "                survival_rate -= 30\n",
        "                print(\"âŒ SYSTEM FAILURE: Output too short!\")\n",
        "            else:\n",
        "                overlap = len(baseline_words & output_words) / len(baseline_words) if baseline_words else 0\n",
        "                challenge_score = min(100, int(overlap * 100))\n",
        "                if challenge_score < 30:\n",
        "                    survival_rate -= 20\n",
        "                elif challenge_score < 60: \n",
        "                    survival_rate -= 10\n",
        "                \n",
        "                print(f\"ğŸ“Š Challenge Score: {challenge_score}/100\")\n",
        "                \n",
        "            score += challenge_score\n",
        "            print(f\"ğŸ’— Survival Rate: {survival_rate}%\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ CRITICAL FAILURE: {str(e)}\")\n",
        "            survival_rate -= 50\n",
        "            print(f\"ğŸ’— Survival Rate: {survival_rate}%\")\n",
        "        \n",
        "        print(\"-\" * 50)\n",
        "        print()\n",
        "        \n",
        "        if survival_rate <= 0:\n",
        "            print(\"ğŸ’€ GAME OVER: System couldn't handle the challenges!\")\n",
        "            print(\"ğŸ’¡ Try improving text normalization and error handling\")\n",
        "            break\n",
        "    \n",
        "    # Final results\n",
        "    print(\"ğŸ FINAL RESULTS:\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"ğŸ“Š Total Score: {score}/500\")\n",
        "    print(f\"ğŸ’— Survival Rate: {max(0, survival_rate)}%\")\n",
        "    print()\n",
        "    \n",
        "    if survival_rate > 70:\n",
        "        achievement = \"ğŸ† LOW-RESOURCE HERO! ğŸ¥‡\"\n",
        "        message = \"Your system is production-ready for challenging conditions!\"\n",
        "    elif survival_rate > 40:\n",
        "        achievement = \"ğŸ¥ˆ SOLID SURVIVOR! ğŸ¥ˆ\"  \n",
        "        message = \"Good robustness - ready for most real-world scenarios\"\n",
        "    elif survival_rate > 0:\n",
        "        achievement = \"ğŸ¥‰ BASIC SURVIVOR ğŸ¥‰\"\n",
        "        message = \"System works but needs improvement for production use\"\n",
        "    else:\n",
        "        achievement = \"ğŸ’€ NEEDS WORK\"\n",
        "        message = \"Focus on error handling and text normalization\"\n",
        "    \n",
        "    print(f\"ğŸ† ACHIEVEMENT: {achievement}\")\n",
        "    print(f\"ğŸ’¬ {message}\")\n",
        "    print()\n",
        "    print(\"ğŸ¯ KEY LESSONS:\")\n",
        "    print(\"âœ… Real-world text is messy - always expect noise!\")\n",
        "    print(\"âœ… Robust preprocessing is crucial for low-resource languages\") \n",
        "    print(\"âœ… Multiple fallback strategies prevent total system failure\")\n",
        "    print(\"âœ… Character-level features help with spelling variations\")\n",
        "    \n",
        "    return score, survival_rate\n",
        "\n",
        "# Start the survival game!\n",
        "print(\"ğŸ® Ready to test your low-resource skills?\")\n",
        "print(\"ğŸ’ª This game will challenge your TextRank system with realistic scenarios\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    final_score, final_survival = low_resource_survival_game()\n",
        "    print(\"\\\\nâœ… Survival game complete!\")\n",
        "    print(f\"ğŸ® You survived with {final_survival}% system integrity!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Game error: {str(e)}\")\n",
        "    print(\"ğŸ’¡ The learning objectives are still valuable - focus on robust preprocessing!\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5f5b3175"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ† Final Achievement Summary & Progress Report\n",
        "\n",
        "### ğŸ‰ Congratulations! You've Completed Session 1!\n",
        "\n",
        "Time to review your accomplishments and prepare for the next challenge!\n"
      ],
      "id": "8fbde419"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "ğŸ† FINAL ACHIEVEMENT TRACKER\n",
        "\n",
        "This cell summarizes your learning journey and achievements\n",
        "throughout Session 1 of the dialogue summarization tutorial.\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def create_achievement_summary():\n",
        "    \"\"\"Generate final achievement summary with visual progress report\"\"\"\n",
        "    \n",
        "    print(\"ğŸ‰ SESSION 1 COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸ® GAMIFIED LEARNING JOURNEY SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print()\n",
        "    \n",
        "    # Achievement system\n",
        "    achievements = {\n",
        "        \"ğŸ¥‰ Setup Master\": {\n",
        "            \"description\": \"Successfully installed all dependencies\", \n",
        "            \"status\": \"âœ… UNLOCKED\",\n",
        "            \"points\": 100,\n",
        "            \"skills\": [\"Environment setup\", \"Dependency management\"]\n",
        "        },\n",
        "        \"ğŸ¥ˆ Language Detective\": {\n",
        "            \"description\": \"Analyzed tokenization across multiple languages\",\n",
        "            \"status\": \"âœ… UNLOCKED\", \n",
        "            \"points\": 200,\n",
        "            \"skills\": [\"Multilingual analysis\", \"Model comparison\"]\n",
        "        },\n",
        "        \"ğŸ¥‡ TextRank Champion\": {\n",
        "            \"description\": \"Built working extractive summarization system\",\n",
        "            \"status\": \"âœ… UNLOCKED\",\n",
        "            \"points\": 300,\n",
        "            \"skills\": [\"Graph algorithms\", \"Extractive summarization\"]\n",
        "        },\n",
        "        \"ğŸ’ Evaluation Expert\": {\n",
        "            \"description\": \"Mastered ROUGE metrics and evaluation\",\n",
        "            \"status\": \"âœ… UNLOCKED\",\n",
        "            \"points\": 250, \n",
        "            \"skills\": [\"ROUGE metrics\", \"Systematic evaluation\"]\n",
        "        },\n",
        "        \"ğŸ… Low-Resource Hero\": {\n",
        "            \"description\": \"Survived the low-resource challenge game\",\n",
        "            \"status\": \"âœ… UNLOCKED\",\n",
        "            \"points\": 400,\n",
        "            \"skills\": [\"Robust preprocessing\", \"Error handling\"]\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(\"ğŸ† ACHIEVEMENTS UNLOCKED:\")\n",
        "    print(\"-\" * 40)\n",
        "    total_points = 0\n",
        "    for title, details in achievements.items():\n",
        "        print(f\"{title}\")\n",
        "        print(f\"   ğŸ“‹ {details['description']}\")\n",
        "        print(f\"   ğŸ¯ Status: {details['status']}\")\n",
        "        print(f\"   â­ Points: {details['points']}\")\n",
        "        print(f\"   ğŸ§  Skills: {', '.join(details['skills'])}\")\n",
        "        total_points += details['points']\n",
        "        print()\n",
        "    \n",
        "    print(f\"ğŸ® TOTAL SCORE: {total_points:,} points\")\n",
        "    print(f\"ğŸ“ MASTERY LEVEL: {'Expert' if total_points > 1000 else 'Advanced' if total_points > 800 else 'Intermediate'}\")\n",
        "    print()\n",
        "    \n",
        "    # Progress visualization\n",
        "    print(\"ğŸ“Š FINAL PROGRESS REPORT:\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"ğŸ“š Session 1 Roadmap - COMPLETED:\")\n",
        "    sections = [\n",
        "        (\"ğŸ› ï¸  Setup & Dependencies\", 100),\n",
        "        (\"ğŸ” Language Analysis\", 100), \n",
        "        (\"ğŸ“ Dialogue Data Creation\", 100),\n",
        "        (\"âš™ï¸  TextRank Implementation\", 100),\n",
        "        (\"ğŸŒ Low-Resource Adaptation\", 100),\n",
        "        (\"ğŸ¯ Final Challenge\", 100)\n",
        "    ]\n",
        "    \n",
        "    for section, progress in sections:\n",
        "        bar = \"â– \" * 5  # Full progress bars\n",
        "        print(f\"â”œâ”€â”€ {section:<25} [{bar}] {progress}% âœ…\")\n",
        "    \n",
        "    print()\n",
        "    print(\"ğŸ¯ KEY TECHNICAL SKILLS MASTERED:\")\n",
        "    print(\"=\"*50)\n",
        "    skills_learned = [\n",
        "        \"ğŸ“Š Data preprocessing and dialogue parsing\",\n",
        "        \"ğŸ•¸ï¸ TextRank algorithm implementation\", \n",
        "        \"ğŸ“ˆ ROUGE evaluation metrics\",\n",
        "        \"ğŸŒ Low-resource adaptation strategies\",\n",
        "        \"ğŸ”§ Robust error handling techniques\",\n",
        "        \"ğŸ“‹ Systematic evaluation frameworks\",\n",
        "        \"ğŸ¨ Interactive visualization techniques\",\n",
        "        \"ğŸ® Gamified learning methodology\"\n",
        "    ]\n",
        "    \n",
        "    for skill in skills_learned:\n",
        "        print(f\"âœ… {skill}\")\n",
        "    \n",
        "    # Create visual progress chart\n",
        "    print(\"\\\\nğŸ“ˆ VISUAL PROGRESS SUMMARY:\")\n",
        "    print(\"=\"*40)\n",
        "    \n",
        "    try:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "        \n",
        "        # Achievement points pie chart\n",
        "        names = list(achievements.keys())\n",
        "        points = [details['points'] for details in achievements.values()]\n",
        "        colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#FF99CC']\n",
        "        \n",
        "        ax1.pie(points, labels=[name.split()[-1] for name in names], colors=colors, \n",
        "               autopct='%1.0f%%', startangle=90)\n",
        "        ax1.set_title('ğŸ† Achievement Points Distribution', fontweight='bold', pad=20)\n",
        "        \n",
        "        # Skills progression bar chart\n",
        "        skill_categories = ['Data Processing', 'Algorithm Implementation', 'Evaluation', \n",
        "                          'Multilingual Analysis', 'Robustness']\n",
        "        skill_scores = [95, 90, 88, 85, 92]  # Simulated scores\n",
        "        \n",
        "        bars = ax2.barh(skill_categories, skill_scores, color=['#4CAF50', '#2196F3', '#FF9800', '#9C27B0', '#F44336'])\n",
        "        ax2.set_xlabel('Mastery Level (%)')\n",
        "        ax2.set_title('ğŸ§  Skill Mastery Assessment', fontweight='bold', pad=20)\n",
        "        ax2.set_xlim(0, 100)\n",
        "        \n",
        "        # Add score labels\n",
        "        for i, bar in enumerate(bars):\n",
        "            width = bar.get_width()\n",
        "            ax2.text(width + 1, bar.get_y() + bar.get_height()/2, \n",
        "                    f'{skill_scores[i]}%', ha='left', va='center', fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"ğŸ“Š Visualization skipped: {str(e)}\")\n",
        "    \n",
        "    print(\"\\\\nğŸš€ WHAT'S NEXT?\")\n",
        "    print(\"=\"*30)\n",
        "    print(\"ğŸ“ Session 1: Complete! âœ…\")\n",
        "    print(\"ğŸ“š Session 2: Prompt Engineering with LLMs\")\n",
        "    print(\"ğŸ“š Session 3: Fine-tuning LLMs for Low-Resource Languages\")  \n",
        "    print(\"ğŸ“š Session 4: Bias Audit and Mitigation Techniques\")\n",
        "    print()\n",
        "    print(\"ğŸ’¡ UPCOMING SESSIONS PREVIEW:\")\n",
        "    print(\"ğŸ“š Session 2: Prompt Engineering with LLMs\")\n",
        "    print(\"- ğŸ¤– Large language model prompting techniques\")\n",
        "    print(\"- ğŸ¯ Zero-shot and few-shot learning\")\n",
        "    print(\"- ğŸ”— Chain-of-thought reasoning\")\n",
        "    print(\"- ğŸŒ Cross-lingual transfer learning\")\n",
        "    print()\n",
        "    print(\"ğŸ“š Session 3: Fine-tuning LLMs for Low-Resource Languages\")\n",
        "    print(\"- ğŸ”§ Parameter-efficient fine-tuning (LoRA, adapters)\")\n",
        "    print(\"- ğŸ“Š Data augmentation for low-resource scenarios\")\n",
        "    print(\"- ğŸ¯ Domain adaptation techniques\")\n",
        "    print(\"- ğŸ’¾ Model compression and deployment\")\n",
        "    print()\n",
        "    print(\"ğŸ“š Session 4: Bias Audit and Mitigation Techniques\")\n",
        "    print(\"- ğŸ” Detecting bias in dialogue summarization\")\n",
        "    print(\"- âš–ï¸ Fairness metrics and evaluation frameworks\")\n",
        "    print(\"- ğŸ›¡ï¸ Mitigation strategies and interventions\")\n",
        "    print(\"- ğŸŒ Cultural sensitivity in multilingual systems\")\n",
        "    print()\n",
        "    print(\"ğŸ‰ CONGRATULATIONS!\")\n",
        "    print(\"You're now equipped with robust baseline techniques for\")\n",
        "    print(\"dialogue summarization in any language!\")\n",
        "    \n",
        "    return total_points, achievements\n",
        "\n",
        "def celebration_message():\n",
        "    \"\"\"Fun celebration message\"\"\"\n",
        "    print(\"\\\\n\" + \"ğŸ‰\" * 20)\n",
        "    print(\"ğŸŠ LEARNING CELEBRATION! ğŸŠ\")  \n",
        "    print(\"ğŸ‰\" * 20)\n",
        "    print(\"\\\\nğŸŒŸ You've transformed from a beginner to a dialogue\")\n",
        "    print(\"   summarization expert using low-resource techniques!\")\n",
        "    print(\"\\\\nğŸ’ª You can now handle:\")\n",
        "    print(\"   â€¢ Noisy, imperfect text data\")\n",
        "    print(\"   â€¢ Multiple languages and writing systems\") \n",
        "    print(\"   â€¢ Resource-constrained environments\")\n",
        "    print(\"   â€¢ Production deployment challenges\")\n",
        "    print(\"\\\\nğŸš€ Ready to tackle LLMs in Session 2!\")\n",
        "    print(\"ğŸ¯ Your foundation in TextRank will make LLM techniques\")  \n",
        "    print(\"   even more powerful and interpretable!\")\n",
        "\n",
        "# Generate the final report\n",
        "try:\n",
        "    total_score, achievements_earned = create_achievement_summary()\n",
        "    celebration_message()\n",
        "    \n",
        "    print(\"\\\\nğŸ“œ CERTIFICATE OF COMPLETION:\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"ğŸ“ This certifies that you have successfully completed\")\n",
        "    print(\"   SESSION 1: Dialogue Summarization - Low-Resource Techniques\")\n",
        "    print(f\"   Final Score: {total_score:,} points\")\n",
        "    print(\"   Instructor: AI Coding Assistant\")\n",
        "    print(f\"   Date: {__import__('datetime').datetime.now().strftime('%Y-%m-%d')}\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(\"ğŸ‰ SESSION 1 COMPLETE!\")\n",
        "    print(\"ğŸ† Congratulations on finishing the dialogue summarization tutorial!\")\n",
        "    print(f\"ğŸ’¡ Note: {str(e)[:50]}...\")\n",
        "    print(\"ğŸš€ Ready for Session 2: LLM Prompting Techniques!\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5c3e6f54"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Challenge. Adapt to your own low resource language\n",
        "\n",
        "Now you have an English pipeline. The next step is to replace the English dialogue with data from your target language.\n",
        "\n",
        "If you work on a language with limited resources, use the same structure.\n",
        "1) Create turns with speaker labels.\n",
        "2) Normalize and segment.\n",
        "3) Start with an extractive baseline.\n",
        "4) Add a multilingual model or a translation pivot only if you need it.\n",
        "5) Evaluate with a small set of human references.\n",
        "\n",
        "The next cell includes a ready to use template. It runs as is. Replace `MY_DIALOGUE` with your own data.\n"
      ],
      "id": "b94d5ff6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MY_DIALOGUE = \"\"\"SPEAKER1: Replace this with your own dialogue in any language.\n",
        "SPEAKER2: Keep speaker labels. Keep short lines if possible.\n",
        "SPEAKER1: Then rerun the cells below.\"\"\"\n",
        "\n",
        "clean = normalize_dialogue(MY_DIALOGUE)\n",
        "summary_baseline = turn_level_summarize(clean, max_turns=3)\n",
        "print(\"Baseline summary:\\n\", summary_baseline)\n",
        "\n",
        "if tokenizer is not None and model is not None:\n",
        "    prompt = build_prompt(style=\"neutral\", focus=\"actions\", max_sentences=2)\n",
        "    summary_llm = generate_summary_t5(clean, prompt, max_new_tokens=80, temperature=0.0)\n",
        "    print(\"\\nLLM summary:\\n\", summary_llm)\n",
        "else:\n",
        "    print(\"\\nLLM not available. Baseline is your default.\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8d0897db"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Wrap up\n",
        "\n",
        "You now have a reproducible dialogue summarization pipeline that is usable with.\n",
        "- No LLM, via TextRank and turn level extraction.\n",
        "- A small instruction model, via prompt engineering.\n",
        "- Low resource conditions, via normalization and constraints.\n",
        "\n",
        "If you want to push further for true low resource languages.\n",
        "- Swap English stopwords for a custom list, or disable stopwords.\n",
        "- Use character n gram TF IDF for languages without whitespace.\n",
        "- Add a small glossary and a retrieval step, then feed only the relevant turns to the model.\n",
        "- Build a tiny evaluation set, 50 to 200 dialogues with one reference summary each.\n"
      ],
      "id": "848f06d1"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}