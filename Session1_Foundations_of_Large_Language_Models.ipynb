{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b48c569d",
      "metadata": {},
      "source": [
        "# Session 1: Foundations of Large Language Models ü§ñ\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/Session1_Foundations_of_Large_Language_Models.ipynb)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
        "\n",
        "**üìö Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "**Core Concepts:**\n",
        "- **LLM Architecture** - Understand transformer models and attention mechanisms\n",
        "- **Tokenization** - How models process and understand text across languages\n",
        "- **Text Representation** - Embeddings, vectors, and semantic similarity\n",
        "- **Model Comparison** - Analyze different LLM architectures and capabilities\n",
        "- **Low-Resource Considerations** - Challenges with underrepresented languages\n",
        "\n",
        "**Practical Skills:**\n",
        "- Compare tokenization across different models\n",
        "- Analyze model behavior with multilingual text\n",
        "- Implement basic text processing pipelines\n",
        "- Evaluate model performance on various languages\n",
        "- Build foundation for advanced NLP applications\n",
        "\n",
        "**Why This Matters:** Understanding LLM fundamentals is crucial for effective use in real-world applications, especially when working with diverse languages and limited computational resources.\n",
        "\n",
        "\n",
        "## Course Context\n",
        "\n",
        "| Session | Focus | Techniques | Prerequisites |\n",
        "|---------|-------|------------|---------------|\n",
        "| **Session 0** | Setup & Orientation | Environment, Basic Concepts | None |\n",
        "| **‚Üí This Session** | **LLM Foundations** | **Tokenization, Embeddings, Model Analysis** | **Session 0** |\n",
        "| **Session 2** | Prompt Engineering | Advanced Prompting, Chain-of-Thought | Sessions 0-1 |\n",
        "| **Session 3** | Fine-tuning | LoRA, QLoRA, Custom Training | Sessions 0-2 |\n",
        "| **Session 4** | Bias & Ethics | Fairness, Evaluation, Mitigation | Sessions 0-3 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup_section",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Environment Setup\n",
        "\n",
        "### What This Section Does\n",
        "This section prepares your coding environment with all necessary libraries for exploring Large Language Model foundations. We'll install packages optimized for **interactive learning** - educational, efficient, and GPU-optional!\n",
        "\n",
        "### Why These Specific Packages?\n",
        "\n",
        "**Core Dependencies:**\n",
        "- `numpy` + `pandas`: Essential for data manipulation and analysis\n",
        "- `scikit-learn`: Similarity metrics and basic ML utilities\n",
        "- `matplotlib`: Visualization of model behaviors and comparisons\n",
        "\n",
        "**LLM Ecosystem:**\n",
        "- `transformers`: Access to pretrained models and tokenizers\n",
        "- `sentence-transformers`: Semantic embeddings and similarity\n",
        "- `torch`: PyTorch backend for model operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install_packages",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick setup for this session\n",
        "!pip install -q transformers sentence-transformers scikit-learn matplotlib pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports_setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports for LLM foundations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"‚úÖ Environment ready for LLM foundations exploration!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tokenization_chapter",
      "metadata": {},
      "source": [
        "# Chapter 1: Understanding Tokenization\n",
        "\n",
        "## What We'll Explore\n",
        "\n",
        "Tokenization is how models convert text into numbers they can process. Let's see how this works with different languages and models.\n",
        "\n",
        "### Step 1: Prepare Test Sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6f525b6",
      "metadata": {},
      "source": [
        "**Model Selection:** We'll compare two popular multilingual models from [Hugging Face Hub](https://huggingface.co/models):\n",
        "\n",
        "- **BERT** (Google): Bidirectional Encoder Representations from Transformers - one of the first successful transformer models\n",
        "- **XLM-RoBERTa** (Facebook): Cross-lingual Language Model based on RoBERTa - specifically designed for multilingual tasks\n",
        "\n",
        "These model names are the official identifiers used to download them from Hugging Face's model repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test_sentences",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Expanded Multilingual Test Corpus for LLM Analysis\n",
        "\n",
        "üìä CORPUS DESIGN:\n",
        "This corpus contains 5 semantic clusters across 3 languages (15 sentences total):\n",
        "- English: Germanic, high-resource language\n",
        "- Luxembourgish: Germanic, low-resource language  \n",
        "- French: Romance, high-resource language\n",
        "\n",
        "üéØ DOMAINS COVERED:\n",
        "1. Medical: Doctor-patient communication & treatment planning\n",
        "2. Daily Life: Weather and environmental descriptions\n",
        "3. Technology: Digital communication and tools\n",
        "4. Education: Learning and academic contexts\n",
        "\n",
        "üî¨ RESEARCH QUESTIONS:\n",
        "1. Semantic Clustering: Do equivalent meanings cluster together in embedding space?\n",
        "2. Language Separation: Do languages form distinct clusters despite shared meanings?\n",
        "3. Domain Effects: Do different domains create separable clusters?\n",
        "4. Resource Impact: Does low-resource Luxembourgish show different patterns?\n",
        "\n",
        "üìà PCA VISUALIZATION EXPECTATIONS:\n",
        "- **Semantic Clusters**: Sentences with same meaning should be close\n",
        "- **Language Patterns**: Each language might form sub-clusters\n",
        "- **Domain Separation**: Medical vs. daily life vs. tech might separate\n",
        "- **Quality Assessment**: Tight cross-lingual clusters = good multilingual model\n",
        "\n",
        "üí° PRACTICAL INSIGHTS:\n",
        "This expanded corpus will reveal:\n",
        "- How well the model handles cross-lingual semantic equivalence\n",
        "- Whether domain affects multilingual performance\n",
        "- Resource availability impact on embedding quality\n",
        "- Model's ability to generalize across typologically related languages\n",
        "\n",
        "Note: Perfect multilingual models would show tight semantic clusters with\n",
        "      mixed languages, not language-separated clusters.\n",
        "\"\"\"\n",
        "\n",
        "# Multilingual test corpus - Expanded for better PCA visualization\n",
        "test_sentences = {\n",
        "    # Medical Domain - Set 1: Doctor-Patient Communication\n",
        "    \"English_medical_1\": \"The doctor explains the diagnosis carefully to the patient.\",\n",
        "    \"Luxembourgish_medical_1\": \"Den Dokter erkl√§ert d'Diagnos ganz roueg dem Patient.\",\n",
        "    \"French_medical_1\": \"Le m√©decin explique le diagnostic avec soin au patient.\",\n",
        "    \n",
        "    # Medical Domain - Set 2: Treatment Planning  \n",
        "    \"English_medical_2\": \"We need to schedule your surgery for next week.\",\n",
        "    \"Luxembourgish_medical_2\": \"Mir musse √§r Operatioun fir n√§chst Woch plangen.\",\n",
        "    \"French_medical_2\": \"Nous devons programmer votre chirurgie pour la semaine prochaine.\",\n",
        "    \n",
        "    # Daily Life Domain - Set 3: Weather/Environment\n",
        "    \"English_daily_1\": \"It's raining heavily outside today.\",\n",
        "    \"Luxembourgish_daily_1\": \"Et gitt haut schw√©ier drausser.\",\n",
        "    \"French_daily_1\": \"Il pleut beaucoup dehors aujourd'hui.\",\n",
        "    \n",
        "    # Technology Domain - Set 4: Digital Communication\n",
        "    \"English_tech_1\": \"Please send me the email with the important documents.\",\n",
        "    \"Luxembourgish_tech_1\": \"Sch√©ckt mir w√©i gelift d'Email mat de wichtege Dokumenter.\",\n",
        "    \"French_tech_1\": \"Veuillez m'envoyer l'email avec les documents importants.\",\n",
        "    \n",
        "    # Education Domain - Set 5: Learning Context\n",
        "    \"English_edu_1\": \"The students are learning new concepts in mathematics.\",\n",
        "    \"Luxembourgish_edu_1\": \"D'Studenten l√©ieren nei Konzepter an der Mathematik.\",\n",
        "    \"French_edu_1\": \"Les √©tudiants apprennent de nouveaux concepts en math√©matiques.\"\n",
        "}\n",
        "\n",
        "# Display corpus for verification\n",
        "print(\"=\" * 80)\n",
        "print(\"EXPANDED MULTILINGUAL TEST CORPUS (15 sentences across 5 semantic groups)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Group sentences by semantic meaning for better display\n",
        "semantic_groups = {\n",
        "    \"Medical Communication\": [\"medical_1\"],\n",
        "    \"Treatment Planning\": [\"medical_2\"], \n",
        "    \"Weather Description\": [\"daily_1\"],\n",
        "    \"Digital Communication\": [\"tech_1\"],\n",
        "    \"Educational Context\": [\"edu_1\"]\n",
        "}\n",
        "\n",
        "for group_name, identifiers in semantic_groups.items():\n",
        "    print(f\"\\nüîπ {group_name.upper()}:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for identifier in identifiers:\n",
        "        for lang in [\"English\", \"Luxembourgish\", \"French\"]:\n",
        "            key = f\"{lang}_{identifier}\"\n",
        "            if key in test_sentences:\n",
        "                sentence = test_sentences[key]\n",
        "                word_count = len(sentence.split())\n",
        "                char_count = len(sentence)\n",
        "                print(f\"  {lang:12} ({word_count:2d}w, {char_count:3d}c): {sentence}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"üìä CORPUS STATISTICS:\")\n",
        "print(f\"   ‚Ä¢ Total sentences: {len(test_sentences)}\")\n",
        "print(f\"   ‚Ä¢ Languages: 3 (English, Luxembourgish, French)\")  \n",
        "print(f\"   ‚Ä¢ Semantic groups: {len(semantic_groups)}\")\n",
        "print(f\"   ‚Ä¢ PCA plot will show {len(test_sentences)} data points\")\n",
        "print(f\"   ‚Ä¢ Expected clusters: Semantic groups should cluster across languages\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tokenization_analysis",
      "metadata": {},
      "source": [
        "### Step 2: Compare Tokenization Across Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c9999e",
      "metadata": {},
      "source": [
        "## üîÑ Alternative: Public Models Only\n",
        "\n",
        "If you want to skip gated model setup and run immediately, uncomment this alternative:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64cfa018",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üöÄ QUICK START: PUBLIC MODELS ONLY (No authentication needed)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "üéØ ALTERNATIVE APPROACH: Use this if you want to start immediately without gated models\n",
        "\n",
        "To use this instead:\n",
        "1. Comment out the previous cell's models_to_compare list\n",
        "2. Uncomment the code below\n",
        "3. Run immediately - no authentication required!\n",
        "\"\"\"\n",
        "\n",
        "# üîì UNCOMMENT FOR PUBLIC-ONLY DEMO:\n",
        "# from transformers import AutoTokenizer\n",
        "# \n",
        "# # Public models only - no authentication required\n",
        "# models_to_compare = [\n",
        "#     \"bert-base-multilingual-cased\",        # WordPiece, multilingual\n",
        "#     \"xlm-roberta-base\",                    # SentencePiece, multilingual  \n",
        "#     \"google/mt5-small\",                    # SentencePiece, multilingual encoder-decoder\n",
        "#     \"gpt2\",                                # BPE, English-focused\n",
        "# ]\n",
        "# \n",
        "# print(\"üîì PUBLIC MODELS SELECTED (No authentication needed):\")\n",
        "# for i, model in enumerate(models_to_compare, 1):\n",
        "#     print(f\"   {i}. {model}\")\n",
        "# print(\"\\n‚úÖ Ready to run immediately!\")\n",
        "\n",
        "print(\"üí° Choose one approach above: Gated models (more advanced) or Public models (immediate start)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "compare_tokenization",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMPREHENSIVE TOKENIZATION COMPARISON ACROSS MODEL ARCHITECTURES\n",
        "# ============================================================================\n",
        "# These models represent different tokenization algorithms and training approaches:\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ============================================================================\n",
        "# üîê COLAB AUTHENTICATION FOR GATED MODELS\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "üí° TO USE GATED MODELS IN COLAB:\n",
        "\n",
        "1. UNCOMMENT the authentication code below\n",
        "2. Run this cell - it will show a popup in Colab\n",
        "3. Enter your Hugging Face token in the popup\n",
        "4. Then the gated models will work!\n",
        "\n",
        "üéØ STEPS TO GET ACCESS:\n",
        "‚Ä¢ Visit: https://huggingface.co/google/gemma-2-2b-it\n",
        "‚Ä¢ Click \"Request Access\" and wait for approval\n",
        "‚Ä¢ Go to: https://huggingface.co/settings/tokens\n",
        "‚Ä¢ Create a token with \"Read\" permissions\n",
        "‚Ä¢ Use that token in the popup below\n",
        "\"\"\"\n",
        "\n",
        "# üéì OPTIONAL ADVANCED FEATURE (for individual student exploration):\n",
        "# \n",
        "# IF you want to try gated models like Gemma (completely optional):\n",
        "# 1. Get approval at: https://huggingface.co/google/gemma-2-2b-it\n",
        "# 2. Create token at: https://huggingface.co/settings/tokens  \n",
        "# 3. Uncomment the code below:\n",
        "#\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()  # This will show a popup for your personal token\n",
        "#\n",
        "# 4. Add \"google/gemma-2-2b-it\" to models_to_compare list above\n",
        "#\n",
        "# ‚ö†Ô∏è NOTE: This is optional! The tutorial is complete with public models only.\n",
        "\n",
        "# Model selection - PUBLIC MODELS ONLY (safe for shared notebooks)\n",
        "models_to_compare = [\n",
        "    \"bert-base-multilingual-cased\",        # WordPiece, multilingual  \n",
        "    \"xlm-roberta-base\",                    # SentencePiece, multilingual\n",
        "    \"google/mt5-small\",                    # SentencePiece, multilingual encoder-decoder (PUBLIC)\n",
        "    \"gpt2\",                                # BPE, English-focused (shows language bias)\n",
        "]\n",
        "\n",
        "# üö® VERIFICATION: All models above are PUBLIC and require NO authentication\n",
        "print(\"‚úÖ USING PUBLIC MODELS ONLY:\")\n",
        "print(\"   ‚ùå NO gated models (like Gemma) in this list\")  \n",
        "print(\"   ‚úÖ All models work without HuggingFace tokens\")\n",
        "print(\"   ‚úÖ Safe for shared notebooks and classroom use\")\n",
        "\n",
        "# Double-check: no gated models present\n",
        "gated_models = [m for m in models_to_compare if 'gemma' in m.lower() or 'llama' in m.lower()]\n",
        "if gated_models:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Found gated models: {gated_models}\")\n",
        "    print(\"   ‚Üí Remove these from the list above\")\n",
        "else:\n",
        "    print(\"   üéØ CONFIRMED: Only public models detected\")\n",
        "\n",
        "# ‚≠ê FOR ADVANCED STUDENTS (Optional - requires individual setup):\n",
        "# If you have gated model access and want to compare cutting-edge models:\n",
        "# 1. Add: \"google/gemma-2-2b-it\" to the list above\n",
        "# 2. Uncomment the authentication code below\n",
        "# 3. This is optional - the tutorial works perfectly with public models only!\n",
        "\n",
        "print(\"üìã MODELS SELECTED FOR COMPARISON:\")\n",
        "print(f\"   üîì Public models: {len(models_to_compare)} models (no authentication needed)\")\n",
        "print()\n",
        "for i, model in enumerate(models_to_compare, 1):\n",
        "    # Determine tokenization algorithm for educational value\n",
        "    if \"bert\" in model.lower():\n",
        "        algorithm = \"WordPiece\"\n",
        "    elif \"xlm\" in model.lower() or \"mt5\" in model.lower():\n",
        "        algorithm = \"SentencePiece\"  \n",
        "    elif \"gpt\" in model.lower():\n",
        "        algorithm = \"BPE\"\n",
        "    else:\n",
        "        algorithm = \"Various\"\n",
        "        \n",
        "    print(f\"   {i}. {model}\")\n",
        "    print(f\"      ‚Üí Algorithm: {algorithm}\")\n",
        "\n",
        "print(f\"\\n‚úÖ READY TO RUN:\")\n",
        "print(f\"   ‚Ä¢ All models are publicly available\")\n",
        "print(f\"   ‚Ä¢ No authentication required\") \n",
        "print(f\"   ‚Ä¢ Tutorial covers multiple tokenization algorithms\")\n",
        "print(f\"   ‚Ä¢ Students can run immediately in any environment\")\n",
        "\n",
        "# ============================================================================\n",
        "# üìö EXPANDED TOKENIZATION TEST CORPUS\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "üéØ MULTI-DOMAIN SENTENCE PAIRS FOR COMPREHENSIVE ANALYSIS:\n",
        "\n",
        "This expanded corpus tests tokenization across different domains and linguistic structures:\n",
        "- Academic (original): Technical terminology\n",
        "- Medical: Specialized vocabulary  \n",
        "- Daily Life: Common conversational language\n",
        "- Technology: Modern digital terminology\n",
        "- Business: Professional/commercial language\n",
        "\n",
        "Each pair is semantically equivalent but may reveal different tokenization patterns\n",
        "due to domain-specific vocabulary, morphological complexity, and training data availability.\n",
        "\"\"\"\n",
        "\n",
        "# Test sentence pairs (English ‚Üî Luxembourgish)\n",
        "test_sentence_pairs = [\n",
        "    # Academic Domain (original)\n",
        "    {\n",
        "        \"domain\": \"Academic\", \n",
        "        \"en\": \"Students are learning about large language models.\",\n",
        "        \"lb\": \"D'Studenten l√©ieren iwwer grouss Sproochmodeller.\",\n",
        "        \"concept\": \"Educational technology\"\n",
        "    },\n",
        "    \n",
        "    # Medical Domain\n",
        "    {\n",
        "        \"domain\": \"Medical\",\n",
        "        \"en\": \"The doctor carefully examines the patient's symptoms.\",\n",
        "        \"lb\": \"Den Dokter √´nnersicht ganz virsiichteg d'Symptomer vum Patient.\",\n",
        "        \"concept\": \"Healthcare interaction\"\n",
        "    },\n",
        "    \n",
        "    # Daily Life Domain\n",
        "    {\n",
        "        \"domain\": \"Daily Life\", \n",
        "        \"en\": \"Today the weather is beautiful and sunny.\",\n",
        "        \"lb\": \"Haut ass d'Wieder sch√©in a sonneg.\",\n",
        "        \"concept\": \"Weather description\"\n",
        "    },\n",
        "    \n",
        "    # Technology Domain\n",
        "    {\n",
        "        \"domain\": \"Technology\",\n",
        "        \"en\": \"The smartphone application works perfectly on all devices.\", \n",
        "        \"lb\": \"D'Smartphone-App funktion√©iert perfekt op all Apparater.\",\n",
        "        \"concept\": \"Digital technology\"\n",
        "    },\n",
        "    \n",
        "    # Business Domain\n",
        "    {\n",
        "        \"domain\": \"Business\",\n",
        "        \"en\": \"The company develops innovative solutions for customers.\",\n",
        "        \"lb\": \"D'Firma entw√©ckelt innovativ L√©isungen fir d'Clienten.\",\n",
        "        \"concept\": \"Commercial activity\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"üìä EXPANDED TOKENIZATION TEST CORPUS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"üìà Analysis Scope:\")\n",
        "print(f\"   ‚Ä¢ {len(test_sentence_pairs)} sentence pairs\")\n",
        "print(f\"   ‚Ä¢ {len(set(p['domain'] for p in test_sentence_pairs))} different domains\")\n",
        "print(f\"   ‚Ä¢ English (high-resource) ‚Üî Luxembourgish (low-resource)\")\n",
        "print(f\"   ‚Ä¢ Tests domain-specific vocabulary effects\")\n",
        "\n",
        "print(f\"\\nüìù SENTENCE PAIRS BY DOMAIN:\")\n",
        "for pair in test_sentence_pairs:\n",
        "    print(f\"\\nüîπ {pair['domain'].upper()} ({pair['concept']}):\")\n",
        "    print(f\"   EN: {pair['en']}\")  \n",
        "    print(f\"   LB: {pair['lb']}\")\n",
        "    print(f\"   Words: EN={len(pair['en'].split())} | LB={len(pair['lb'].split())}\")\n",
        "\n",
        "# For backward compatibility with existing code, keep original variables\n",
        "text_en = test_sentence_pairs[0][\"en\"]  # Academic example as default\n",
        "text_lr = test_sentence_pairs[0][\"lb\"]\n",
        "\n",
        "def show_tokenization(model_name, text):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(\"Text :\", text)\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"Number of tokens:\", len(tokens))\n",
        "    \n",
        "    # Return data for DataFrame creation\n",
        "    return {\n",
        "        'model': model_name.split('/')[-1],  # Short name\n",
        "        'text': text,\n",
        "        'num_tokens': len(tokens),\n",
        "        'num_words': len(text.split()),\n",
        "        'tokens_per_word': len(tokens) / len(text.split()) if text.split() else 0,\n",
        "        'tokens_preview': tokens[:5]  # First 5 tokens for reference\n",
        "    }\n",
        "\n",
        "# Collect results for analysis\n",
        "df_results = []\n",
        "\n",
        "# ============================================================================\n",
        "# üîç COMPREHENSIVE TOKENIZATION ANALYSIS ACROSS DOMAINS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üöÄ RUNNING COMPREHENSIVE TOKENIZATION ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üìä Processing {len(test_sentence_pairs)} sentence pairs across {len(set(p['domain'] for p in test_sentence_pairs))} domains\")\n",
        "print(f\"ü§ñ Testing {len(models_to_compare)} different model architectures\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Process each sentence pair across all models\n",
        "for pair_idx, sentence_pair in enumerate(test_sentence_pairs):\n",
        "    domain = sentence_pair['domain']\n",
        "    concept = sentence_pair['concept']\n",
        "    \n",
        "    print(f\"\\nüîπ DOMAIN {pair_idx+1}: {domain.upper()} ({concept})\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Analyze English sentence\n",
        "    print(\"üá¨üáß ENGLISH:\")\n",
        "    print(f\"   Text: {sentence_pair['en']}\")\n",
        "    for model_name in models_to_compare:\n",
        "        result = show_tokenization(model_name, sentence_pair['en'])\n",
        "        result['language'] = 'English'\n",
        "        result['domain'] = domain  \n",
        "        result['concept'] = concept\n",
        "        result['sentence_pair_id'] = pair_idx\n",
        "        df_results.append(result)\n",
        "    \n",
        "    # Analyze Luxembourgish sentence\n",
        "    print(f\"\\nüá±üá∫ LUXEMBOURGISH:\")\n",
        "    print(f\"   Text: {sentence_pair['lb']}\")\n",
        "    for model_name in models_to_compare:\n",
        "        result = show_tokenization(model_name, sentence_pair['lb'])\n",
        "        result['language'] = 'Luxembourgish'\n",
        "        result['domain'] = domain\n",
        "        result['concept'] = concept  \n",
        "        result['sentence_pair_id'] = pair_idx\n",
        "        df_results.append(result)\n",
        "\n",
        "print(f\"\\nüéØ ANALYSIS COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Convert to pandas DataFrame for comprehensive analysis\n",
        "import pandas as pd\n",
        "df_results = pd.DataFrame(df_results)\n",
        "\n",
        "print(f\"\\nüìä COMPREHENSIVE RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"‚úÖ Total entries collected: {len(df_results)}\")\n",
        "print(f\"üìã DataFrame columns: {list(df_results.columns)}\")\n",
        "print(f\"üåç Languages analyzed: {list(df_results['language'].unique())}\")\n",
        "print(f\"üè¢ Domains covered: {list(df_results['domain'].unique())}\")\n",
        "print(f\"ü§ñ Models tested: {list(df_results['model'].unique())}\")\n",
        "\n",
        "# Quick domain-based analysis preview\n",
        "print(f\"\\nüîç DOMAIN-BASED TOKENIZATION EFFICIENCY PREVIEW:\")\n",
        "print(\"-\" * 50)\n",
        "for domain in df_results['domain'].unique():\n",
        "    domain_data = df_results[df_results['domain'] == domain]\n",
        "    avg_tokens_per_word = domain_data['tokens_per_word'].mean()\n",
        "    print(f\"   {domain:12}: Avg {avg_tokens_per_word:.2f} tokens/word\")\n",
        "\n",
        "print(f\"\\nüí° INSIGHTS AVAILABLE:\")\n",
        "print(f\"   ‚Ä¢ Cross-domain tokenization efficiency comparison\")\n",
        "print(f\"   ‚Ä¢ Language-specific challenges by domain\")  \n",
        "print(f\"   ‚Ä¢ Model architecture performance across contexts\")\n",
        "print(f\"   ‚Ä¢ Resource availability impact (EN vs LB)\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb45c876",
      "metadata": {},
      "source": [
        "### üîç Inspecting Tokenizer Types Programmatically\n",
        "\n",
        "Sometimes you need to determine what tokenization algorithm a model uses (WordPiece, BPE, SentencePiece, etc.). While there's no universal flag, you can inspect the tokenizer programmatically:\n",
        "\n",
        "**Why This Matters:**\n",
        "- Different algorithms handle subwords differently\n",
        "- Understanding the algorithm helps predict tokenization behavior\n",
        "- Important for debugging and optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b6b9cd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TOKENIZER INTROSPECTION: Understanding Algorithm Types\n",
        "# ============================================================================\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"xlm-roberta-base\"\n",
        "tok = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(\"Tokenizer class:\", tok.__class__.__name__)\n",
        "print(\"Backend:\", getattr(tok, \"backend_tokenizer\", None))\n",
        "print(\"Special tokens:\", tok.special_tokens_map)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f602c90",
      "metadata": {},
      "source": [
        "### üéØ Key Takeaways: Tokenization Algorithms in Practice\n",
        "\n",
        "**Understanding these differences helps you:**\n",
        "\n",
        "1. **Choose the Right Model**: \n",
        "   - Need to handle many languages? ‚Üí SentencePiece models (XLM-RoBERTa, mT5)\n",
        "   - Working primarily with English? ‚Üí WordPiece or BPE might be sufficient\n",
        "   - Need fast inference? ‚Üí Consider algorithm efficiency for your text type\n",
        "\n",
        "2. **Predict Performance**:\n",
        "   - SentencePiece typically handles low-resource languages better\n",
        "   - WordPiece good for languages with complex morphology\n",
        "   - BPE optimized for languages similar to training data\n",
        "\n",
        "3. **Debug Issues**:\n",
        "   - Unexpected tokenization? Check the algorithm type\n",
        "   - High token counts? Algorithm might not be suited for your language\n",
        "   - Special token conflicts? Inspect the special_tokens_map\n",
        "\n",
        "**Next**: Let's see how these tokenization differences affect semantic representations..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa9f195b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üîß DEPENDENCY FIX: Calculate PCA Coordinates First\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üö® FIXING NAMEERROR: Calculating coords_2d before visualization\")\n",
        "\n",
        "# Import required modules\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Apply PCA to create the 2D coordinates needed for visualization\n",
        "print(\"üìê Computing PCA coordinates...\")\n",
        "pca = PCA(n_components=2, random_state=42) \n",
        "coords_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Verify PCA results\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "print(f\"‚úÖ coords_2d is now defined!\")\n",
        "print(f\"   ‚Ä¢ Shape: {coords_2d.shape}\") \n",
        "print(f\"   ‚Ä¢ Variance retained: {sum(explained_var)*100:.1f}%\")\n",
        "print(f\"   ‚Ä¢ Ready for visualization in next cell\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tokenization_reflection",
      "metadata": {},
      "source": [
        "### ü§î Reflection Questions\n",
        "\n",
        "Look at the results above and consider:\n",
        "\n",
        "- Which language uses more tokens per word?\n",
        "- How might more tokens affect inference cost and speed?\n",
        "- Do you see any unusual token splits (broken words, weird subwords)?\n",
        "\n",
        "**Key Insight:** Languages with fewer training examples often get split into more subword tokens, increasing computational costs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "embeddings_chapter",
      "metadata": {},
      "source": [
        "# üìä Chapter 2: Text Embeddings & Semantic Similarity\n",
        "\n",
        "## Understanding Vector Representations\n",
        "\n",
        "**What are embeddings?** Numbers that capture the meaning of text in high-dimensional space.\n",
        "\n",
        "Let's see how different models create these representations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sentence_embeddings",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a multilingual sentence embedding model\n",
        "embedder_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "embedder = SentenceTransformer(embedder_name, device=device)\n",
        "\n",
        "print(f\"üìä Loaded embedding model: {embedder_name}\")\n",
        "\n",
        "# Get embeddings for our test sentences\n",
        "sentences = list(test_sentences.values())\n",
        "languages = list(test_sentences.keys())\n",
        "\n",
        "embeddings = embedder.encode(sentences, convert_to_numpy=True)\n",
        "print(f\"‚úÖ Created embeddings with shape: {embeddings.shape}\")\n",
        "print(f\"   Each sentence ‚Üí {embeddings.shape[1]} dimensional vector\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b65fee22",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üö® QUICK FIX: Visualization for Expanded Corpus (Fixed IndexError)\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"üîß FIXING VISUALIZATION FOR {len(test_sentences)} SENTENCES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create the visualization with dynamic color generation\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Import required modules for color generation\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "\n",
        "# Generate colors dynamically based on number of sentences\n",
        "n_sentences = len(test_sentences)\n",
        "sentence_keys = list(test_sentences.keys())\n",
        "\n",
        "print(f\"üìä Visualizing {n_sentences} sentence embeddings\")\n",
        "print(f\"üé® Generating {n_sentences} distinct colors automatically\")\n",
        "\n",
        "# Create color array that matches number of sentences\n",
        "colors = cm.Set3(np.linspace(0, 1, n_sentences))\n",
        "\n",
        "# Plot each sentence with generated colors\n",
        "for i, sentence_key in enumerate(sentence_keys):\n",
        "    # Use generated color\n",
        "    color = colors[i]\n",
        "    \n",
        "    # Create short labels for better readability\n",
        "    if '_' in sentence_key and len(sentence_key.split('_')) >= 2:\n",
        "        parts = sentence_key.split('_')\n",
        "        short_label = f\"{parts[0][:2]}-{parts[1][:3]}\"  # \"En-med\", \"Fr-dai\", etc.\n",
        "    else:\n",
        "        short_label = sentence_key[:8]  # Truncate long keys\n",
        "    \n",
        "    # Plot the point\n",
        "    plt.scatter(coords_2d[i, 0], coords_2d[i, 1], \n",
        "               c=[color], s=100, alpha=0.8, \n",
        "               edgecolor='black', linewidth=0.5)\n",
        "    \n",
        "    # Add text annotation\n",
        "    plt.annotate(short_label, (coords_2d[i, 0], coords_2d[i, 1]), \n",
        "                xytext=(5, 5), textcoords='offset points', \n",
        "                fontsize=8, alpha=0.9, fontweight='bold')\n",
        "\n",
        "# Customize plot\n",
        "plt.title(\"Sentence Embeddings: Expanded Multilingual Corpus\\n15 Sentences Across 5 Domains and 3 Languages\", \n",
        "         fontsize=12, pad=15)\n",
        "plt.xlabel(\"Principal Component 1 (Largest Variation Direction)\")\n",
        "plt.ylabel(\"Principal Component 2 (Second Largest Variation Direction)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add explanation box\n",
        "textstr = \"üîç Look for:\\\\n‚Ä¢ Semantic clusters (same meanings group)\\\\n‚Ä¢ Language mixing within clusters\\\\n‚Ä¢ Domain-based patterns\"\n",
        "props = dict(boxstyle='round', facecolor='lightblue', alpha=0.8)\n",
        "plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=9,\n",
        "        verticalalignment='top', bbox=props)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ VISUALIZATION COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"üí° ANALYSIS GUIDE:\")\n",
        "print(\"   üéØ GOOD multilingual model: Mixed languages in semantic clusters\")\n",
        "print(\"   ‚ö†Ô∏è  CONCERNING: Languages separated regardless of meaning\")\n",
        "print(\"   üìä Position = Semantic similarity (distance = meaning difference)\")\n",
        "print(\"   üåç Cross-lingual success = Same concepts cluster across languages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7371ca65",
      "metadata": {},
      "source": [
        "## Applying PCA for Visualization\n",
        "\n",
        "This section applies Principal Component Analysis to reduce high-dimensional embeddings to 2D coordinates for visualization while analyzing variance retention."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13d85cbb",
      "metadata": {},
      "source": [
        "## üìä Understanding PCA (Principal Component Analysis)\n",
        "\n",
        "**ü§î The Problem:** Our embeddings are 384-dimensional vectors - impossible to visualize directly!\n",
        "\n",
        "**üéØ The Solution:** PCA reduces high-dimensional data to 2D while preserving the most important relationships.\n",
        "\n",
        "### üìö How PCA Works:\n",
        "\n",
        "1. **Find Principal Components**: Directions in the data with maximum variance\n",
        "2. **Project Data**: Transform original data onto these new axes\n",
        "3. **Keep Top Components**: Use only the first 2 components for 2D visualization\n",
        "\n",
        "### üí° Key Insights:\n",
        "\n",
        "- **Component 1**: Captures the most variation in the data\n",
        "- **Component 2**: Captures the second most variation  \n",
        "- **Relationship Preservation**: Similar sentences should stay close even after reduction\n",
        "- **Information Loss**: We lose some information, but keep the most important patterns\n",
        "\n",
        "### üéØ Why This Matters:\n",
        "\n",
        "- Allows us to **visualize** high-dimensional embeddings\n",
        "- Helps us **understand** if similar meanings cluster together across languages\n",
        "- **Quality check** for our multilingual model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142cbf7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üî¨ APPLYING PCA FOR VISUALIZATION  \n",
        "# ============================================================================\n",
        "\n",
        "print(\"üìä PCA ANALYSIS\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"üìê Original embedding dimensions: {embeddings.shape[1]}\")\n",
        "print(f\"üéØ Reducing to: 2 dimensions for plotting\") \n",
        "print(f\"‚ö° Method: Principal Component Analysis\")\n",
        "\n",
        "# Apply PCA reduction\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "coords_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Analyze the results\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "print(f\"\\nüìä VARIANCE EXPLANATION:\")\n",
        "print(f\"   ‚Ä¢ Component 1: {explained_var[0]*100:.1f}% of original variance\")\n",
        "print(f\"   ‚Ä¢ Component 2: {explained_var[1]*100:.1f}% of original variance\") \n",
        "print(f\"   ‚Ä¢ Total retained: {sum(explained_var)*100:.1f}% of information\")\n",
        "\n",
        "print(f\"\\nüí° INTERPRETATION:\")\n",
        "if sum(explained_var) > 0.7:\n",
        "    print(f\"   ‚úÖ Great! We retained most of the important patterns\")\n",
        "elif sum(explained_var) > 0.5:\n",
        "    print(f\"   ‚ö†Ô∏è  Decent retention - visualization should be meaningful\")\n",
        "else:\n",
        "    print(f\"   üî¥ Low retention - visualization may not show all relationships\")\n",
        "\n",
        "print(f\"\\nüéØ COORDINATES READY FOR PLOTTING:\")\n",
        "print(f\"   Shape: {coords_2d.shape} (each sentence ‚Üí x,y coordinates)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38e325d4",
      "metadata": {},
      "source": [
        "## üîß CORRECTED Visualization for Expanded Corpus\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT NOTE**: This cell fixes the `IndexError` you might encounter with visualization cells that use hardcoded color arrays. \n",
        "\n",
        "**What was wrong?** Earlier cells used a fixed 5-color array (`['red', 'blue', 'green', 'orange', 'purple']`) but we now have 15 sentences in our expanded corpus.\n",
        "\n",
        "**How we fix it?** Dynamic color generation using matplotlib's colormap that creates exactly the right number of colors for any corpus size.\n",
        "\n",
        "**üéØ Use this cell instead** of any problematic visualization cells you encounter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142af783",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üé® FIXED VISUALIZATION: Dynamic Colors for Expanded Corpus\n",
        "# ============================================================================\n",
        "\n",
        "# Create visualization that handles any number of sentences (fixes IndexError)\n",
        "plt.figure(figsize=(12, 9))\n",
        "\n",
        "# Import colormap modules for dynamic color generation\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "\n",
        "# Generate sufficient colors for expanded test corpus\n",
        "n_sentences = len(test_sentences)\n",
        "sentence_keys = list(test_sentences.keys())\n",
        "colors_array = cm.Set3(np.linspace(0, 1, n_sentences))\n",
        "\n",
        "print(f\"üé® Creating visualization for {n_sentences} sentences with dynamic colors\")\n",
        "\n",
        "# Plot each sentence with proper color handling\n",
        "for i, sentence_key in enumerate(sentence_keys):\n",
        "    # Use dynamic color - no more IndexError!\n",
        "    color = colors_array[i]\n",
        "    \n",
        "    # Create meaningful short labels from sentence keys\n",
        "    if '_' in sentence_key:\n",
        "        parts = sentence_key.split('_')\n",
        "        short_label = f\"{parts[0][:2]}-{parts[1][:3]}\"  # \"En-med\", \"Lu-dai\", etc.\n",
        "    else:\n",
        "        short_label = sentence_key[:6]\n",
        "    \n",
        "    # Plot the point\n",
        "    plt.scatter(coords_2d[i, 0], coords_2d[i, 1], \n",
        "               c=[color], s=120, alpha=0.7, \n",
        "               edgecolor='black', linewidth=0.4)\n",
        "    \n",
        "    # Add readable annotation\n",
        "    plt.annotate(short_label, (coords_2d[i, 0], coords_2d[i, 1]), \n",
        "                xytext=(7, 7), textcoords='offset points', \n",
        "                fontsize=8, fontweight='bold')\n",
        "\n",
        "plt.title(\"Multilingual Sentence Embeddings in 2D Space\\nExpanded Corpus: 5 Domains √ó 3 Languages\", \n",
        "         fontsize=12, pad=15)\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "\n",
        "# Add interpretation guide as text box\n",
        "guide_text = (\"üîç ANALYSIS GUIDE:\\n\"\n",
        "             \"‚Ä¢ Distance = Semantic similarity\\n\" \n",
        "             \"‚Ä¢ Good: Same meanings cluster\\n\"\n",
        "             \"‚Ä¢ Concerning: Languages separate\")\n",
        "\n",
        "plt.text(0.02, 0.98, guide_text, transform=plt.gca().transAxes,\n",
        "         bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"lightyellow\", alpha=0.8),\n",
        "         fontsize=9, verticalalignment='top')\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ VISUALIZATION COMPLETE - IndexError fixed with dynamic colors!\")\n",
        "\n",
        "print(f\"\\nüí° KEY OBSERVATIONS TO LOOK FOR:\")\n",
        "print(f\"   üìè Distance = Semantic similarity (closer = more similar meaning)\")\n",
        "print(f\"   üéØ Good model: Mixed languages within semantic clusters\")\n",
        "print(f\"   ‚ö†Ô∏è  Poor model: Languages separated regardless of meaning\")\n",
        "print(f\"   üåç Cross-lingual success: Same concepts group across languages\")\n",
        "print(f\"   üè¢ Domain effects: Professional vs casual language patterns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "visualize_embeddings",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üé® VISUALIZE PCA RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "# Create visualization (using coords_2d from previous cell)\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "for i, (lang, sentence) in enumerate(test_sentences.items()):\n",
        "    plt.scatter(coords_2d[i, 0], coords_2d[i, 1], \n",
        "               c=colors[i], s=200, alpha=0.7, label=lang)\n",
        "    plt.annotate(lang, (coords_2d[i, 0], coords_2d[i, 1]), \n",
        "                xytext=(10, 10), textcoords='offset points', fontsize=12)\n",
        "\n",
        "plt.title(\"Sentence Embeddings in 2D Space\\n(All sentences have similar meaning)\", fontsize=14)\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üí° Key Observation: Similar-meaning sentences in different languages should cluster together!\")\n",
        "\n",
        "print(f\"\\nüî¨ PCA VISUALIZATION ANALYSIS:\")\n",
        "print(f\"   üìè What distance means: Closer points = more similar semantic meaning\")\n",
        "print(f\"   üéØ What to look for: Languages clustering together despite different words\")\n",
        "print(f\"   ‚öñÔ∏è  What variance tells us: Higher variance = more distinguishable patterns\")\n",
        "print(f\"   üåç Cross-lingual success: Different languages expressing same meaning should be near each other\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a59b24d4",
      "metadata": {},
      "source": [
        "### üìä Understanding Similarity Values: What Do The Numbers Mean?\n",
        "\n",
        "The cosine similarity values you see above tell us how semantically similar the sentences are. Here's how to interpret them:\n",
        "\n",
        "**üìê Cosine Similarity Scale (0.0 to 1.0):**\n",
        "- **0.9-1.0**: Nearly identical meaning (excellent cross-lingual alignment)\n",
        "- **0.7-0.89**: High similarity (strong semantic equivalence) \n",
        "- **0.5-0.69**: Moderate similarity (related concepts, some semantic overlap)\n",
        "- **0.3-0.49**: Low similarity (weakly related or different topics)\n",
        "- **0.0-0.29**: Very low similarity (mostly unrelated concepts)\n",
        "\n",
        "**What To Expect for Our Semantically Equivalent Sentences:**\n",
        "- **Good multilingual models**: Should show 0.7-0.9+ similarity across languages\n",
        "- **Diagonal values**: Should always be 1.0 (sentence compared to itself)\n",
        "- **Lower than expected scores**: May indicate model struggles with certain languages\n",
        "\n",
        "**Real-World Implications:**\n",
        "- **High scores (>0.7)**: Model is suitable for multilingual applications like translation, search\n",
        "- **Medium scores (0.5-0.7)**: Proceed with caution, may need language-specific tuning\n",
        "- **Low scores (<0.5)**: Consider different model or additional training for that language\n",
        "\n",
        "**Why Scores Might Be Lower Than Expected:**\n",
        "- Model had limited training data in the low-resource language\n",
        "- Different sentence structures or vocabulary between languages  \n",
        "- Domain mismatch (model trained on general text, tested on medical text)\n",
        "- **Tokenization issues affecting embedding quality** ‚Üê Let's explain this!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf5ee480",
      "metadata": {},
      "source": [
        "## Calculate Similarity Matrix\n",
        "\n",
        "This section computes pairwise cosine similarities between sentence embeddings to prepare for detailed cross-lingual analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17920c42",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CALCULATE SIMILARITY MATRIX (Required for Later Analysis)\n",
        "# ============================================================================\n",
        "\n",
        "# Calculate pairwise cosine similarities between all sentence embeddings  \n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "print(\"üîó SIMILARITY MATRIX CALCULATED\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"‚úÖ Matrix shape: {similarity_matrix.shape}\")\n",
        "print(f\"‚úÖ Values range from 0.0 (unrelated) to 1.0 (identical)\")\n",
        "print(f\"‚úÖ Ready for detailed analysis in upcoming cells\")\n",
        "\n",
        "# Quick preview of the matrix\n",
        "print(f\"\\nüìä Quick Preview (first few values):\")\n",
        "lang_names = list(test_sentences.keys())\n",
        "for i in range(min(2, len(lang_names))):\n",
        "    for j in range(min(2, len(lang_names))):\n",
        "        sim = similarity_matrix[i, j]\n",
        "        print(f\"   {lang_names[i]} ‚Üî {lang_names[j]}: {sim:.3f}\")\n",
        "\n",
        "print(f\"\\nüí° Full analysis coming in the next sections!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff61a5c",
      "metadata": {},
      "source": [
        "### üîß Deep Dive: How Tokenization Issues Affect Embedding Quality\n",
        "\n",
        "**The Connection:** Tokenization ‚Üí Embeddings ‚Üí Similarity Scores\n",
        "\n",
        "This is a crucial concept that many people overlook! Here's how poor tokenization can ruin your similarity analysis:\n",
        "\n",
        "#### üß© **The Process Chain:**\n",
        "```\n",
        "Raw Text ‚Üí Tokenization ‚Üí Token Embeddings ‚Üí Sentence Embedding ‚Üí Similarity Score\n",
        "```\n",
        "\n",
        "**When tokenization goes wrong, everything downstream suffers!**\n",
        "\n",
        "#### üìù **Concrete Examples:**\n",
        "\n",
        "**Example 1: Word Breaking**\n",
        "```\n",
        "English: \"carefully\" ‚Üí [\"careful\", \"##ly\"] (good: preserves meaning)\n",
        "Low-resource: \"sorgf√§ltig\" ‚Üí [\"so\", \"##r\", \"##g\", \"##f√§\", \"##lt\", \"##ig\"] (bad: loses word structure)\n",
        "```\n",
        "\n",
        "**Impact:** The low-resource word gets broken into meaningless fragments. The model can't learn that \"sorgf√§ltig\" = \"carefully\" because it never sees \"sorgf√§ltig\" as a coherent unit.\n",
        "\n",
        "**Example 2: Unknown Token Explosion**\n",
        "```\n",
        "English: \"doctor\" ‚Üí [\"doctor\"] (1 token, well-known)\n",
        "Low-resource: \"Dokter\" ‚Üí [\"[UNK]\"] (1 unknown token, no meaning)\n",
        "```\n",
        "\n",
        "**Impact:** The model has no representation for \"[UNK]\", so it gets a generic \"unknown\" embedding that doesn't capture the medical concept.\n",
        "\n",
        "**Example 3: Inconsistent Splitting**\n",
        "```\n",
        "Same concept, different tokenization:\n",
        "\"diagnosis\" ‚Üí [\"diagnosis\"] \n",
        "\"Diagnos\" ‚Üí [\"Dia\", \"##gno\", \"##s\"]\n",
        "```\n",
        "\n",
        "**Impact:** Even though both mean \"diagnosis,\" they get completely different embeddings because the tokenizer treats them as unrelated token sequences.\n",
        "\n",
        "#### ‚ö° **The Cascade Effect:**\n",
        "\n",
        "1. **Bad tokenization** ‚Üí Fragments or unknown tokens\n",
        "2. **Poor token embeddings** ‚Üí Generic or meaningless vectors  \n",
        "3. **Bad sentence embeddings** ‚Üí Average of poor-quality token vectors\n",
        "4. **Low similarity scores** ‚Üí Model appears to \"not understand\" the language\n",
        "\n",
        "#### üõ°Ô∏è **How to Detect This:**\n",
        "- Look at tokenization output: many tiny fragments = problem\n",
        "- High number of [UNK] tokens = problem  \n",
        "- Same meaning, very different token patterns = problem\n",
        "\n",
        "#### üí° **Solutions:**\n",
        "- Choose models trained specifically on your target language\n",
        "- Use SentencePiece-based models (better with unseen languages)\n",
        "- Consider domain-specific models if your text has specialized vocabulary\n",
        "- Fine-tune tokenizers on your target language data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c71ef4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXPANDED MULTILINGUAL WORD ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "# Multilingual concepts across language families and resource levels\n",
        "multilingual_concepts = {\n",
        "    \"medical_professional\": {\n",
        "        \"English\": \"doctor\",           # Germanic, high-resource\n",
        "        \"German\": \"Arzt\",            # Germanic, high-resource  \n",
        "        \"Luxembourgish\": \"Dokter\",   # Germanic, low-resource\n",
        "        \"French\": \"m√©decin\",         # Romance, high-resource\n",
        "        \"Spanish\": \"doctor\",         # Romance, high-resource\n",
        "        \"Dutch\": \"dokter\",           # Germanic, medium-resource\n",
        "        \"Italian\": \"medico\",         # Romance, high-resource\n",
        "    },\n",
        "    \"medical_assessment\": {\n",
        "        \"English\": \"diagnosis\",\n",
        "        \"German\": \"Diagnose\", \n",
        "        \"Luxembourgish\": \"Diagnos\",\n",
        "        \"French\": \"diagnostic\",\n",
        "        \"Spanish\": \"diagn√≥stico\", \n",
        "        \"Dutch\": \"diagnose\",\n",
        "        \"Italian\": \"diagnosi\",\n",
        "    },\n",
        "    \"with_care\": {\n",
        "        \"English\": \"carefully\",\n",
        "        \"German\": \"sorgf√§ltig\",\n",
        "        \"Luxembourgish\": \"roueg\", \n",
        "        \"French\": \"soigneusement\",\n",
        "        \"Spanish\": \"cuidadosamente\",\n",
        "        \"Dutch\": \"zorgvuldig\",\n",
        "        \"Italian\": \"attentamente\",\n",
        "    },\n",
        "    \"sick_person\": {\n",
        "        \"English\": \"patient\",\n",
        "        \"German\": \"Patient\",\n",
        "        \"Luxembourgish\": \"Patient\",\n",
        "        \"French\": \"patient\", \n",
        "        \"Spanish\": \"paciente\",\n",
        "        \"Dutch\": \"pati√´nt\",\n",
        "        \"Italian\": \"paziente\",\n",
        "    },\n",
        "    \"explains_meaning\": {\n",
        "        \"English\": \"explains\",\n",
        "        \"German\": \"erkl√§rt\",\n",
        "        \"Luxembourgish\": \"erkl√§ert\", \n",
        "        \"French\": \"explique\",\n",
        "        \"Spanish\": \"explica\",\n",
        "        \"Dutch\": \"legt uit\",\n",
        "        \"Italian\": \"spiega\",\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üåç EXPANDED MULTILINGUAL TOKENIZATION ANALYSIS\")\n",
        "print(\"=\" * 75)\n",
        "print(\"üìä LANGUAGE FAMILIES & RESOURCE LEVELS:\")\n",
        "print(\"   üá¨üáß Germanic Family:\")\n",
        "print(\"      ‚Ä¢ English (high-resource) ‚Üí German (high-resource)\")  \n",
        "print(\"      ‚Ä¢ Dutch (medium-resource) ‚Üí Luxembourgish (low-resource)\")\n",
        "print(\"   üá´üá∑ Romance Family:\")\n",
        "print(\"      ‚Ä¢ French, Spanish, Italian (all high-resource)\")\n",
        "print()\n",
        "print(\"üéØ RESEARCH QUESTIONS:\")\n",
        "print(\"   ‚Ä¢ Do models favor same-family languages? (Germanic vs Romance)\")\n",
        "print(\"   ‚Ä¢ How severe is the low-resource penalty? (Luxembourgish)\")\n",
        "print(\"   ‚Ä¢ Which architectures handle cross-lingual diversity best?\")\n",
        "print(\"=\" * 75)\n",
        "\n",
        "# Generate comparison pairs (English baseline vs all others)\n",
        "word_pairs = []\n",
        "target_languages = [\"German\", \"Luxembourgish\", \"French\", \"Spanish\", \"Dutch\", \"Italian\"]\n",
        "\n",
        "for concept, translations in multilingual_concepts.items():\n",
        "    concept_display = concept.replace(\"_\", \" \").title()\n",
        "    english_word = translations[\"English\"]\n",
        "    \n",
        "    for lang in target_languages:\n",
        "        if lang in translations:\n",
        "            other_word = translations[lang]\n",
        "            # Add resource level info for analysis\n",
        "            resource_level = \"high\" if lang in [\"German\", \"French\", \"Spanish\", \"Italian\"] else \"med\" if lang == \"Dutch\" else \"low\"\n",
        "            family = \"Germanic\" if lang in [\"German\", \"Luxembourgish\", \"Dutch\"] else \"Romance\"\n",
        "            \n",
        "            pair_label = f\"{concept_display} (EN‚Üí{lang}/{family}/{resource_level})\"\n",
        "            word_pairs.append((english_word, other_word, pair_label))\n",
        "\n",
        "print(f\"\\nüìà ANALYSIS SCOPE:\")\n",
        "print(f\"   ‚Ä¢ {len(word_pairs)} cross-lingual word pairs generated\")\n",
        "print(f\"   ‚Ä¢ {len(multilingual_concepts)} semantic concepts tested\")\n",
        "print(f\"   ‚Ä¢ {len(target_languages)} target languages analyzed\")\n",
        "print(f\"   ‚Ä¢ 2 language families (Germanic + Romance)\")\n",
        "print(f\"   ‚Ä¢ 3 resource levels (high, medium, low)\")\n",
        "print(\"\\nüî¨ This will reveal systematic tokenization biases across:\")\n",
        "print(\"   ‚Üí Language families (typological similarity)\")\n",
        "print(\"   ‚Üí Resource availability (training data volume)\")  \n",
        "print(\"   ‚Üí Model architectures (BERT vs XLM-R approaches)\")\n",
        "print(\"=\" * 75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49925251",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# WHAT'S BEEN ENHANCED: Before vs After\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üîÑ EXPANSION SUMMARY:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä BEFORE (Original):\")\n",
        "print(\"   ‚Ä¢ 5 word pairs (EN ‚Üî LB only)\")\n",
        "print(\"   ‚Ä¢ 1 language family comparison\")  \n",
        "print(\"   ‚Ä¢ Limited resource level analysis\")\n",
        "print()\n",
        "print(\"üöÄ AFTER (Enhanced):\")\n",
        "print(f\"   ‚Ä¢ {len(word_pairs)} word pairs across multiple language pairs\")\n",
        "print(\"   ‚Ä¢ 2 language families (Germanic + Romance)\")\n",
        "print(\"   ‚Ä¢ 3 resource levels (high/medium/low)\")\n",
        "print(\"   ‚Ä¢ 6-7 languages total coverage\")\n",
        "print()\n",
        "print(\"üéØ EDUCATIONAL VALUE:\")\n",
        "print(\"   ‚úÖ Students can see systematic tokenization patterns\")\n",
        "print(\"   ‚úÖ Compare language family effects (Germanic vs Romance)\")  \n",
        "print(\"   ‚úÖ Understand resource availability impact\")\n",
        "print(\"   ‚úÖ Identify model architecture differences\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Sample the enhanced word_pairs to show the structure\n",
        "print(f\"\\nüìù SAMPLE OF ENHANCED WORD PAIRS:\")\n",
        "print(\"   (First 6 pairs as examples)\")\n",
        "for i, (word1, word2, label) in enumerate(word_pairs[:6]):\n",
        "    print(f\"   {i+1:2}. {word1:12} ‚Üí {word2:15} | {label}\")\n",
        "if len(word_pairs) > 6:\n",
        "    print(f\"   ... and {len(word_pairs)-6} more pairs\")\n",
        "\n",
        "print(f\"\\nüí° This systematic expansion makes tokenization analysis much more educational!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "944a444d",
      "metadata": {},
      "source": [
        "## Practical Demonstration: Tokenization Quality Impact\n",
        "\n",
        "This demonstration shows how tokenization quality affects embedding similarity by analyzing word fragmentation and unknown tokens across different models and languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fc71b0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üéì STUDENT GUIDE: How to Use Gated Models in Colab (Optional Advanced Section)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "üìö QUESTION: How can students use Gemma (or other gated models) in Google Colab?\n",
        "\n",
        "‚úÖ ANSWER: Follow these steps (one-time setup per student):\n",
        "\n",
        "STEP 1: Get Model Access (Outside of Colab)\n",
        "==========================================\n",
        "1. Go to: https://huggingface.co/google/gemma-2-2b-it\n",
        "2. Click the \"Request Access\" button\n",
        "3. Wait for approval from Google\n",
        "4. You'll get an email when approved\n",
        "\n",
        "STEP 2: Create Hugging Face Token (Outside of Colab)  \n",
        "===================================================\n",
        "1. Go to: https://huggingface.co/settings/tokens\n",
        "2. Click \"New token\"\n",
        "3. Choose \"Read\" permissions (sufficient for downloading models)\n",
        "4. Copy the token (starts with \"hf_...\")\n",
        "\n",
        "STEP 3: Authenticate in Colab (Every Session)\n",
        "=============================================\n",
        "Run this code at the start of your Colab session:\n",
        "\"\"\"\n",
        "\n",
        "print(\"üîë TO USE GATED MODELS IN COLAB:\")\n",
        "print(\"1. Get model access approval (one-time)\")  \n",
        "print(\"2. Create HF token (one-time)\")\n",
        "print(\"3. Login in Colab (every session)\")\n",
        "print(\"\\nExample authentication code for Colab:\")\n",
        "print(\"-\" * 40)\n",
        "print(\"# Option A: Interactive login (recommended for beginners)\")\n",
        "print(\"from huggingface_hub import notebook_login\")\n",
        "print(\"notebook_login()  # This will show a popup to enter your token\")\n",
        "print()\n",
        "print(\"# Option B: Direct token login (for advanced users)\")  \n",
        "print(\"from huggingface_hub import login\")\n",
        "print(\"login(token='hf_your_token_here')  # Replace with your actual token\")\n",
        "print()\n",
        "print(\"# Option C: Environment variable (most secure)\")\n",
        "print(\"import os\")\n",
        "print(\"os.environ['HF_TOKEN'] = 'your_token_here'\")\n",
        "print(\"from huggingface_hub import login\") \n",
        "print(\"login()\")\n",
        "\n",
        "print(f\"\\nüí° AFTER AUTHENTICATION:\")\n",
        "print(f\"   Just uncomment the gated model in the list above!\")\n",
        "print(f\"   models_to_compare.append('google/gemma-2-2b-it')\")\n",
        "\n",
        "print(f\"\\nüéØ FOR INSTRUCTORS:\")\n",
        "print(f\"   ‚Ä¢ You could demo this live for interested students\")\n",
        "print(f\"   ‚Ä¢ Or provide it as bonus/homework material\") \n",
        "print(f\"   ‚Ä¢ Main tutorial works fine with public models only\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6333bc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PRACTICAL DEMONSTRATION: Tokenization Quality Impact\n",
        "# ============================================================================\n",
        "\n",
        "def demonstrate_tokenization_quality(word_pairs, model_name):\n",
        "    \"\"\"\n",
        "    Show how tokenization quality varies between equivalent words across languages.\n",
        "    \n",
        "    Args:\n",
        "        word_pairs: List of (lang1_word, lang2_word, meaning) tuples\n",
        "        model_name: HuggingFace model to test\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç TOKENIZATION QUALITY ANALYSIS: {model_name}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        \n",
        "        for word1, word2, meaning in word_pairs:\n",
        "            tokens1 = tokenizer.tokenize(word1)\n",
        "            tokens2 = tokenizer.tokenize(word2)\n",
        "            \n",
        "            # Count fragmentations and unknowns\n",
        "            frag1 = len(tokens1)\n",
        "            frag2 = len(tokens2)\n",
        "            unk1 = sum(1 for t in tokens1 if '[UNK]' in t or '<unk>' in t)\n",
        "            unk2 = sum(1 for t in tokens2 if '[UNK]' in t or '<unk>' in t)\n",
        "            \n",
        "            # Quality assessment\n",
        "            quality1 = \"üü¢ Good\" if frag1 == 1 and unk1 == 0 else (\"üü° OK\" if unk1 == 0 else \"üî¥ Poor\")\n",
        "            quality2 = \"üü¢ Good\" if frag2 == 1 and unk2 == 0 else (\"üü° OK\" if unk2 == 0 else \"üî¥ Poor\")\n",
        "            \n",
        "            print(f\"\\nüìù Concept: '{meaning}'\")\n",
        "            print(f\"   {word1:15} ‚Üí {tokens1} | Fragments: {frag1}, UNK: {unk1} | {quality1}\")\n",
        "            print(f\"   {word2:15} ‚Üí {tokens2} | Fragments: {frag2}, UNK: {unk2} | {quality2}\")\n",
        "            \n",
        "            # Predict embedding quality\n",
        "            if quality1 == quality2 == \"üü¢ Good\":\n",
        "                prediction = \"üéØ High similarity expected\"\n",
        "            elif \"üî¥ Poor\" in [quality1, quality2]:\n",
        "                prediction = \"‚ö†Ô∏è  Low similarity likely (tokenization issues)\"\n",
        "            else:\n",
        "                prediction = \"ü§î Moderate similarity possible\"\n",
        "            \n",
        "            print(f\"   üí° Similarity prediction: {prediction}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {model_name}: {e}\")\n",
        "\n",
        "# Test with concrete examples from our corpus\n",
        "word_pairs = [\n",
        "    (\"doctor\", \"Dokter\", \"medical professional\"),\n",
        "    (\"diagnosis\", \"Diagnos\", \"medical assessment\"),  \n",
        "    (\"carefully\", \"roueg\", \"with care\"),\n",
        "    (\"patient\", \"Patient\", \"sick person\"),\n",
        "    (\"explains\", \"erkl√§ert\", \"makes clear\")\n",
        "]\n",
        "\n",
        "# Test with our models to see quality differences\n",
        "test_models = [\"bert-base-multilingual-cased\", \"xlm-roberta-base\"]\n",
        "\n",
        "for model in test_models:\n",
        "    demonstrate_tokenization_quality(word_pairs, model)\n",
        "\n",
        "print(f\"\\nüí° INTERPRETATION:\")\n",
        "print(f\"   üü¢ Good tokenization ‚Üí Better embeddings ‚Üí Higher similarity scores\")\n",
        "print(f\"   üî¥ Poor tokenization ‚Üí Worse embeddings ‚Üí Lower similarity scores\")\n",
        "print(f\"   This explains why some language pairs might score lower than expected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "434b72d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SIMILARITY ANALYSIS & INTERPRETATION\n",
        "# ============================================================================\n",
        "\n",
        "# Analyze the similarity results with automatic interpretation\n",
        "print(\"üîç DETAILED SIMILARITY ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get language names from our test sentences\n",
        "lang_names = list(test_sentences.keys())\n",
        "\n",
        "# Calculate cross-lingual similarities (excluding self-comparisons)\n",
        "cross_lingual_similarities = []\n",
        "print(\"\\nüìä Cross-lingual Similarity Scores:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for i, lang1 in enumerate(lang_names):\n",
        "    for j, lang2 in enumerate(lang_names):\n",
        "        if i < j:  # Avoid duplicates and self-comparisons\n",
        "            sim = similarity_matrix[i, j]\n",
        "            cross_lingual_similarities.append(sim)\n",
        "            \n",
        "            # Provide automatic interpretation\n",
        "            if sim >= 0.8:\n",
        "                quality = \"üü¢ EXCELLENT\"\n",
        "                note = \"Very strong semantic alignment\"\n",
        "            elif sim >= 0.7:\n",
        "                quality = \"üü° GOOD\"  \n",
        "                note = \"Clear semantic similarity\"\n",
        "            elif sim >= 0.5:\n",
        "                quality = \"üü† MODERATE\"\n",
        "                note = \"Some semantic overlap, could be better\"\n",
        "            else:\n",
        "                quality = \"üî¥ CONCERNING\"\n",
        "                note = \"Weak alignment - investigate model/language\"\n",
        "                \n",
        "            print(f\"   {lang1:12} ‚Üî {lang2:12}: {sim:.3f} | {quality} - {note}\")\n",
        "\n",
        "# Calculate summary statistics\n",
        "if cross_lingual_similarities:\n",
        "    avg_similarity = sum(cross_lingual_similarities) / len(cross_lingual_similarities)\n",
        "    max_similarity = max(cross_lingual_similarities)\n",
        "    min_similarity = min(cross_lingual_similarities)\n",
        "    \n",
        "    print(f\"\\nüìà SUMMARY STATISTICS:\")\n",
        "    print(f\"   ‚Ä¢ Average cross-lingual similarity: {avg_similarity:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Best language pair similarity: {max_similarity:.3f}\")  \n",
        "    print(f\"   ‚Ä¢ Worst language pair similarity: {min_similarity:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Number of language pairs: {len(cross_lingual_similarities)}\")\n",
        "    \n",
        "    # Overall assessment\n",
        "    print(f\"\\nüéØ OVERALL MODEL ASSESSMENT:\")\n",
        "    if avg_similarity >= 0.75:\n",
        "        print(f\"   üéâ EXCELLENT: This model shows strong multilingual understanding!\")\n",
        "        print(f\"      ‚Üí Suitable for production multilingual applications\")\n",
        "    elif avg_similarity >= 0.60:\n",
        "        print(f\"   ‚úÖ GOOD: Model shows decent cross-lingual capabilities\")  \n",
        "        print(f\"      ‚Üí Usable for multilingual tasks with some caution\")\n",
        "    elif avg_similarity >= 0.45:\n",
        "        print(f\"   ‚ö†Ô∏è  FAIR: Model has limited multilingual alignment\")\n",
        "        print(f\"      ‚Üí Consider fine-tuning or using different model\")\n",
        "    else:\n",
        "        print(f\"   üö® POOR: Model struggles with multilingual understanding\")\n",
        "        print(f\"      ‚Üí Not recommended for cross-lingual applications\")\n",
        "        \n",
        "    print(f\"\\nüí° ACTIONABLE INSIGHTS:\")\n",
        "    print(f\"   ‚Ä¢ Use this analysis to choose appropriate models for your languages\")\n",
        "    print(f\"   ‚Ä¢ Lower scores indicate need for more training data or different architectures\")\n",
        "    print(f\"   ‚Ä¢ Compare different models using this same methodology\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b97aad9",
      "metadata": {},
      "source": [
        "## Similarity Analysis & Interpretation\n",
        "\n",
        "This section interprets cross-lingual cosine similarities and summarizes model quality with actionable insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "similarity_analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate semantic similarities\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "print(\"üîç SEMANTIC SIMILARITY ANALYSIS\")\n",
        "print(\"\\nSimilarity Matrix (1.0 = identical, 0.0 = unrelated):\")\n",
        "print()\n",
        "\n",
        "# Create a nice formatted table\n",
        "lang_names = list(test_sentences.keys())\n",
        "print(f\"{'Language':<12} \", end=\"\")\n",
        "for lang in lang_names:\n",
        "    print(f\"{lang:<10}\", end=\"\")\n",
        "print()\n",
        "\n",
        "for i, lang1 in enumerate(lang_names):\n",
        "    print(f\"{lang1:<12} \", end=\"\")\n",
        "    for j, lang2 in enumerate(lang_names):\n",
        "        sim = similarity_matrix[i, j]\n",
        "        print(f\"{sim:.3f}     \", end=\"\")\n",
        "    print()\n",
        "\n",
        "print(f\"\\nüí° Cross-lingual similarities (excluding self-comparisons):\")\n",
        "for i, lang1 in enumerate(lang_names):\n",
        "    for j, lang2 in enumerate(lang_names):\n",
        "        if i < j:  # Avoid duplicates\n",
        "            sim = similarity_matrix[i, j]\n",
        "            print(f\"   {lang1} ‚Üî {lang2}: {sim:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4bb5edd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üè¢ ENHANCED ANALYSIS: DOMAIN-SPECIFIC TOKENIZATION PATTERNS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üöÄ DOMAIN-SPECIFIC TOKENIZATION ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if 'domain' in df_results.columns:\n",
        "    print(\"üìä Available domains:\", list(df_results['domain'].unique()))\n",
        "    \n",
        "    # Domain-specific efficiency analysis\n",
        "    print(f\"\\nüéØ CROSS-DOMAIN TOKENIZATION EFFICIENCY:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    domain_summary = df_results.groupby(['domain', 'language'])['tokens_per_word'].mean().round(2)\n",
        "    \n",
        "    for domain in df_results['domain'].unique():\n",
        "        print(f\"\\nüìç {domain.upper()}:\")\n",
        "        try:\n",
        "            en_efficiency = domain_summary[domain]['English']  \n",
        "            lb_efficiency = domain_summary[domain]['Luxembourgish']\n",
        "            penalty = ((lb_efficiency - en_efficiency) / en_efficiency * 100)\n",
        "            \n",
        "            print(f\"   English:       {en_efficiency:.2f} tokens/word\")\n",
        "            print(f\"   Luxembourgish: {lb_efficiency:.2f} tokens/word\")\n",
        "            print(f\"   Resource penalty: {penalty:+.1f}%\")\n",
        "            \n",
        "            # Domain-specific interpretation\n",
        "            if penalty < 25:\n",
        "                assessment = \"üü¢ Minimal penalty\"\n",
        "                advice = \"Good multilingual coverage for this domain\"\n",
        "            elif penalty < 60:  \n",
        "                assessment = \"üü° Moderate penalty\"\n",
        "                advice = \"Acceptable but monitor computational costs\"\n",
        "            else:\n",
        "                assessment = \"üî¥ High penalty\"\n",
        "                advice = \"Consider domain-specific model fine-tuning\"\n",
        "                \n",
        "            print(f\"   Assessment: {assessment} - {advice}\")\n",
        "            \n",
        "        except KeyError as e:\n",
        "            print(f\"   ‚ö†Ô∏è  Missing data: {str(e)}\")\n",
        "    \n",
        "    # Most challenging domains\n",
        "    print(f\"\\nüéØ DOMAIN RANKING (by multilingual difficulty):\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    domain_penalties = {}\n",
        "    for domain in df_results['domain'].unique():\n",
        "        try:\n",
        "            en_eff = domain_summary[domain]['English']\n",
        "            lb_eff = domain_summary[domain]['Luxembourgish'] \n",
        "            penalty = ((lb_eff - en_eff) / en_eff * 100)\n",
        "            domain_penalties[domain] = penalty\n",
        "        except KeyError:\n",
        "            continue\n",
        "    \n",
        "    # Sort by difficulty (highest penalty = most challenging)\n",
        "    sorted_domains = sorted(domain_penalties.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    for rank, (domain, penalty) in enumerate(sorted_domains, 1):\n",
        "        difficulty = \"üî¥ High\" if penalty > 60 else \"üü° Medium\" if penalty > 25 else \"üü¢ Low\"\n",
        "        print(f\"   {rank}. {domain:12} ({penalty:+.1f}% penalty) - {difficulty} difficulty\")\n",
        "    \n",
        "    # Educational insights\n",
        "    print(f\"\\nüí° KEY INSIGHTS FOR STUDENTS:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(\"   ‚Ä¢ Technical domains (Medical, Technology) may have specialized vocabulary\")\n",
        "    print(\"   ‚Ä¢ Daily conversation may be easier for multilingual models\")\n",
        "    print(\"   ‚Ä¢ Business language shows formal vs informal tokenization patterns\")\n",
        "    print(\"   ‚Ä¢ Academic language tests model coverage of educational terms\")\n",
        "    print(\"   ‚Ä¢ Cross-domain consistency indicates robust multilingual training\")\n",
        "    \n",
        "else:\n",
        "    print(\"üìù Domain-specific analysis not available\")\n",
        "    print(\"   ‚Üí Use the expanded sentence pairs above to enable this analysis\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model_comparison",
      "metadata": {},
      "source": [
        "# Chapter 3: Model Comparison Summary\n",
        "\n",
        "Let's summarize what we've learned about different models and languages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create_summary",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a summary of our analysis\n",
        "summary_df = df_results.pivot_table(\n",
        "    index='language', \n",
        "    columns='model', \n",
        "    values=['tokens_per_word', 'num_tokens'], \n",
        "    aggfunc='mean'\n",
        ").round(2)\n",
        "\n",
        "print(\"üìä TOKENIZATION EFFICIENCY SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\nTokens per word (lower = more efficient):\")\n",
        "print(summary_df['tokens_per_word'])\n",
        "\n",
        "print(\"\\nTotal tokens per sentence:\")\n",
        "print(summary_df['num_tokens'])\n",
        "\n",
        "# Find the most efficient model for each language\n",
        "print(\"\\nüîç DEBUG INFO:\")\n",
        "print(f\"   Languages in test_sentences: {list(test_sentences.keys())}\")\n",
        "print(f\"   Languages in df_results: {list(df_results['language'].unique())}\")\n",
        "print(f\"   DataFrame shape: {df_results.shape}\")\n",
        "\n",
        "print(\"\\nüèÜ RECOMMENDATIONS:\")\n",
        "# Use the languages that actually exist in the DataFrame to avoid errors\n",
        "for lang in df_results['language'].unique():\n",
        "    lang_data = df_results[df_results['language'] == lang]\n",
        "    \n",
        "    if not lang_data.empty and len(lang_data) > 0:\n",
        "        try:\n",
        "            best_idx = lang_data['tokens_per_word'].idxmin()\n",
        "            best_model = lang_data.loc[best_idx, 'model']\n",
        "            best_ratio = lang_data['tokens_per_word'].min()\n",
        "            print(f\"   {lang:15}: Best model is {best_model} (ratio: {best_ratio:.2f})\")\n",
        "        except Exception as e:\n",
        "            print(f\"   {lang:15}: Error processing data - {str(e)}\")\n",
        "    else:\n",
        "        print(f\"   {lang:15}: No data available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81648b9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üîß FIX DATA STRUCTURE (Ensure df_results is a DataFrame)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üîç CHECKING DATA STRUCTURE:\")\n",
        "print(f\"   Type of df_results: {type(df_results)}\")\n",
        "\n",
        "# Ensure df_results is a DataFrame (fix for AttributeError)\n",
        "if isinstance(df_results, list):\n",
        "    print(\"   ‚ö†Ô∏è  Converting list to DataFrame...\")\n",
        "    df_results = pd.DataFrame(df_results)\n",
        "    print(f\"   ‚úÖ Converted! Shape: {df_results.shape}\")\n",
        "    print(f\"   üìä Columns: {list(df_results.columns)}\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ Already a DataFrame! Shape: {df_results.shape}\")\n",
        "\n",
        "print(f\"\\nüéØ READY FOR ANALYSIS!\")\n",
        "print(f\"   Data type: {type(df_results)}\")\n",
        "print(f\"   Available for pivot_table operations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
        "# üéì Session 1 Complete\n",
        "\n",
        "## What You've Learned\n",
        "\n",
        "Congratulations! You've explored the core foundations of Large Language Models:\n",
        "\n",
        "- ‚úÖ **Tokenization**: How models convert text into processable tokens\n",
        "- ‚úÖ **Cross-lingual Analysis**: Understanding language differences in model processing  \n",
        "- ‚úÖ **Text Embeddings**: Converting text to meaningful vector representations\n",
        "- ‚úÖ **Model Comparison**: Evaluating different architectures for your needs\n",
        "- ‚úÖ **Practical Skills**: Analyzing tokenization quality and embedding behavior\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Optional: Try It Yourself - Dialogue Summarization\n",
        "\n",
        "*Want to apply these concepts? Try creating your own dialogue summarization system using the foundations you've learned:*\n",
        "\n",
        "1. **Choose your own dialogue data** (conversations, meetings, chat logs)\n",
        "2. **Apply tokenization analysis** to understand processing costs\n",
        "3. **Use embeddings** to find similar conversation segments  \n",
        "4. **Compare models** for your specific language/domain\n",
        "5. **Implement TextRank** for extractive summarization (research the algorithm!)\n",
        "\n",
        "*This makes great homework or project work to deepen your understanding!*\n",
        "\n",
        "### Your Toolkit for Future Projects\n",
        "\n",
        "```python\n",
        "# Core functions you can reuse:\n",
        "analyze_tokenization(text, model_name)    # Compare tokenization efficiency\n",
        "embedder.encode(sentences)                # Create semantic embeddings\n",
        "cosine_similarity(embeddings)            # Measure text similarity\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
