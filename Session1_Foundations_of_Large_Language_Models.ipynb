{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b48c569d",
      "metadata": {},
      "source": [
        "# Session 1: Foundations of Large Language Models ü§ñ\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/Session1_Foundations_of_Large_Language_Models.ipynb)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
        "\n",
        "**üìö Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "**Core Concepts:**\n",
        "- **LLM Architecture** - Understand transformer models and attention mechanisms\n",
        "- **Tokenization** - How models process and understand text across languages\n",
        "- **Text Representation** - Embeddings, vectors, and semantic similarity\n",
        "- **Model Comparison** - Analyze different LLM architectures and capabilities\n",
        "- **Low-Resource Considerations** - Challenges with underrepresented languages\n",
        "\n",
        "**Practical Skills:**\n",
        "- Compare tokenization across different models\n",
        "- Analyze model behavior with multilingual text\n",
        "- Implement basic text processing pipelines\n",
        "- Evaluate model performance on various languages\n",
        "- Build foundation for advanced NLP applications\n",
        "\n",
        "**Why This Matters:** Understanding LLM fundamentals is crucial for effective use in real-world applications, especially when working with diverse languages and limited computational resources.\n",
        "\n",
        "\n",
        "## Course Context\n",
        "\n",
        "| Session | Focus | Techniques | Prerequisites |\n",
        "|---------|-------|------------|---------------|\n",
        "| **Session 0** | Setup & Orientation | Environment, Basic Concepts | None |\n",
        "| **‚Üí This Session** | **LLM Foundations** | **Tokenization, Embeddings, Model Analysis** | **Session 0** |\n",
        "| **Session 2** | Prompt Engineering | Advanced Prompting, Chain-of-Thought | Sessions 0-1 |\n",
        "| **Session 3** | Fine-tuning | LoRA, QLoRA, Custom Training | Sessions 0-2 |\n",
        "| **Session 4** | Bias & Ethics | Fairness, Evaluation, Mitigation | Sessions 0-3 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup_section",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Environment Setup\n",
        "\n",
        "### What This Section Does\n",
        "This section prepares your coding environment with all necessary libraries for exploring Large Language Model foundations. We'll install packages optimized for **interactive learning** - educational, efficient, and GPU-optional!\n",
        "\n",
        "### Why These Specific Packages?\n",
        "\n",
        "**Core Dependencies:**\n",
        "- `numpy` + `pandas`: Essential for data manipulation and analysis\n",
        "- `scikit-learn`: Similarity metrics and basic ML utilities\n",
        "- `matplotlib`: Visualization of model behaviors and comparisons\n",
        "\n",
        "**LLM Ecosystem:**\n",
        "- `transformers`: Access to pretrained models and tokenizers\n",
        "- `sentence-transformers`: Semantic embeddings and similarity\n",
        "- `torch`: PyTorch backend for model operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install_packages",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick setup for this session\n",
        "!pip install -q transformers sentence-transformers scikit-learn matplotlib pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports_setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports for LLM foundations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"‚úÖ Environment ready for LLM foundations exploration!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tokenization_chapter",
      "metadata": {},
      "source": [
        "# Chapter 1: Understanding Tokenization\n",
        "\n",
        "## What We'll Explore\n",
        "\n",
        "Tokenization is how models convert text into numbers they can process. Let's see how this works with different languages and models.\n",
        "\n",
        "### Step 1: Prepare Test Sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6f525b6",
      "metadata": {},
      "source": [
        "**Model Selection:** We'll compare two popular multilingual models from [Hugging Face Hub](https://huggingface.co/models):\n",
        "\n",
        "- **BERT** (Google): Bidirectional Encoder Representations from Transformers - one of the first successful transformer models\n",
        "- **XLM-RoBERTa** (Facebook): Cross-lingual Language Model based on RoBERTa - specifically designed for multilingual tasks\n",
        "\n",
        "These model names are the official identifiers used to download them from Hugging Face's model repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test_sentences",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Multilingual Test Corpus Definition\n",
        "\n",
        "This corpus contains semantically equivalent sentences across three languages \n",
        "representing different language families and resource levels:\n",
        "- English: Germanic, high-resource language\n",
        "- Luxembourgish: Germanic, low-resource language\n",
        "- French: Romance, high-resource language\n",
        "\n",
        "Domain: Medical/Healthcare (to test domain-specific tokenization)\n",
        "Semantic equivalence: All sentences convey the same meaning\n",
        "\n",
        "Research Question: \n",
        "    How do multilingual models handle typologically similar vs. different \n",
        "    languages with varying resource availability?\n",
        "\n",
        "Expected Findings (Hypothesis):\n",
        "    1. Resource Availability Effect:\n",
        "       - English & French (high-resource) ‚Üí Lower tokens-per-word ratio\n",
        "       - Luxembourgish (low-resource) ‚Üí Higher tokens-per-word ratio\n",
        "       - Reason: Models trained predominantly on high-resource languages learn\n",
        "                 better subword representations for those languages\n",
        "    \n",
        "    2. Typological Similarity:\n",
        "       - English ‚Üî Luxembourgish (both Germanic): May show some overlap in \n",
        "         tokenization patterns despite resource difference\n",
        "       - French (Romance) vs. Germanic languages: Different morphological \n",
        "         patterns may lead to different tokenization strategies\n",
        "    \n",
        "    3. Model Architecture Differences:\n",
        "       - BERT: Trained on fewer languages, may show stronger resource bias\n",
        "       - XLM-RoBERTa: Trained on 100 languages, may handle low-resource \n",
        "         languages more efficiently\n",
        "\n",
        "Practical Implications:\n",
        "    If Luxembourgish requires 2-3x more tokens than English:\n",
        "    ‚Üí Processing costs increase proportionally\n",
        "    ‚Üí Context window fills up faster (fewer words fit in same token budget)\n",
        "    ‚Üí Inference latency increases\n",
        "    ‚Üí This quantifies the \"low-resource penalty\" in production systems\n",
        "\n",
        "Note: You may substitute these examples with sentences from your target language\n",
        "      and domain for comparative analysis.\n",
        "\"\"\"\n",
        "\n",
        "# Multilingual test corpus\n",
        "test_sentences = {\n",
        "    \"English\": \"The doctor explains the diagnosis carefully to the patient.\",\n",
        "    \"Luxembourgish\": \"Den Dokter erkl√§ert d'Diagnos ganz roueg dem Patient.\",\n",
        "    \"French\": \"Le m√©decin explique le diagnostic avec soin au patient.\"\n",
        "}\n",
        "\n",
        "# Display corpus for verification\n",
        "print(\"=\" * 70)\n",
        "print(\"MULTILINGUAL TEST CORPUS\")\n",
        "print(\"=\" * 70)\n",
        "for language, sentence in test_sentences.items():\n",
        "    word_count = len(sentence.split())\n",
        "    char_count = len(sentence)\n",
        "    print(f\"\\n{language:15} | Words: {word_count:2d} | Characters: {char_count:3d}\")\n",
        "    print(f\"{'':15} | {sentence}\")\n",
        "print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tokenization_analysis",
      "metadata": {},
      "source": [
        "### Step 2: Compare Tokenization Across Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "compare_tokenization",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMPREHENSIVE TOKENIZATION COMPARISON ACROSS MODEL ARCHITECTURES\n",
        "# ============================================================================\n",
        "# These models represent different tokenization algorithms and training approaches:\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "models_to_compare = [\n",
        "    \"bert-base-multilingual-cased\",        # WordPiece, multilingual\n",
        "    \"xlm-roberta-base\",                    # SentencePiece, multilingual\n",
        "    #\"google/mt5-small\",                    # SentencePiece, multilingual encoder-decoder\n",
        "    #\"gpt2\",                                # BPE, English only (no spaces before non English chars)\n",
        "    \"google/gemma-2-2b-it\"                  # SentencePiece/Unigram, decoder-only chat-style model\n",
        "]\n",
        "\n",
        "text_en = \"Students are learning about large language models.\"\n",
        "text_lr = \"D'Studenten l√©ieren iwwer grouss Sproochmodeller.\"  # replace with your low resource sentence\n",
        "\n",
        "def show_tokenization(model_name, text):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(\"Text :\", text)\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"Number of tokens:\", len(tokens))\n",
        "    \n",
        "    # Return data for DataFrame creation\n",
        "    return {\n",
        "        'model': model_name.split('/')[-1],  # Short name\n",
        "        'text': text,\n",
        "        'num_tokens': len(tokens),\n",
        "        'num_words': len(text.split()),\n",
        "        'tokens_per_word': len(tokens) / len(text.split()) if text.split() else 0,\n",
        "        'tokens_preview': tokens[:5]  # First 5 tokens for reference\n",
        "    }\n",
        "\n",
        "# Collect results for analysis\n",
        "df_results = []\n",
        "\n",
        "print(\"üî§ ENGLISH TEXT ANALYSIS\")\n",
        "print(\"-\" * 40)\n",
        "for model_name in models_to_compare:\n",
        "    result = show_tokenization(model_name, text_en)\n",
        "    result['language'] = 'English'\n",
        "    df_results.append(result)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\nüåç LOW RESOURCE LANGUAGE EXAMPLES\\n\")\n",
        "\n",
        "for model_name in models_to_compare:\n",
        "    result = show_tokenization(model_name, text_lr)\n",
        "    result['language'] = 'Luxembourgish'  # or whatever your low-resource language is\n",
        "    df_results.append(result)\n",
        "\n",
        "# Convert to pandas DataFrame for easy analysis\n",
        "import pandas as pd\n",
        "df_results = pd.DataFrame(df_results)\n",
        "\n",
        "print(f\"\\nüìä Results collected in DataFrame: {len(df_results)} entries\")\n",
        "print(f\"    Columns: {list(df_results.columns)}\")\n",
        "print(f\"    Ready for summary analysis!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb45c876",
      "metadata": {},
      "source": [
        "### üîç Inspecting Tokenizer Types Programmatically\n",
        "\n",
        "Sometimes you need to determine what tokenization algorithm a model uses (WordPiece, BPE, SentencePiece, etc.). While there's no universal flag, you can inspect the tokenizer programmatically:\n",
        "\n",
        "**Why This Matters:**\n",
        "- Different algorithms handle subwords differently\n",
        "- Understanding the algorithm helps predict tokenization behavior\n",
        "- Important for debugging and optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b6b9cd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TOKENIZER INTROSPECTION: Understanding Algorithm Types\n",
        "# ============================================================================\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"xlm-roberta-base\"\n",
        "tok = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(\"Tokenizer class:\", tok.__class__.__name__)\n",
        "print(\"Backend:\", getattr(tok, \"backend_tokenizer\", None))\n",
        "print(\"Special tokens:\", tok.special_tokens_map)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f602c90",
      "metadata": {},
      "source": [
        "### üéØ Key Takeaways: Tokenization Algorithms in Practice\n",
        "\n",
        "**Understanding these differences helps you:**\n",
        "\n",
        "1. **Choose the Right Model**: \n",
        "   - Need to handle many languages? ‚Üí SentencePiece models (XLM-RoBERTa, mT5)\n",
        "   - Working primarily with English? ‚Üí WordPiece or BPE might be sufficient\n",
        "   - Need fast inference? ‚Üí Consider algorithm efficiency for your text type\n",
        "\n",
        "2. **Predict Performance**:\n",
        "   - SentencePiece typically handles low-resource languages better\n",
        "   - WordPiece good for languages with complex morphology\n",
        "   - BPE optimized for languages similar to training data\n",
        "\n",
        "3. **Debug Issues**:\n",
        "   - Unexpected tokenization? Check the algorithm type\n",
        "   - High token counts? Algorithm might not be suited for your language\n",
        "   - Special token conflicts? Inspect the special_tokens_map\n",
        "\n",
        "**Next**: Let's see how these tokenization differences affect semantic representations..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tokenization_reflection",
      "metadata": {},
      "source": [
        "### ü§î Reflection Questions\n",
        "\n",
        "Look at the results above and consider:\n",
        "\n",
        "- Which language uses more tokens per word?\n",
        "- How might more tokens affect inference cost and speed?\n",
        "- Do you see any unusual token splits (broken words, weird subwords)?\n",
        "\n",
        "**Key Insight:** Languages with fewer training examples often get split into more subword tokens, increasing computational costs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "embeddings_chapter",
      "metadata": {},
      "source": [
        "# üìä Chapter 2: Text Embeddings & Semantic Similarity\n",
        "\n",
        "## Understanding Vector Representations\n",
        "\n",
        "**What are embeddings?** Numbers that capture the meaning of text in high-dimensional space.\n",
        "\n",
        "Let's see how different models create these representations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sentence_embeddings",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a multilingual sentence embedding model\n",
        "embedder_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "embedder = SentenceTransformer(embedder_name, device=device)\n",
        "\n",
        "print(f\"üìä Loaded embedding model: {embedder_name}\")\n",
        "\n",
        "# Get embeddings for our test sentences\n",
        "sentences = list(test_sentences.values())\n",
        "languages = list(test_sentences.keys())\n",
        "\n",
        "embeddings = embedder.encode(sentences, convert_to_numpy=True)\n",
        "print(f\"‚úÖ Created embeddings with shape: {embeddings.shape}\")\n",
        "print(f\"   Each sentence ‚Üí {embeddings.shape[1]} dimensional vector\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13d85cbb",
      "metadata": {},
      "source": [
        "## üìä Understanding PCA (Principal Component Analysis)\n",
        "\n",
        "**ü§î The Problem:** Our embeddings are 384-dimensional vectors - impossible to visualize directly!\n",
        "\n",
        "**üéØ The Solution:** PCA reduces high-dimensional data to 2D while preserving the most important relationships.\n",
        "\n",
        "### üìö How PCA Works:\n",
        "\n",
        "1. **Find Principal Components**: Directions in the data with maximum variance\n",
        "2. **Project Data**: Transform original data onto these new axes\n",
        "3. **Keep Top Components**: Use only the first 2 components for 2D visualization\n",
        "\n",
        "### üí° Key Insights:\n",
        "\n",
        "- **Component 1**: Captures the most variation in the data\n",
        "- **Component 2**: Captures the second most variation  \n",
        "- **Relationship Preservation**: Similar sentences should stay close even after reduction\n",
        "- **Information Loss**: We lose some information, but keep the most important patterns\n",
        "\n",
        "### üéØ Why This Matters:\n",
        "\n",
        "- Allows us to **visualize** high-dimensional embeddings\n",
        "- Helps us **understand** if similar meanings cluster together across languages\n",
        "- **Quality check** for our multilingual model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142cbf7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üî¨ APPLYING PCA FOR VISUALIZATION  \n",
        "# ============================================================================\n",
        "\n",
        "print(\"üìä PCA ANALYSIS\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"üìê Original embedding dimensions: {embeddings.shape[1]}\")\n",
        "print(f\"üéØ Reducing to: 2 dimensions for plotting\") \n",
        "print(f\"‚ö° Method: Principal Component Analysis\")\n",
        "\n",
        "# Apply PCA reduction\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "coords_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Analyze the results\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "print(f\"\\nüìä VARIANCE EXPLANATION:\")\n",
        "print(f\"   ‚Ä¢ Component 1: {explained_var[0]*100:.1f}% of original variance\")\n",
        "print(f\"   ‚Ä¢ Component 2: {explained_var[1]*100:.1f}% of original variance\") \n",
        "print(f\"   ‚Ä¢ Total retained: {sum(explained_var)*100:.1f}% of information\")\n",
        "\n",
        "print(f\"\\nüí° INTERPRETATION:\")\n",
        "if sum(explained_var) > 0.7:\n",
        "    print(f\"   ‚úÖ Great! We retained most of the important patterns\")\n",
        "elif sum(explained_var) > 0.5:\n",
        "    print(f\"   ‚ö†Ô∏è  Decent retention - visualization should be meaningful\")\n",
        "else:\n",
        "    print(f\"   üî¥ Low retention - visualization may not show all relationships\")\n",
        "\n",
        "print(f\"\\nüéØ COORDINATES READY FOR PLOTTING:\")\n",
        "print(f\"   Shape: {coords_2d.shape} (each sentence ‚Üí x,y coordinates)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "visualize_embeddings",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üé® VISUALIZE PCA RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "# Create visualization (using coords_2d from previous cell)\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "for i, (lang, sentence) in enumerate(test_sentences.items()):\n",
        "    plt.scatter(coords_2d[i, 0], coords_2d[i, 1], \n",
        "               c=colors[i], s=200, alpha=0.7, label=lang)\n",
        "    plt.annotate(lang, (coords_2d[i, 0], coords_2d[i, 1]), \n",
        "                xytext=(10, 10), textcoords='offset points', fontsize=12)\n",
        "\n",
        "plt.title(\"Sentence Embeddings in 2D Space\\n(All sentences have similar meaning)\", fontsize=14)\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üí° Key Observation: Similar-meaning sentences in different languages should cluster together!\")\n",
        "\n",
        "print(f\"\\nüî¨ PCA VISUALIZATION ANALYSIS:\")\n",
        "print(f\"   üìè What distance means: Closer points = more similar semantic meaning\")\n",
        "print(f\"   üéØ What to look for: Languages clustering together despite different words\")\n",
        "print(f\"   ‚öñÔ∏è  What variance tells us: Higher variance = more distinguishable patterns\")\n",
        "print(f\"   üåç Cross-lingual success: Different languages expressing same meaning should be near each other\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a59b24d4",
      "metadata": {},
      "source": [
        "### üìä Understanding Similarity Values: What Do The Numbers Mean?\n",
        "\n",
        "The cosine similarity values you see above tell us how semantically similar the sentences are. Here's how to interpret them:\n",
        "\n",
        "**üìê Cosine Similarity Scale (0.0 to 1.0):**\n",
        "- **0.9-1.0**: Nearly identical meaning (excellent cross-lingual alignment)\n",
        "- **0.7-0.89**: High similarity (strong semantic equivalence) \n",
        "- **0.5-0.69**: Moderate similarity (related concepts, some semantic overlap)\n",
        "- **0.3-0.49**: Low similarity (weakly related or different topics)\n",
        "- **0.0-0.29**: Very low similarity (mostly unrelated concepts)\n",
        "\n",
        "**What To Expect for Our Semantically Equivalent Sentences:**\n",
        "- **Good multilingual models**: Should show 0.7-0.9+ similarity across languages\n",
        "- **Diagonal values**: Should always be 1.0 (sentence compared to itself)\n",
        "- **Lower than expected scores**: May indicate model struggles with certain languages\n",
        "\n",
        "**Real-World Implications:**\n",
        "- **High scores (>0.7)**: Model is suitable for multilingual applications like translation, search\n",
        "- **Medium scores (0.5-0.7)**: Proceed with caution, may need language-specific tuning\n",
        "- **Low scores (<0.5)**: Consider different model or additional training for that language\n",
        "\n",
        "**Why Scores Might Be Lower Than Expected:**\n",
        "- Model had limited training data in the low-resource language\n",
        "- Different sentence structures or vocabulary between languages  \n",
        "- Domain mismatch (model trained on general text, tested on medical text)\n",
        "- **Tokenization issues affecting embedding quality** ‚Üê Let's explain this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17920c42",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CALCULATE SIMILARITY MATRIX (Required for Later Analysis)\n",
        "# ============================================================================\n",
        "\n",
        "# Calculate pairwise cosine similarities between all sentence embeddings  \n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "print(\"üîó SIMILARITY MATRIX CALCULATED\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"‚úÖ Matrix shape: {similarity_matrix.shape}\")\n",
        "print(f\"‚úÖ Values range from 0.0 (unrelated) to 1.0 (identical)\")\n",
        "print(f\"‚úÖ Ready for detailed analysis in upcoming cells\")\n",
        "\n",
        "# Quick preview of the matrix\n",
        "print(f\"\\nüìä Quick Preview (first few values):\")\n",
        "lang_names = list(test_sentences.keys())\n",
        "for i in range(min(2, len(lang_names))):\n",
        "    for j in range(min(2, len(lang_names))):\n",
        "        sim = similarity_matrix[i, j]\n",
        "        print(f\"   {lang_names[i]} ‚Üî {lang_names[j]}: {sim:.3f}\")\n",
        "\n",
        "print(f\"\\nüí° Full analysis coming in the next sections!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff61a5c",
      "metadata": {},
      "source": [
        "### üîß Deep Dive: How Tokenization Issues Affect Embedding Quality\n",
        "\n",
        "**The Connection:** Tokenization ‚Üí Embeddings ‚Üí Similarity Scores\n",
        "\n",
        "This is a crucial concept that many people overlook! Here's how poor tokenization can ruin your similarity analysis:\n",
        "\n",
        "#### üß© **The Process Chain:**\n",
        "```\n",
        "Raw Text ‚Üí Tokenization ‚Üí Token Embeddings ‚Üí Sentence Embedding ‚Üí Similarity Score\n",
        "```\n",
        "\n",
        "**When tokenization goes wrong, everything downstream suffers!**\n",
        "\n",
        "#### üìù **Concrete Examples:**\n",
        "\n",
        "**Example 1: Word Breaking**\n",
        "```\n",
        "English: \"carefully\" ‚Üí [\"careful\", \"##ly\"] (good: preserves meaning)\n",
        "Low-resource: \"sorgf√§ltig\" ‚Üí [\"so\", \"##r\", \"##g\", \"##f√§\", \"##lt\", \"##ig\"] (bad: loses word structure)\n",
        "```\n",
        "\n",
        "**Impact:** The low-resource word gets broken into meaningless fragments. The model can't learn that \"sorgf√§ltig\" = \"carefully\" because it never sees \"sorgf√§ltig\" as a coherent unit.\n",
        "\n",
        "**Example 2: Unknown Token Explosion**\n",
        "```\n",
        "English: \"doctor\" ‚Üí [\"doctor\"] (1 token, well-known)\n",
        "Low-resource: \"Dokter\" ‚Üí [\"[UNK]\"] (1 unknown token, no meaning)\n",
        "```\n",
        "\n",
        "**Impact:** The model has no representation for \"[UNK]\", so it gets a generic \"unknown\" embedding that doesn't capture the medical concept.\n",
        "\n",
        "**Example 3: Inconsistent Splitting**\n",
        "```\n",
        "Same concept, different tokenization:\n",
        "\"diagnosis\" ‚Üí [\"diagnosis\"] \n",
        "\"Diagnos\" ‚Üí [\"Dia\", \"##gno\", \"##s\"]\n",
        "```\n",
        "\n",
        "**Impact:** Even though both mean \"diagnosis,\" they get completely different embeddings because the tokenizer treats them as unrelated token sequences.\n",
        "\n",
        "#### ‚ö° **The Cascade Effect:**\n",
        "\n",
        "1. **Bad tokenization** ‚Üí Fragments or unknown tokens\n",
        "2. **Poor token embeddings** ‚Üí Generic or meaningless vectors  \n",
        "3. **Bad sentence embeddings** ‚Üí Average of poor-quality token vectors\n",
        "4. **Low similarity scores** ‚Üí Model appears to \"not understand\" the language\n",
        "\n",
        "#### üõ°Ô∏è **How to Detect This:**\n",
        "- Look at tokenization output: many tiny fragments = problem\n",
        "- High number of [UNK] tokens = problem  \n",
        "- Same meaning, very different token patterns = problem\n",
        "\n",
        "#### üí° **Solutions:**\n",
        "- Choose models trained specifically on your target language\n",
        "- Use SentencePiece-based models (better with unseen languages)\n",
        "- Consider domain-specific models if your text has specialized vocabulary\n",
        "- Fine-tune tokenizers on your target language data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fc71b0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üéì STUDENT GUIDE: How to Use Gated Models in Colab (Optional Advanced Section)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "üìö QUESTION: How can students use Gemma (or other gated models) in Google Colab?\n",
        "\n",
        "‚úÖ ANSWER: Follow these steps (one-time setup per student):\n",
        "\n",
        "STEP 1: Get Model Access (Outside of Colab)\n",
        "==========================================\n",
        "1. Go to: https://huggingface.co/google/gemma-2-2b-it\n",
        "2. Click the \"Request Access\" button\n",
        "3. Wait for approval from Google\n",
        "4. You'll get an email when approved\n",
        "\n",
        "STEP 2: Create Hugging Face Token (Outside of Colab)  \n",
        "===================================================\n",
        "1. Go to: https://huggingface.co/settings/tokens\n",
        "2. Click \"New token\"\n",
        "3. Choose \"Read\" permissions (sufficient for downloading models)\n",
        "4. Copy the token (starts with \"hf_...\")\n",
        "\n",
        "STEP 3: Authenticate in Colab (Every Session)\n",
        "=============================================\n",
        "Run this code at the start of your Colab session:\n",
        "\"\"\"\n",
        "\n",
        "print(\"üîë TO USE GATED MODELS IN COLAB:\")\n",
        "print(\"1. Get model access approval (one-time)\")  \n",
        "print(\"2. Create HF token (one-time)\")\n",
        "print(\"3. Login in Colab (every session)\")\n",
        "print(\"\\nExample authentication code for Colab:\")\n",
        "print(\"-\" * 40)\n",
        "print(\"# Option A: Interactive login (recommended for beginners)\")\n",
        "print(\"from huggingface_hub import notebook_login\")\n",
        "print(\"notebook_login()  # This will show a popup to enter your token\")\n",
        "print()\n",
        "print(\"# Option B: Direct token login (for advanced users)\")  \n",
        "print(\"from huggingface_hub import login\")\n",
        "print(\"login(token='hf_your_token_here')  # Replace with your actual token\")\n",
        "print()\n",
        "print(\"# Option C: Environment variable (most secure)\")\n",
        "print(\"import os\")\n",
        "print(\"os.environ['HF_TOKEN'] = 'your_token_here'\")\n",
        "print(\"from huggingface_hub import login\") \n",
        "print(\"login()\")\n",
        "\n",
        "print(f\"\\nüí° AFTER AUTHENTICATION:\")\n",
        "print(f\"   Just uncomment the gated model in the list above!\")\n",
        "print(f\"   models_to_compare.append('google/gemma-2-2b-it')\")\n",
        "\n",
        "print(f\"\\nüéØ FOR INSTRUCTORS:\")\n",
        "print(f\"   ‚Ä¢ You could demo this live for interested students\")\n",
        "print(f\"   ‚Ä¢ Or provide it as bonus/homework material\") \n",
        "print(f\"   ‚Ä¢ Main tutorial works fine with public models only\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6333bc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PRACTICAL DEMONSTRATION: Tokenization Quality Impact\n",
        "# ============================================================================\n",
        "\n",
        "def demonstrate_tokenization_quality(word_pairs, model_name):\n",
        "    \"\"\"\n",
        "    Show how tokenization quality varies between equivalent words across languages.\n",
        "    \n",
        "    Args:\n",
        "        word_pairs: List of (lang1_word, lang2_word, meaning) tuples\n",
        "        model_name: HuggingFace model to test\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç TOKENIZATION QUALITY ANALYSIS: {model_name}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        \n",
        "        for word1, word2, meaning in word_pairs:\n",
        "            tokens1 = tokenizer.tokenize(word1)\n",
        "            tokens2 = tokenizer.tokenize(word2)\n",
        "            \n",
        "            # Count fragmentations and unknowns\n",
        "            frag1 = len(tokens1)\n",
        "            frag2 = len(tokens2)\n",
        "            unk1 = sum(1 for t in tokens1 if '[UNK]' in t or '<unk>' in t)\n",
        "            unk2 = sum(1 for t in tokens2 if '[UNK]' in t or '<unk>' in t)\n",
        "            \n",
        "            # Quality assessment\n",
        "            quality1 = \"üü¢ Good\" if frag1 == 1 and unk1 == 0 else (\"üü° OK\" if unk1 == 0 else \"üî¥ Poor\")\n",
        "            quality2 = \"üü¢ Good\" if frag2 == 1 and unk2 == 0 else (\"üü° OK\" if unk2 == 0 else \"üî¥ Poor\")\n",
        "            \n",
        "            print(f\"\\nüìù Concept: '{meaning}'\")\n",
        "            print(f\"   {word1:15} ‚Üí {tokens1} | Fragments: {frag1}, UNK: {unk1} | {quality1}\")\n",
        "            print(f\"   {word2:15} ‚Üí {tokens2} | Fragments: {frag2}, UNK: {unk2} | {quality2}\")\n",
        "            \n",
        "            # Predict embedding quality\n",
        "            if quality1 == quality2 == \"üü¢ Good\":\n",
        "                prediction = \"üéØ High similarity expected\"\n",
        "            elif \"üî¥ Poor\" in [quality1, quality2]:\n",
        "                prediction = \"‚ö†Ô∏è  Low similarity likely (tokenization issues)\"\n",
        "            else:\n",
        "                prediction = \"ü§î Moderate similarity possible\"\n",
        "            \n",
        "            print(f\"   üí° Similarity prediction: {prediction}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {model_name}: {e}\")\n",
        "\n",
        "# Test with concrete examples from our corpus\n",
        "word_pairs = [\n",
        "    (\"doctor\", \"Dokter\", \"medical professional\"),\n",
        "    (\"diagnosis\", \"Diagnos\", \"medical assessment\"),  \n",
        "    (\"carefully\", \"roueg\", \"with care\"),\n",
        "    (\"patient\", \"Patient\", \"sick person\"),\n",
        "    (\"explains\", \"erkl√§ert\", \"makes clear\")\n",
        "]\n",
        "\n",
        "# Test with our models to see quality differences\n",
        "test_models = [\"bert-base-multilingual-cased\", \"xlm-roberta-base\"]\n",
        "\n",
        "for model in test_models:\n",
        "    demonstrate_tokenization_quality(word_pairs, model)\n",
        "\n",
        "print(f\"\\nüí° INTERPRETATION:\")\n",
        "print(f\"   üü¢ Good tokenization ‚Üí Better embeddings ‚Üí Higher similarity scores\")\n",
        "print(f\"   üî¥ Poor tokenization ‚Üí Worse embeddings ‚Üí Lower similarity scores\")\n",
        "print(f\"   This explains why some language pairs might score lower than expected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "434b72d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SIMILARITY ANALYSIS & INTERPRETATION\n",
        "# ============================================================================\n",
        "\n",
        "# Analyze the similarity results with automatic interpretation\n",
        "print(\"üîç DETAILED SIMILARITY ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get language names from our test sentences\n",
        "lang_names = list(test_sentences.keys())\n",
        "\n",
        "# Calculate cross-lingual similarities (excluding self-comparisons)\n",
        "cross_lingual_similarities = []\n",
        "print(\"\\nüìä Cross-lingual Similarity Scores:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for i, lang1 in enumerate(lang_names):\n",
        "    for j, lang2 in enumerate(lang_names):\n",
        "        if i < j:  # Avoid duplicates and self-comparisons\n",
        "            sim = similarity_matrix[i, j]\n",
        "            cross_lingual_similarities.append(sim)\n",
        "            \n",
        "            # Provide automatic interpretation\n",
        "            if sim >= 0.8:\n",
        "                quality = \"üü¢ EXCELLENT\"\n",
        "                note = \"Very strong semantic alignment\"\n",
        "            elif sim >= 0.7:\n",
        "                quality = \"üü° GOOD\"  \n",
        "                note = \"Clear semantic similarity\"\n",
        "            elif sim >= 0.5:\n",
        "                quality = \"üü† MODERATE\"\n",
        "                note = \"Some semantic overlap, could be better\"\n",
        "            else:\n",
        "                quality = \"üî¥ CONCERNING\"\n",
        "                note = \"Weak alignment - investigate model/language\"\n",
        "                \n",
        "            print(f\"   {lang1:12} ‚Üî {lang2:12}: {sim:.3f} | {quality} - {note}\")\n",
        "\n",
        "# Calculate summary statistics\n",
        "if cross_lingual_similarities:\n",
        "    avg_similarity = sum(cross_lingual_similarities) / len(cross_lingual_similarities)\n",
        "    max_similarity = max(cross_lingual_similarities)\n",
        "    min_similarity = min(cross_lingual_similarities)\n",
        "    \n",
        "    print(f\"\\nüìà SUMMARY STATISTICS:\")\n",
        "    print(f\"   ‚Ä¢ Average cross-lingual similarity: {avg_similarity:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Best language pair similarity: {max_similarity:.3f}\")  \n",
        "    print(f\"   ‚Ä¢ Worst language pair similarity: {min_similarity:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Number of language pairs: {len(cross_lingual_similarities)}\")\n",
        "    \n",
        "    # Overall assessment\n",
        "    print(f\"\\nüéØ OVERALL MODEL ASSESSMENT:\")\n",
        "    if avg_similarity >= 0.75:\n",
        "        print(f\"   üéâ EXCELLENT: This model shows strong multilingual understanding!\")\n",
        "        print(f\"      ‚Üí Suitable for production multilingual applications\")\n",
        "    elif avg_similarity >= 0.60:\n",
        "        print(f\"   ‚úÖ GOOD: Model shows decent cross-lingual capabilities\")  \n",
        "        print(f\"      ‚Üí Usable for multilingual tasks with some caution\")\n",
        "    elif avg_similarity >= 0.45:\n",
        "        print(f\"   ‚ö†Ô∏è  FAIR: Model has limited multilingual alignment\")\n",
        "        print(f\"      ‚Üí Consider fine-tuning or using different model\")\n",
        "    else:\n",
        "        print(f\"   üö® POOR: Model struggles with multilingual understanding\")\n",
        "        print(f\"      ‚Üí Not recommended for cross-lingual applications\")\n",
        "        \n",
        "    print(f\"\\nüí° ACTIONABLE INSIGHTS:\")\n",
        "    print(f\"   ‚Ä¢ Use this analysis to choose appropriate models for your languages\")\n",
        "    print(f\"   ‚Ä¢ Lower scores indicate need for more training data or different architectures\")\n",
        "    print(f\"   ‚Ä¢ Compare different models using this same methodology\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b97aad9",
      "metadata": {},
      "source": [
        "## Similarity Analysis & Interpretation\n",
        "\n",
        "This section interprets cross-lingual cosine similarities and summarizes model quality with actionable insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "similarity_analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate semantic similarities\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "print(\"üîç SEMANTIC SIMILARITY ANALYSIS\")\n",
        "print(\"\\nSimilarity Matrix (1.0 = identical, 0.0 = unrelated):\")\n",
        "print()\n",
        "\n",
        "# Create a nice formatted table\n",
        "lang_names = list(test_sentences.keys())\n",
        "print(f\"{'Language':<12} \", end=\"\")\n",
        "for lang in lang_names:\n",
        "    print(f\"{lang:<10}\", end=\"\")\n",
        "print()\n",
        "\n",
        "for i, lang1 in enumerate(lang_names):\n",
        "    print(f\"{lang1:<12} \", end=\"\")\n",
        "    for j, lang2 in enumerate(lang_names):\n",
        "        sim = similarity_matrix[i, j]\n",
        "        print(f\"{sim:.3f}     \", end=\"\")\n",
        "    print()\n",
        "\n",
        "print(f\"\\nüí° Cross-lingual similarities (excluding self-comparisons):\")\n",
        "for i, lang1 in enumerate(lang_names):\n",
        "    for j, lang2 in enumerate(lang_names):\n",
        "        if i < j:  # Avoid duplicates\n",
        "            sim = similarity_matrix[i, j]\n",
        "            print(f\"   {lang1} ‚Üî {lang2}: {sim:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model_comparison",
      "metadata": {},
      "source": [
        "# Chapter 3: Model Comparison Summary\n",
        "\n",
        "Let's summarize what we've learned about different models and languages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create_summary",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a summary of our analysis\n",
        "summary_df = df_results.pivot_table(\n",
        "    index='language', \n",
        "    columns='model', \n",
        "    values=['tokens_per_word', 'num_tokens'], \n",
        "    aggfunc='mean'\n",
        ").round(2)\n",
        "\n",
        "print(\"üìä TOKENIZATION EFFICIENCY SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\nTokens per word (lower = more efficient):\")\n",
        "print(summary_df['tokens_per_word'])\n",
        "\n",
        "print(\"\\nTotal tokens per sentence:\")\n",
        "print(summary_df['num_tokens'])\n",
        "\n",
        "# Find the most efficient model for each language\n",
        "print(\"\\nüîç DEBUG INFO:\")\n",
        "print(f\"   Languages in test_sentences: {list(test_sentences.keys())}\")\n",
        "print(f\"   Languages in df_results: {list(df_results['language'].unique())}\")\n",
        "print(f\"   DataFrame shape: {df_results.shape}\")\n",
        "\n",
        "print(\"\\nüèÜ RECOMMENDATIONS:\")\n",
        "# Use the languages that actually exist in the DataFrame to avoid errors\n",
        "for lang in df_results['language'].unique():\n",
        "    lang_data = df_results[df_results['language'] == lang]\n",
        "    \n",
        "    if not lang_data.empty and len(lang_data) > 0:\n",
        "        try:\n",
        "            best_idx = lang_data['tokens_per_word'].idxmin()\n",
        "            best_model = lang_data.loc[best_idx, 'model']\n",
        "            best_ratio = lang_data['tokens_per_word'].min()\n",
        "            print(f\"   {lang:15}: Best model is {best_model} (ratio: {best_ratio:.2f})\")\n",
        "        except Exception as e:\n",
        "            print(f\"   {lang:15}: Error processing data - {str(e)}\")\n",
        "    else:\n",
        "        print(f\"   {lang:15}: No data available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81648b9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üîß FIX DATA STRUCTURE (Ensure df_results is a DataFrame)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üîç CHECKING DATA STRUCTURE:\")\n",
        "print(f\"   Type of df_results: {type(df_results)}\")\n",
        "\n",
        "# Ensure df_results is a DataFrame (fix for AttributeError)\n",
        "if isinstance(df_results, list):\n",
        "    print(\"   ‚ö†Ô∏è  Converting list to DataFrame...\")\n",
        "    df_results = pd.DataFrame(df_results)\n",
        "    print(f\"   ‚úÖ Converted! Shape: {df_results.shape}\")\n",
        "    print(f\"   üìä Columns: {list(df_results.columns)}\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ Already a DataFrame! Shape: {df_results.shape}\")\n",
        "\n",
        "print(f\"\\nüéØ READY FOR ANALYSIS!\")\n",
        "print(f\"   Data type: {type(df_results)}\")\n",
        "print(f\"   Available for pivot_table operations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
        "# üéì Session 1 Complete\n",
        "\n",
        "## What You've Learned\n",
        "\n",
        "Congratulations! You've explored the core foundations of Large Language Models:\n",
        "\n",
        "- ‚úÖ **Tokenization**: How models convert text into processable tokens\n",
        "- ‚úÖ **Cross-lingual Analysis**: Understanding language differences in model processing  \n",
        "- ‚úÖ **Text Embeddings**: Converting text to meaningful vector representations\n",
        "- ‚úÖ **Model Comparison**: Evaluating different architectures for your needs\n",
        "- ‚úÖ **Practical Skills**: Analyzing tokenization quality and embedding behavior\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Optional: Try It Yourself - Dialogue Summarization\n",
        "\n",
        "*Want to apply these concepts? Try creating your own dialogue summarization system using the foundations you've learned:*\n",
        "\n",
        "1. **Choose your own dialogue data** (conversations, meetings, chat logs)\n",
        "2. **Apply tokenization analysis** to understand processing costs\n",
        "3. **Use embeddings** to find similar conversation segments  \n",
        "4. **Compare models** for your specific language/domain\n",
        "5. **Implement TextRank** for extractive summarization (research the algorithm!)\n",
        "\n",
        "*This makes great homework or project work to deepen your understanding!*\n",
        "\n",
        "### Your Toolkit for Future Projects\n",
        "\n",
        "```python\n",
        "# Core functions you can reuse:\n",
        "analyze_tokenization(text, model_name)    # Compare tokenization efficiency\n",
        "embedder.encode(sentences)                # Create semantic embeddings\n",
        "cosine_similarity(embeddings)            # Measure text similarity\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
