{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Install the packages we need\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Simple function to install required packages\"\"\"\n",
    "    packages = [\n",
    "        \"transformers\",  # For loading multilingual models\n",
    "        \"torch\",         # PyTorch (the engine behind most models)\n",
    "        \"matplotlib\",    # For creating charts\n",
    "        \"pandas\",        # For organizing data in tables\n",
    "        \"numpy\"          # For number crunching\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ”„ Installing packages...\")\n",
    "    for pkg in packages:\n",
    "        print(f\"  Installing {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "    \n",
    "    print(\"âœ… All packages installed!\")\n",
    "\n",
    "# Run the installation\n",
    "try:\n",
    "    install_packages()\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ Installation had issues, but you can probably continue.\")\n",
    "    print(\"If you already have these packages, you're good to go!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our results to a pandas table (easier to work with)\n",
    "df = pd.DataFrame(results)\n",
    "print(\"ğŸ“‹ Results Summary:\")\n",
    "print(df)\n",
    "\n",
    "# Create a simple bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df['Language'], df['Chars_per_Token'], color=['skyblue', 'lightgreen', 'orange', 'pink', 'lightcoral'])\n",
    "plt.title('ğŸ“Š Characters per Token by Language\\n(Higher = More efficient tokenization)', fontsize=14)\n",
    "plt.xlabel('Language', fontsize=12)\n",
    "plt.ylabel('Characters per Token', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, v in enumerate(df['Chars_per_Token']):\n",
    "    plt.text(i, v + 0.1, f'{v:.1f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ” What do you notice?\")\n",
    "print(\"ğŸ’¡ Languages with higher bars are tokenized more efficiently!\")\n",
    "print(\"ğŸ’¡ Languages with lower bars get 'chopped up' more by the model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: ğŸ¤” Let's think about what we learned\n",
    "\n",
    "**What did our experiment show us?**\n",
    "\n",
    "Take a look at your chart above and think about these questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¤” Discussion Questions - Think about these!\n",
    "\n",
    "print(\"ğŸ” REFLECTION QUESTIONS\")\n",
    "print(\"=\" * 40)\n",
    "print()\n",
    "print(\"1. ğŸ“Š Which language had the HIGHEST characters per token?\")\n",
    "print(\"   ğŸ’­ What does this mean for that language?\")\n",
    "print()\n",
    "print(\"2. ğŸ“Š Which language had the LOWEST characters per token?\") \n",
    "print(\"   ğŸ’­ What challenges might this create?\")\n",
    "print()\n",
    "print(\"3. ğŸŒ If you were building a chatbot, which language might be:\")\n",
    "print(\"   â€¢ Easiest for the model to understand?\")\n",
    "print(\"   â€¢ Most challenging for the model?\")\n",
    "print()\n",
    "print(\"4. ğŸ’¡ Why do you think some languages get 'chopped up' more than others?\")\n",
    "print(\"   Hint: Think about the alphabet, word structure, etc.\")\n",
    "print()\n",
    "\n",
    "# Let's find the best and worst performing languages\n",
    "best_lang = df.loc[df['Chars_per_Token'].idxmax()]\n",
    "worst_lang = df.loc[df['Chars_per_Token'].idxmin()]\n",
    "\n",
    "print(\"ğŸ“ˆ QUICK ANALYSIS:\")\n",
    "print(f\"ğŸ† Most efficient tokenization: {best_lang['Language']} ({best_lang['Chars_per_Token']:.1f} chars/token)\")\n",
    "print(f\"âš ï¸  Least efficient tokenization: {worst_lang['Language']} ({worst_lang['Chars_per_Token']:.1f} chars/token)\")\n",
    "print()\n",
    "print(\"ğŸ’¡ This means the model might understand\", best_lang['Language'], \"better than\", worst_lang['Language'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ Your Observation Notes\n",
    "# Feel free to edit this cell and add your thoughts!\n",
    "\n",
    "observations = {\n",
    "    \"Most efficient language\": \"Fill this in based on your chart\",\n",
    "    \"Least efficient language\": \"Fill this in based on your chart\", \n",
    "    \"Surprising finding\": \"What surprised you about the results?\",\n",
    "    \"Implications for dialogue summarization\": \"How might this affect dialogue summarization?\",\n",
    "    \"Questions for further investigation\": \"What would you like to explore next?\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ MY OBSERVATIONS FROM SESSION 0\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in observations.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n",
    "print(\"\\nğŸ’¡ Remember these insights for Session 1!\")\n",
    "print(\"They'll help you make better decisions about:\")\n",
    "print(\"â€¢ Which models to use\")\n",
    "print(\"â€¢ How to preprocess your text\") \n",
    "print(\"â€¢ What challenges to expect\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Congratulations! You've completed Session 0!\n",
    "\n",
    "**What you accomplished:**\n",
    "âœ… Set up your first multilingual model  \n",
    "âœ… Learned about tokenization  \n",
    "âœ… Compared how different languages are handled  \n",
    "âœ… Created your first language comparison chart  \n",
    "âœ… Documented your observations  \n",
    "\n",
    "**Key takeaways:**\n",
    "- Different languages are tokenized very differently\n",
    "- Some languages work better with current models than others\n",
    "- This affects how well models understand different languages\n",
    "- These insights will help you in dialogue summarization\n",
    "\n",
    "**ğŸš€ You're now ready for Session 1: Dialogue Summarization!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Quick Summary of Your Results\n",
    "\n",
    "print(\"ğŸ“Š SESSION 0 SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"ğŸ”¬ Languages tested: {len(test_sentences)}\")\n",
    "print(f\"ğŸ¤– Model used: mBERT (multilingual BERT)\")\n",
    "print(f\"ğŸ“ˆ Most efficient: {best_lang['Language']} ({best_lang['Chars_per_Token']:.1f} chars/token)\")\n",
    "print(f\"ğŸ“‰ Least efficient: {worst_lang['Language']} ({worst_lang['Chars_per_Token']:.1f} chars/token)\")\n",
    "print()\n",
    "print(\"ğŸš€ Ready for Session 1: Dialogue Summarization!\")\n",
    "print(\"ğŸ’¡ Use these insights to make better model choices!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
