{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "467d9f0f",
   "metadata": {},
   "source": [
    "# Session 2: Pretrained Models and Prompt Engineering ü§ñ\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**üìö Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/2_pretrained_models_prompt_engineering.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
    "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to **LLM-based approaches** for dialogue summarization! This session focuses on prompt engineering strategies that work across languages, with special attention to low-resource language challenges.\n",
    "\n",
    "**üéØ Focus:** Prompt engineering, few-shot learning, Chain-of-Thought  \n",
    "**üíª Requirements:** GPU recommended for large models (T5, mT5)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**üìã Recommended learning path:**\n",
    "1. **Session 0:** Setup and tokenization basics ‚úÖ  \n",
    "2. **Session 1:** Baseline summarization techniques ‚úÖ\n",
    "3. **This session (Session 2):** LLM prompt engineering ‚Üê You are here!\n",
    "\n",
    "## What You Will Learn\n",
    "\n",
    "1. **üèóÔ∏è Pretrained model families** and their strengths/weaknesses\n",
    "2. **üé® Prompt design vs. prompt engineering** principles\n",
    "3. **üéØ Zero-shot, one-shot, and few-shot** prompting strategies\n",
    "4. **üß† Chain-of-Thought prompting** for complex reasoning\n",
    "5. **üåç Cross-lingual prompt transfer** techniques\n",
    "6. **üìä Evaluation and cultural appropriateness** assessment\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will:\n",
    "- ‚úÖ Compare different families of pretrained models\n",
    "- ‚úÖ Design effective prompts for classification and QA\n",
    "- ‚úÖ Apply few-shot learning with multilingual examples\n",
    "- ‚úÖ Use Chain-of-Thought prompting across languages\n",
    "- ‚úÖ Evaluate outputs for correctness, fluency, and cultural fit\n",
    "- ‚úÖ Adapt prompting strategies to your target language\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "- **Cells marked üîç Checkpoint** are recommended stopping points\n",
    "- **Cells marked üéØ Challenge** are hands-on exercises  \n",
    "- **Cells marked üí¨ Discussion** are for group activities\n",
    "- **Run cells in order** - some require model loading time\n",
    "- **If models are slow:** Use smaller variants or CPU-only mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db6028",
   "metadata": {},
   "source": [
    "## 0. Setup and Model Loading\n",
    "\n",
    "We'll use multilingual T5 models for this session. These models are instruction-tuned and work well across languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efff16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Setup for Session 2: Prompt Engineering\n",
    "# Install additional packages needed for LLMs\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_packages(packages):\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "            print(f\"‚úÖ {package}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {package}: {str(e)[:50]}...\")\n",
    "\n",
    "print(\"üöÄ Installing LLM packages...\")\n",
    "packages = [\n",
    "    \"transformers>=4.35.0\",\n",
    "    \"torch>=1.13.0\", \n",
    "    \"sentencepiece\",\n",
    "    \"accelerate\",\n",
    "    \"datasets\"\n",
    "]\n",
    "\n",
    "install_packages(packages)\n",
    "\n",
    "# Essential imports\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"üéØ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ñ GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac65079",
   "metadata": {},
   "source": [
    "## 1. Pretrained Model Families Overview üèóÔ∏è\n",
    "\n",
    "**Understanding your options for multilingual text generation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4800a6d2",
   "metadata": {},
   "source": [
    "## 2. üéØ Hands-on Prompt Engineering Workshop\n",
    "\n",
    "**Your Mission:** Design and test prompts for classification and question answering in English and your target low-resource language.\n",
    "\n",
    "### Task Overview:\n",
    "1. **Classification:** Categorize dialogues into topics (meeting, social, support, transaction, other)\n",
    "2. **Question Answering:** Extract information from context using Chain-of-Thought reasoning\n",
    "3. **Evaluation:** Rate outputs for correctness, fluency, and cultural appropriateness\n",
    "\n",
    "### üèóÔ∏è Model Family Comparison\n",
    "\n",
    "**Available approaches for multilingual tasks:**\n",
    "\n",
    "| **Approach** | **Pros** | **Cons** | **Best For** |\n",
    "|--------------|----------|----------|--------------|\n",
    "| **Local LLM (mT5)** | Privacy, customizable, offline | Requires compute resources | Controlled environments |\n",
    "| **API (GPT-3.5/4)** | State-of-art, no setup | Cost per token, internet needed | Production applications |\n",
    "| **Hosted (Colab)** | Free experimentation | Limited resources | Learning and prototyping |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b01478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ Load multilingual model and create prompt engineering toolkit\n",
    "\n",
    "print(\"üì• Loading multilingual T5 model...\")\n",
    "print(\"‚è±Ô∏è  This may take 2-3 minutes on first run\")\n",
    "\n",
    "# Use smaller model for workshop - upgrade to base for better quality\n",
    "MODEL_NAME = \"google/mt5-small\"  \n",
    "# MODEL_NAME = \"google/mt5-base\"  # Better quality, requires more resources\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded on {device}\")\n",
    "    print(f\"üìä Parameters: ~{sum(p.numel() for p in model.parameters()) / 1e6:.0f}M\")\n",
    "    \n",
    "    def generate_text(prompt: str, max_length: int = 100, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Generate text using our multilingual model\"\"\"\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs, max_length=max_length, temperature=temperature,\n",
    "                do_sample=True, pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Test the model\n",
    "    test_output = generate_text(\"Classify this dialogue: A: Let's meet at 3pm. B: Perfect!\", max_length=50)\n",
    "    print(f\"\\nüß™ Test: {test_output}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")\n",
    "    print(\"üí° Try restarting runtime or using CPU-only mode\")\n",
    "    model, tokenizer = None, None\n",
    "\n",
    "# üìä Multilingual test data for classification and QA\n",
    "test_data = {\n",
    "    \"English\": {\n",
    "        \"classification\": [\n",
    "            {\"dialogue\": \"A: Can we schedule the meeting for 3pm? B: Yes, I'll send the invite.\", \"topic\": \"meeting\"},\n",
    "            {\"dialogue\": \"A: How was your weekend? B: Great! Went hiking with friends.\", \"topic\": \"social\"},\n",
    "            {\"dialogue\": \"A: My laptop won't start. B: Try holding the power button for 10 seconds.\", \"topic\": \"support\"},\n",
    "        ],\n",
    "        \"qa\": {\n",
    "            \"context\": \"Alice and Bob plan a meeting. Alice suggests 3pm but Bob is busy until 4pm. They agree to meet at 4:30pm in the conference room.\",\n",
    "            \"questions\": [\"What time did they agree to meet?\", \"Where will they meet?\"],\n",
    "            \"answers\": [\"4:30pm\", \"conference room\"]\n",
    "        }\n",
    "    },\n",
    "    \"French\": {\n",
    "        \"classification\": [\n",
    "            {\"dialogue\": \"A: Pouvons-nous programmer la r√©union pour 15h? B: Oui, j'enverrai l'invitation.\", \"topic\": \"meeting\"},\n",
    "            {\"dialogue\": \"A: Comment s'est pass√© ton week-end? B: Super! J'ai fait de la randonn√©e.\", \"topic\": \"social\"},\n",
    "        ],\n",
    "        \"qa\": {\n",
    "            \"context\": \"Marie et Jean planifient une r√©union. Marie propose 15h mais Jean est occup√© jusqu'√† 16h. Ils conviennent de se rencontrer √† 16h30 en salle de conf√©rence.\",\n",
    "            \"questions\": [\"√Ä quelle heure ont-ils convenu de se rencontrer?\"],\n",
    "            \"answers\": [\"16h30\"]\n",
    "        }\n",
    "    },\n",
    "    # üåç ADD YOUR LANGUAGE HERE:\n",
    "    # \"YourLanguage\": {\n",
    "    #     \"classification\": [\n",
    "    #         {\"dialogue\": \"Your dialogue\", \"topic\": \"meeting\"}\n",
    "    #     ],\n",
    "    #     \"qa\": {\n",
    "    #         \"context\": \"Your context\",\n",
    "    #         \"questions\": [\"Your question?\"],\n",
    "    #         \"answers\": [\"Your answer\"]\n",
    "    #     }\n",
    "    # }\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã Test data loaded for {len(test_data)} languages\")\n",
    "for lang in test_data:\n",
    "    print(f\"  {lang}: {len(test_data[lang]['classification'])} classification + {len(test_data[lang]['qa']['questions'])} QA examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291438cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1 üéØ Zero-shot, Few-shot, and Chain-of-Thought Comparison\n",
    "\n",
    "# üîß Prompt engineering toolkit\n",
    "def create_zero_shot_prompt(dialogue: str, task: str = \"classification\") -> str:\n",
    "    \"\"\"Zero-shot prompt - no examples provided\"\"\"\n",
    "    if task == \"classification\":\n",
    "        return f\"\"\"Classify this dialogue into one topic: meeting, social, support, transaction, other.\n",
    "\n",
    "Dialogue: {dialogue}\n",
    "\n",
    "Topic:\"\"\"\n",
    "    else:  # QA\n",
    "        return f\"\"\"Answer the question based on the context.\n",
    "\n",
    "Context: {dialogue}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "def create_few_shot_prompt(dialogue: str, examples: list, task: str = \"classification\") -> str:\n",
    "    \"\"\"Few-shot prompt - includes examples\"\"\"\n",
    "    if task == \"classification\":\n",
    "        prompt = \"Classify dialogues into topics: meeting, social, support, transaction, other.\\n\\nExamples:\\n\\n\"\n",
    "        for ex in examples[:2]:  # Use 2 examples to avoid length issues\n",
    "            prompt += f\"Dialogue: {ex['dialogue']}\\nTopic: {ex['topic']}\\n\\n\"\n",
    "        prompt += f\"Dialogue: {dialogue}\\nTopic:\"\n",
    "        return prompt\n",
    "    else:  # QA\n",
    "        return f\"\"\"Answer questions based on context.\n",
    "\n",
    "Context: {dialogue}\n",
    "\n",
    "Answer with specific information:\"\"\"\n",
    "\n",
    "def create_chain_of_thought_prompt(context: str, question: str) -> str:\n",
    "    \"\"\"Chain-of-Thought prompt for step-by-step reasoning\"\"\"\n",
    "    return f\"\"\"Answer the question step by step based on the context.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Let me think step by step:\n",
    "1. What is the question asking?\n",
    "2. What relevant information is in the context?\n",
    "3. What is the answer?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# üß™ Run comprehensive prompt testing\n",
    "def test_all_prompting_strategies():\n",
    "    \"\"\"Test zero-shot, few-shot, and Chain-of-Thought across languages\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"üéØ COMPREHENSIVE PROMPT ENGINEERING TEST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for language, data in test_data.items():\n",
    "        print(f\"\\nüåç TESTING: {language.upper()}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Test classification\n",
    "        if data[\"classification\"]:\n",
    "            test_dialogue = data[\"classification\"][0][\"dialogue\"]\n",
    "            true_topic = data[\"classification\"][0][\"topic\"]\n",
    "            \n",
    "            print(f\"üìù Classification task: {test_dialogue[:60]}...\")\n",
    "            print(f\"üìã Expected: {true_topic}\")\n",
    "            \n",
    "            # Zero-shot classification\n",
    "            zero_prompt = create_zero_shot_prompt(test_dialogue, \"classification\")\n",
    "            if model:\n",
    "                zero_result = generate_text(zero_prompt, max_length=50, temperature=0.1)\n",
    "                print(f\"üéØ Zero-shot: {zero_result}\")\n",
    "                \n",
    "                # Few-shot classification (using English examples for transfer)\n",
    "                few_prompt = create_few_shot_prompt(test_dialogue, test_data[\"English\"][\"classification\"], \"classification\")\n",
    "                few_result = generate_text(few_prompt, max_length=50, temperature=0.1)\n",
    "                print(f\"üìö Few-shot: {few_result}\")\n",
    "                \n",
    "                results.append({\n",
    "                    \"language\": language, \"task\": \"classification\", \"method\": \"zero-shot\",\n",
    "                    \"input\": test_dialogue[:50] + \"...\", \"output\": zero_result, \"expected\": true_topic\n",
    "                })\n",
    "                results.append({\n",
    "                    \"language\": language, \"task\": \"classification\", \"method\": \"few-shot\", \n",
    "                    \"input\": test_dialogue[:50] + \"...\", \"output\": few_result, \"expected\": true_topic\n",
    "                })\n",
    "            \n",
    "        # Test QA with Chain-of-Thought\n",
    "        if data[\"qa\"][\"questions\"]:\n",
    "            context = data[\"qa\"][\"context\"]\n",
    "            question = data[\"qa\"][\"questions\"][0]\n",
    "            expected_answer = data[\"qa\"][\"answers\"][0]\n",
    "            \n",
    "            print(f\"\\\\n‚ùì QA task: {question}\")\n",
    "            print(f\"üìã Expected: {expected_answer}\")\n",
    "            \n",
    "            # Chain-of-Thought QA\n",
    "            cot_prompt = create_chain_of_thought_prompt(context, question)\n",
    "            if model:\n",
    "                cot_result = generate_text(cot_prompt, max_length=120, temperature=0.2)\n",
    "                print(f\"üß† Chain-of-Thought: {cot_result}\")\n",
    "                \n",
    "                results.append({\n",
    "                    \"language\": language, \"task\": \"qa\", \"method\": \"chain-of-thought\",\n",
    "                    \"input\": question, \"output\": cot_result, \"expected\": expected_answer\n",
    "                })\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the comprehensive test\n",
    "if model:\n",
    "    test_results = test_all_prompting_strategies()\n",
    "    print(f\"‚úÖ Completed testing across {len(test_data)} languages\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Model not available - showing prompt structure only\")\n",
    "    # Show example prompts\n",
    "    example_dialogue = \"A: Can we meet at 3pm? B: Perfect!\"\n",
    "    print(\"\\\\nüìù EXAMPLE PROMPTS:\")\n",
    "    print(\"\\\\nüéØ Zero-shot:\")\n",
    "    print(create_zero_shot_prompt(example_dialogue))\n",
    "    print(\"\\\\nüìö Few-shot structure:\")\n",
    "    print(create_few_shot_prompt(example_dialogue, [{\"dialogue\": \"Example\", \"topic\": \"meeting\"}])[:200] + \"...\")\n",
    "\n",
    "### 2.2 üìä Evaluation Framework\n",
    "\n",
    "def create_evaluation_rubric():\n",
    "    \"\"\"Evaluation framework for model outputs\"\"\"\n",
    "    return {\n",
    "        \"correctness\": {\n",
    "            \"1\": \"Completely wrong\", \"2\": \"Partially wrong\", \"3\": \"Mostly right\", \n",
    "            \"4\": \"Right answer\", \"5\": \"Perfect with reasoning\"\n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"1\": \"Unnatural/errors\", \"2\": \"Awkward phrasing\", \"3\": \"Acceptable\", \n",
    "            \"4\": \"Good language\", \"5\": \"Native-like\"\n",
    "        },\n",
    "        \"cultural_appropriateness\": {\n",
    "            \"1\": \"Inappropriate\", \"2\": \"Questionable\", \"3\": \"Neutral\", \n",
    "            \"4\": \"Appropriate\", \"5\": \"Culturally aware\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "def evaluate_output(output: str, expected: str, language: str, task: str, method: str):\n",
    "    \"\"\"Template for manual evaluation\"\"\"\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"expected\": expected,\n",
    "        \"language\": language,\n",
    "        \"task\": task,\n",
    "        \"method\": method,\n",
    "        \"correctness_score\": 0,  # Fill in 1-5\n",
    "        \"fluency_score\": 0,      # Fill in 1-5\n",
    "        \"cultural_score\": 0,     # Fill in 1-5\n",
    "        \"notes\": \"\",            # Your observations\n",
    "        \"improvement_suggestions\": \"\"\n",
    "    }\n",
    "\n",
    "print(\"\\\\nüìã EVALUATION FRAMEWORK\")\n",
    "print(\"=\"*40)\n",
    "rubric = create_evaluation_rubric()\n",
    "for dimension, scale in rubric.items():\n",
    "    print(f\"\\\\n{dimension.upper()}:\")\n",
    "    for score, description in scale.items():\n",
    "        print(f\"  {score}: {description}\")\n",
    "\n",
    "print(f\"\\\\nüéØ YOUR TURN: Evaluate the outputs above using this 1-5 scale\")\n",
    "print(\"üí° Focus on how well each method works for your target language\")\n",
    "\n",
    "### 2.3 üí¨ Discussion Questions and Key Takeaways\n",
    "\n",
    "discussion_guide = \"\"\"\n",
    "ü§î REFLECTION QUESTIONS:\n",
    "\n",
    "1. **Cross-language Performance:**\n",
    "   - Which prompting method worked best for your target language?\n",
    "   - How did performance differ between English and your language?\n",
    "\n",
    "2. **Method Comparison:** \n",
    "   - When did few-shot examples help vs. hurt?\n",
    "   - How effective was Chain-of-Thought reasoning in non-English?\n",
    "\n",
    "3. **Cultural Considerations:**\n",
    "   - What cultural assumptions did you notice in outputs?\n",
    "   - How would you adapt prompts for your cultural context?\n",
    "\n",
    "4. **Practical Applications:**\n",
    "   - Which approach would you use in production?\n",
    "   - What are the trade-offs between methods?\n",
    "\n",
    "üìù ACTION ITEMS:\n",
    "‚ñ° Document 3 key insights about your target language\n",
    "‚ñ° Identify best prompting strategies for your use case  \n",
    "‚ñ° Note major challenges needing further research\n",
    "‚ñ° Plan next steps for your project\n",
    "\n",
    "üéØ KEY TAKEAWAYS:\n",
    "‚Ä¢ Prompt structure matters more than complexity\n",
    "‚Ä¢ Cultural context significantly impacts performance  \n",
    "‚Ä¢ Few-shot examples can bridge language gaps effectively\n",
    "‚Ä¢ Chain-of-Thought helps with reasoning across languages\n",
    "‚Ä¢ Evaluation must consider cultural appropriateness\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\\\n\" + discussion_guide)\n",
    "\n",
    "print(\"\\\\nüéâ CONGRATULATIONS!\")\n",
    "print(\"You've completed hands-on prompt engineering for low-resource languages!\")\n",
    "print(\"Use these techniques responsibly and keep experimenting! üöÄ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
