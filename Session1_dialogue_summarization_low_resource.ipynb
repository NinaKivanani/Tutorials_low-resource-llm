{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48c569d",
   "metadata": {},
   "source": [
    "# Session 1: Dialogue Summarization - Low-Resource Techniques üìù\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**üìö Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/Session1_dialogue_summarization_low_resource.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
    "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "This notebook focuses on **non-LLM approaches** for dialogue summarization. You'll learn robust baseline techniques that work even when you have little labeled data, limited tools, and challenging text quality - perfect for low-resource languages.\n",
    "\n",
    "**üéØ Focus:** TextRank and extractive methods (no LLMs required)  \n",
    "**üíª Requirements:** CPU is sufficient - no GPU needed!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**üìã Recommended learning path:**\n",
    "1. **Session 0:** Setup and tokenization basics  \n",
    "2. **This session (Session 1):** Baseline summarization techniques  \n",
    "3. **Session 2:** Prompt engineering with LLMs ‚Üê For LLM-based approaches!\n",
    "\n",
    "## What You Will Build\n",
    "\n",
    "1. **üìä A clean dialogue dataset** from raw text parsing\n",
    "2. **üéØ A robust TextRank baseline** (extractive summarization)\n",
    "3. **üìè An evaluation harness** (ROUGE metrics, sanity checks)\n",
    "4. **üåç Low-resource adaptation strategies** (noise handling, normalization)\n",
    "5. **üîß A reusable toolkit** for any low-resource language\n",
    "\n",
    "**üí° Note:** For LLM-based summarization with prompting, see Session 2!\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will:\n",
    "- ‚úÖ Parse and clean dialogue data from raw text\n",
    "- ‚úÖ Implement TextRank extractive summarization  \n",
    "- ‚úÖ Evaluate summaries with ROUGE metrics\n",
    "- ‚úÖ Simulate and handle low-resource conditions\n",
    "- ‚úÖ Apply robust preprocessing for noisy text\n",
    "- ‚úÖ Adapt methods to your target language\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "- **Cells marked üîç Checkpoint** are recommended stopping points\n",
    "- **Cells marked üéØ Challenge** are optional exercises  \n",
    "- **Run cells in order** - most steps are deterministic with fixed seeds\n",
    "- **If stuck:** restart runtime and re-run from the top\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e85f7",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "### Option A. Minimal pip setup (recommended for workshops)\n",
    "\n",
    "Run the next cell. It installs only what this notebook uses.\n",
    "\n",
    "### Option B. Conda environment (Python 3.9)\n",
    "\n",
    "If you prefer an isolated environment.\n",
    "\n",
    "```bash\n",
    "conda create -n dialogue-sum python=3.9 -y\n",
    "conda activate dialogue-sum\n",
    "pip install -U pip\n",
    "pip install \"numpy<2\" pandas scikit-learn networkx matplotlib tqdm\n",
    "pip install \"transformers==4.49.0\" \"datasets==3.2.0\" \"accelerate>=0.25.0\" sentencepiece\n",
    "pip install rouge-score evaluate\n",
    "pip install ipywidgets\n",
    "```\n",
    "\n",
    "If you are on a managed cluster, you may need to load a CUDA module before installing PyTorch. Use the PyTorch install command recommended for your platform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24fd6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal dependencies for this notebook.\n",
    "# If you already have these installed, you can skip this cell.\n",
    "\n",
    "# üì¶ Colab-Optimized Installation for Session 1\n",
    "# This cell fixes common dependency conflicts in Google Colab\n",
    "\n",
    "import sys, subprocess\n",
    "\n",
    "def pip_install_colab_safe(pkgs):\n",
    "    \"\"\"Install packages with Colab-safe dependency handling\"\"\"\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\"] + pkgs\n",
    "    print(\"Installing:\", \" \".join(pkgs))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ INSTALLING DEPENDENCIES FOR SESSION 1\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚è±Ô∏è  This will take 2-3minutes...\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Step 1: Fix numpy/pandas compatibility (common Colab issue)\n",
    "    print(\"üîß Step 1/3: Fixing numpy/pandas compatibility...\")\n",
    "    pip_install_colab_safe([\"numpy==1.24.3\", \"pandas==2.0.3\"])\n",
    "    \n",
    "    # Step 2: Core data science packages\n",
    "    print(\"üìä Step 2/3: Installing core packages...\")\n",
    "    pip_install_colab_safe([\n",
    "        \"scikit-learn\",\n",
    "        \"networkx\", \n",
    "        \"matplotlib\",\n",
    "        \"tqdm\",\n",
    "        \"rouge-score\"  # For ROUGE evaluation metrics\n",
    "    ])\n",
    "    \n",
    "    # Step 3: NLP packages (with compatible versions)\n",
    "    print(\"ü§ó Step 3/3: Installing NLP packages...\")\n",
    "    pip_install_colab_safe([\n",
    "        \"transformers==4.35.0\",  # Compatible with numpy 1.24\n",
    "        \"datasets==2.14.0\",      # Compatible with pandas 2.0\n",
    "        \"accelerate==0.23.0\",\n",
    "        \"sentencepiece\",\n",
    "        \"sentence-transformers==2.2.2\"  # For embeddings\n",
    "    ])\n",
    "    \n",
    "    # Optional: widgets for interactive elements\n",
    "    try:\n",
    "        import ipywidgets\n",
    "    except ImportError:\n",
    "        pip_install_colab_safe([\"ipywidgets\"])\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚úÖ INSTALLATION COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"üéâ All packages installed successfully!\")\n",
    "    print(\"üí° If you see warnings above, that's normal in Colab\")\n",
    "    \n",
    "    # Verify key packages are available\n",
    "    print(\"\\nüîç Verifying package installation...\")\n",
    "    import importlib\n",
    "    key_packages = [\"rouge_score\", \"networkx\", \"sklearn\", \"transformers\", \"sentence_transformers\"]\n",
    "    \n",
    "    for pkg in key_packages:\n",
    "        try:\n",
    "            importlib.import_module(pkg)\n",
    "            print(f\"‚úÖ {pkg}\")\n",
    "        except ImportError:\n",
    "            print(f\"‚ùå {pkg} - MISSING!\")\n",
    "            print(f\"   Try running: !pip install {pkg.replace('_', '-')}\")\n",
    "    \n",
    "    print(\"\\nüéØ Ready to proceed with the tutorial!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print()\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚ö†Ô∏è  INSTALLATION ISSUE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Error:\", str(e))\n",
    "    print()\n",
    "    print(\"üîÑ SOLUTION: Try this:\")\n",
    "    print(\"1. Runtime ‚Üí Restart Runtime\")\n",
    "    print(\"2. Re-run this cell\")\n",
    "    print(\"3. If still stuck, notify instructor\")\n",
    "    print()\n",
    "    print(\"üí° You may be able to continue if packages were already installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üö® TROUBLESHOOTING: If you get \"ModuleNotFoundError: No module named 'rouge_score'\"\n",
    "# Run this cell to fix the issue:\n",
    "\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "    print(\"‚úÖ rouge_score is working correctly!\")\n",
    "except ImportError:\n",
    "    print(\"üîß Installing rouge-score...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rouge-score\"])\n",
    "    print(\"‚úÖ rouge-score installed successfully!\")\n",
    "    \n",
    "    # Test the import again\n",
    "    try:\n",
    "        from rouge_score import rouge_scorer\n",
    "        print(\"‚úÖ rouge_score import now working!\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Still having issues: {e}\")\n",
    "        print(\"üí° Try: Runtime ‚Üí Restart Runtime, then re-run all cells from the top\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63f16f",
   "metadata": {},
   "source": [
    "### üö® Common Issue: ModuleNotFoundError for rouge_score\n",
    "\n",
    "**If you see: `ModuleNotFoundError: No module named 'rouge_score'`**\n",
    "\n",
    "**Solution 1 (Recommended):** Run the troubleshooting cell above ‚òùÔ∏è\n",
    "\n",
    "**Solution 2 (Manual fix):**\n",
    "1. Run this command in a code cell: `!pip install rouge-score`\n",
    "2. Go to **Runtime ‚Üí Restart Runtime**\n",
    "3. Re-run all cells from the beginning\n",
    "\n",
    "**Solution 3 (If nothing else works):**\n",
    "1. **Runtime ‚Üí Restart Runtime** \n",
    "2. **Runtime ‚Üí Run All** (this will re-install everything)\n",
    "\n",
    "**Why this happens:** Sometimes package installation fails silently in Colab, or packages get overwritten by conflicting dependencies.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9611d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For tokenization and embedding analysis\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "random.seed(842)\n",
    "np.random.seed(842)\n",
    "\n",
    "print(\"‚úÖ All imports ready!\")\n",
    "\n",
    "# üö® PLACEHOLDER: LLM functions moved to Session 2\n",
    "# This prevents NameError when running leftover LLM code sections\n",
    "def generate_summary_t5(*args, **kwargs):\n",
    "    \"\"\"Placeholder function - LLM functionality moved to Session 2\"\"\"\n",
    "    return \"[This functionality moved to Session 2: Pretrained Models and Prompt Engineering]\"\n",
    "\n",
    "# Define placeholder variables for compatibility\n",
    "tokenizer = None\n",
    "model = None\n",
    "ZERO_SHOT_PROMPT = \"See Session 2 for prompting examples\"\n",
    "ONE_SHOT_PROMPT = \"See Session 2 for prompting examples\"\n",
    "\n",
    "print(\"üí° Note: LLM functions are placeholders. For actual LLM functionality, see Session 2!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68d295",
   "metadata": {},
   "source": [
    "## 0.5 Tokenization and Embedding Analysis üîç\n",
    "\n",
    "Before we dive into dialogue summarization, let's understand how multilingual models handle different languages. This is crucial for low-resource language work.\n",
    "\n",
    "**üéØ What you learn:**\n",
    "- How different models tokenize the same text\n",
    "- Impact of tokenization on model performance\n",
    "- Sentence embedding quality across languages\n",
    "- How to visualize language representation gaps\n",
    "\n",
    "**üí° Why this matters:** Poor tokenization can severely hurt performance in low-resource languages. Understanding this helps you choose better models and preprocessing strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ef581",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 0.5.1 Multilingual Tokenization Comparison\n",
    "\n",
    "# Sample sentences in different languages - add your target language here!\n",
    "test_sentences = {\n",
    "    \"English\": \"Hello, how are you doing today?\",\n",
    "    \"French\": \"Bonjour, comment allez-vous aujourd'hui?\", \n",
    "    \"German\": \"Hallo, wie geht es dir heute?\",\n",
    "    \"Spanish\": \"Hola, ¬øc√≥mo est√°s hoy?\",\n",
    "    \"Arabic\": \"ŸÖÿ±ÿ≠ÿ®ÿßÿå ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉ ÿßŸÑŸäŸàŸÖÿü\",\n",
    "    \"Chinese\": \"‰Ω†Â•ΩÔºå‰Ω†‰ªäÂ§©ÊÄé‰πàÊ†∑Ôºü\",\n",
    "    \"Hindi\": \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Ü‡§ú ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç?\",\n",
    "    # Add your low-resource language here:\n",
    "    # \"YourLanguage\": \"Your sentence here\"\n",
    "}\n",
    "\n",
    "# Models to compare - from monolingual to multilingual\n",
    "models_to_test = [\n",
    "    \"bert-base-uncased\",           # English-only\n",
    "    \"distilbert-base-multilingual-cased\",  # Lightweight multilingual\n",
    "    \"xlm-roberta-base\",            # Strong multilingual\n",
    "    \"microsoft/mdeberta-v3-base\"   # Latest multilingual\n",
    "]\n",
    "\n",
    "def analyze_tokenization(sentence: str, model_name: str):\n",
    "    \"\"\"Analyze how a model tokenizes a sentence\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    token_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    \n",
    "    return {\n",
    "        'model': model_name.split('/')[-1],  # Short name\n",
    "        'sentence': sentence,\n",
    "        'num_tokens': len(tokens),\n",
    "        'tokens': tokens[:10],  # First 10 tokens for display\n",
    "        'subword_ratio': len(tokens) / len(sentence.split()),\n",
    "        'has_unk': '[UNK]' in tokens or '<unk>' in tokens\n",
    "    }\n",
    "\n",
    "print(\"üîç TOKENIZATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test each language with each model\n",
    "results = []\n",
    "for lang, sentence in test_sentences.items():\n",
    "    print(f\"\\nüìù Language: {lang}\")\n",
    "    print(f\"   Text: {sentence}\")\n",
    "    print()\n",
    "    \n",
    "    for model_name in models_to_test[:2]:  # Test first 2 models to save time\n",
    "        try:\n",
    "            result = analyze_tokenization(sentence, model_name)\n",
    "            results.append({**result, 'language': lang})\n",
    "            \n",
    "            print(f\"   ü§ñ {result['model']}\")\n",
    "            print(f\"      Tokens: {result['num_tokens']} | Subword ratio: {result['subword_ratio']:.2f}\")\n",
    "            print(f\"      Sample: {' '.join(result['tokens'][:5])}...\")\n",
    "            if result['has_unk']:\n",
    "                print(f\"      ‚ö†Ô∏è  Contains unknown tokens!\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {model_name}: Error - {str(e)[:50]}...\")\n",
    "        print()\n",
    "\n",
    "# Convert to DataFrame for easy analysis\n",
    "df_tokens = pd.DataFrame(results)\n",
    "if len(df_tokens) > 0:\n",
    "    print(\"üìä TOKENIZATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    summary = df_tokens.groupby(['language', 'model']).agg({\n",
    "        'num_tokens': 'mean',\n",
    "        'subword_ratio': 'mean', \n",
    "        'has_unk': 'any'\n",
    "    }).round(2)\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results to display\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 0.5.2 Multilingual Sentence Embeddings and Visualization\n",
    "\n",
    "# Create sentence embeddings for cross-lingual comparison\n",
    "print(\"üåç SENTENCE EMBEDDING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚è±Ô∏è  Loading multilingual sentence transformer...\")\n",
    "\n",
    "try:\n",
    "    # Use a lightweight multilingual sentence transformer\n",
    "    embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    \n",
    "    # Select a subset of languages for embedding analysis\n",
    "    embedding_sentences = {\n",
    "        \"English\": [\"Hello, how are you?\", \"I need help with my computer\", \"What time is it?\"],\n",
    "        \"French\": [\"Bonjour, comment allez-vous?\", \"J'ai besoin d'aide avec mon ordinateur\", \"Quelle heure est-il?\"],\n",
    "        \"German\": [\"Hallo, wie geht es dir?\", \"Ich brauche Hilfe mit meinem Computer\", \"Wie sp√§t ist es?\"],\n",
    "        \"Arabic\": [\"ŸÖÿ±ÿ≠ÿ®ÿßÿå ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉÿü\", \"ÿ£ÿ≠ÿ™ÿßÿ¨ ŸÖÿ≥ÿßÿπÿØÿ© ŸÖÿπ ÿßŸÑŸÉŸÖÿ®ŸäŸàÿ™ÿ±\", \"ŸÉŸÖ ÿßŸÑÿ≥ÿßÿπÿ©ÿü\"],\n",
    "        # Add your low-resource language here:\n",
    "        # \"YourLanguage\": [\"Greeting\", \"Help request\", \"Time question\"]\n",
    "    }\n",
    "    \n",
    "    # Generate embeddings\n",
    "    all_sentences = []\n",
    "    all_languages = []\n",
    "    all_labels = []\n",
    "    \n",
    "    labels = [\"greeting\", \"help_request\", \"time_question\"]\n",
    "    \n",
    "    print(\"\\nüîÑ Generating embeddings...\")\n",
    "    for lang, sentences in embedding_sentences.items():\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            all_sentences.append(sentence)\n",
    "            all_languages.append(lang)\n",
    "            all_labels.append(labels[i])\n",
    "    \n",
    "    # Get embeddings\n",
    "    embeddings = embedding_model.encode(all_sentences)\n",
    "    print(f\"‚úÖ Generated {len(embeddings)} embeddings of dimension {embeddings.shape[1]}\")\n",
    "    \n",
    "    # Simple similarity analysis\n",
    "    print(\"\\nüîç CROSS-LINGUAL SIMILARITY ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    # Calculate similarities between same semantic content across languages\n",
    "    for i, label in enumerate(labels):\n",
    "        print(f\"\\nüìù Analyzing: {label}\")\n",
    "        \n",
    "        # Get sentences with this label\n",
    "        indices = [j for j, l in enumerate(all_labels) if l == label]\n",
    "        label_sentences = [all_sentences[j] for j in indices]\n",
    "        label_languages = [all_languages[j] for j in indices]\n",
    "        label_embeddings = embeddings[indices]\n",
    "        \n",
    "        # Calculate pairwise similarities\n",
    "        similarities = cosine_similarity(label_embeddings)\n",
    "        \n",
    "        for idx1 in range(len(label_sentences)):\n",
    "            for idx2 in range(idx1 + 1, len(label_sentences)):\n",
    "                sim = similarities[idx1, idx2]\n",
    "                lang1, lang2 = label_languages[idx1], label_languages[idx2]\n",
    "                print(f\"   {lang1} ‚Üî {lang2}: {sim:.3f}\")\n",
    "    \n",
    "    # Visualization with dimensionality reduction\n",
    "    print(\"\\nüìä EMBEDDING VISUALIZATION\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    try:\n",
    "        from sklearn.decomposition import PCA\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Reduce to 2D for visualization\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        embeddings_2d = pca.fit_transform(embeddings)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Color by language\n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
    "        unique_langs = list(set(all_languages))\n",
    "        \n",
    "        for i, lang in enumerate(unique_langs):\n",
    "            lang_indices = [j for j, l in enumerate(all_languages) if l == lang]\n",
    "            lang_embeddings = embeddings_2d[lang_indices]\n",
    "            plt.scatter(lang_embeddings[:, 0], lang_embeddings[:, 1], \n",
    "                       c=colors[i % len(colors)], label=lang, alpha=0.7, s=100)\n",
    "            \n",
    "            # Add text labels\n",
    "            for j, idx in enumerate(lang_indices):\n",
    "                plt.annotate(all_labels[idx][:4], \n",
    "                           (lang_embeddings[j, 0], lang_embeddings[j, 1]),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        plt.title('Multilingual Sentence Embeddings (PCA)')\n",
    "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"üí° What to look for:\")\n",
    "        print(\"- Do same semantic concepts cluster together across languages?\")\n",
    "        print(\"- Are some languages closer/farther from others?\") \n",
    "        print(\"- Is your target language well-represented?\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Visualization error: {str(e)}\")\n",
    "        print(\"üí° This is normal in some environments - the analysis above is still valid!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading sentence transformer: {str(e)}\")\n",
    "    print(\"üí° This might happen in resource-constrained environments\")\n",
    "    print(\"üìù Key takeaway: Always test embedding quality for your target language!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1dd99",
   "metadata": {},
   "source": [
    "### 0.5.3 üéØ Your Turn: Analyze Your Target Language\n",
    "\n",
    "**Task:** Add your low-resource language to the analysis above and document your findings.\n",
    "\n",
    "```python\n",
    "# üìù ADD YOUR LANGUAGE HERE\n",
    "# \n",
    "# 1. Add your language to test_sentences (line ~10 above)\n",
    "# 2. Add your language to embedding_sentences (line ~85 above)  \n",
    "# 3. Re-run the cells above\n",
    "# 4. Fill out the analysis below:\n",
    "\n",
    "your_language_analysis = {\n",
    "    \"language_name\": \"\",  # e.g. \"Luxembourgish\", \"Yoruba\", \"Armenian\"\n",
    "    \n",
    "    # Tokenization findings (from section 0.5.1)\n",
    "    \"avg_tokens_per_word\": 0.0,  # Higher = more subword splitting\n",
    "    \"has_unknown_tokens\": False,  # True/False\n",
    "    \"best_tokenizer\": \"\",  # Which model handled your language best?\n",
    "    \n",
    "    # Embedding findings (from section 0.5.2)  \n",
    "    \"semantic_similarity_score\": 0.0,  # Average similarity to other languages\n",
    "    \"embedding_quality\": \"\",  # \"good\", \"fair\", \"poor\"\n",
    "    \"clustering_with\": \"\",  # Which language does yours cluster closest to?\n",
    "    \n",
    "    # Overall assessment\n",
    "    \"model_recommendation\": \"\",  # Which model would you choose for your language?\n",
    "    \"main_challenges\": \"\",  # What are the biggest issues you observed?\n",
    "    \"next_steps\": \"\"  # What would you do to improve performance?\n",
    "}\n",
    "\n",
    "print(\"üìã YOUR LANGUAGE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "for key, value in your_language_analysis.items():\n",
    "    if value:  # Only print filled fields\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"\\nüí° Discussion Points:\")\n",
    "print(\"- How does tokenization quality affect downstream tasks?\")\n",
    "print(\"- What strategies could improve representation for your language?\")  \n",
    "print(\"- When would you choose simpler baselines vs. multilingual models?\")\n",
    "```\n",
    "\n",
    "üîç **Checkpoint 0.5: Foundation Knowledge Complete!**\n",
    "\n",
    "**‚úÖ What you've learned:**\n",
    "- How to evaluate tokenization quality across languages\n",
    "- How to measure cross-lingual embedding similarity  \n",
    "- How to visualize language representation gaps\n",
    "- How to identify the best models for your target language\n",
    "\n",
    "**üéØ Next:** We'll use this knowledge to build robust dialogue summarization systems!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76613e6",
   "metadata": {},
   "source": [
    "## 1. Get a real dialogue dataset\n",
    "\n",
    "To keep this notebook self contained, we use a public domain English play. The text is not invented for this tutorial. It is an excerpt from *The Importance of Being Earnest* by Oscar Wilde (first published in 1895, public domain in many jurisdictions).\n",
    "\n",
    "We will parse it into speaker turns, then create short dialogue segments that resemble real conversations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_TEXT = '''\n",
    "[Enter Lane.]\n",
    "\n",
    "LANE. Why, Mr. Worthing, I suppose this is one of your pleasant\n",
    "surprises? I have been expecting you back some time ago.\n",
    "\n",
    "JACK. I have not been able to return sooner. I have been detained in\n",
    "town.\n",
    "\n",
    "LANE. I have received a message from Mr. Algernon. He says he will be\n",
    "down at four o'clock.\n",
    "\n",
    "JACK. Is Mr. Algernon here?\n",
    "\n",
    "LANE. Yes, sir. He is in the dining-room.\n",
    "\n",
    "JACK. I must see him at once.\n",
    "\n",
    "[Enter Algernon.]\n",
    "\n",
    "ALGERNON. How are you, my dear Ernest? What brings you up to town?\n",
    "\n",
    "JACK. Oh, pleasure, pleasure. What else should bring one anywhere?\n",
    "\n",
    "ALGERNON. Eating as usual, I see.\n",
    "\n",
    "JACK. I believe it is customary in good society to take some\n",
    "refreshment at five o'clock.\n",
    "\n",
    "ALGERNON. Well, it is a custom that I approve of, and I will do my best\n",
    "to start it again. However, you are not quite truthful. You did not\n",
    "come up for pleasure.\n",
    "\n",
    "JACK. What on earth do you mean?\n",
    "\n",
    "ALGERNON. You came up to town to tell me to keep away from your cousin.\n",
    "\n",
    "JACK. My cousin?\n",
    "\n",
    "ALGERNON. Yes. That charming girl you are always talking about.\n",
    "\n",
    "JACK. Cecily?\n",
    "\n",
    "ALGERNON. Cecily. She is my cousin now, you know.\n",
    "\n",
    "JACK. You have never met her.\n",
    "\n",
    "ALGERNON. She is my cousin because I intend to marry her.\n",
    "'''\n",
    "\n",
    "def parse_play_to_turns(text: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse a simple play excerpt into (speaker, utterance) turns.\n",
    "    Assumptions.\n",
    "    1) Speaker turns look like 'NAME.' at the start of a line.\n",
    "    2) Stage directions are in [brackets] or parentheses and are dropped.\n",
    "\n",
    "    Returns a DataFrame with columns: turn_id, speaker, text.\n",
    "    \"\"\"\n",
    "    lines = [ln.strip() for ln in text.strip().splitlines() if ln.strip()]\n",
    "    turns = []\n",
    "    current_speaker = None\n",
    "    buffer = []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal buffer, current_speaker\n",
    "        if current_speaker and buffer:\n",
    "            utt = \" \".join(buffer).strip()\n",
    "            utt = re.sub(r\"\\s+\", \" \", utt)\n",
    "            if utt:\n",
    "                turns.append({\"speaker\": current_speaker, \"text\": utt})\n",
    "        buffer = []\n",
    "\n",
    "    speaker_pat = re.compile(r\"^([A-Z][A-Z\\s'\\-]+)\\.\\s*(.*)$\")\n",
    "\n",
    "    for ln in lines:\n",
    "        if ln.startswith(\"[\") and ln.endswith(\"]\"):\n",
    "            continue\n",
    "        if ln.startswith(\"(\") and ln.endswith(\")\"):\n",
    "            continue\n",
    "\n",
    "        m = speaker_pat.match(ln)\n",
    "        if m:\n",
    "            flush()\n",
    "            current_speaker = m.group(1).strip()\n",
    "            rest = m.group(2).strip()\n",
    "            if rest:\n",
    "                buffer.append(rest)\n",
    "        else:\n",
    "            buffer.append(ln)\n",
    "\n",
    "    flush()\n",
    "    df = pd.DataFrame(turns)\n",
    "    df.insert(0, \"turn_id\", range(len(df)))\n",
    "    return df\n",
    "\n",
    "turns_df = parse_play_to_turns(RAW_TEXT)\n",
    "turns_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ad9d0",
   "metadata": {},
   "source": [
    "### 1.1 Create dialogue windows\n",
    "\n",
    "Many dialogue datasets are long conversations. Summarization is easier to teach with smaller windows. We will create overlapping windows of turns, then treat each window as a dialogue sample.\n",
    "\n",
    "You can adjust the window size. Smaller windows are easier for small models. Larger windows stress test context handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b57c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dialogue_windows(turns: pd.DataFrame, window_turns: int = 10, stride: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a turn DataFrame into overlapping dialogue windows.\n",
    "\n",
    "    Returns a DataFrame with: sample_id, dialogue_text, speakers_involved, n_turns.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    n = len(turns)\n",
    "    sample_id = 0\n",
    "    for start in range(0, max(1, n - window_turns + 1), stride):\n",
    "        end = min(n, start + window_turns)\n",
    "        chunk = turns.iloc[start:end]\n",
    "        dialogue_lines = [f\"{r.speaker}: {r.text}\" for r in chunk.itertuples()]\n",
    "        dialogue_text = \"\\n\".join(dialogue_lines)\n",
    "        speakers = sorted(set(chunk[\"speaker\"].tolist()))\n",
    "        samples.append(\n",
    "            {\n",
    "                \"sample_id\": sample_id,\n",
    "                \"dialogue_text\": dialogue_text,\n",
    "                \"speakers_involved\": speakers,\n",
    "                \"n_turns\": int(end - start),\n",
    "            }\n",
    "        )\n",
    "        sample_id += 1\n",
    "        if end == n:\n",
    "            break\n",
    "    return pd.DataFrame(samples)\n",
    "\n",
    "samples_df = make_dialogue_windows(turns_df, window_turns=10, stride=6)\n",
    "samples_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6296f",
   "metadata": {},
   "source": [
    "### 1.2 Preview one sample\n",
    "\n",
    "Read the dialogue. Then, in your own words, write a one sentence summary in the next cell. Keep it short. This will become our first human reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070188fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = samples_df.loc[0, \"dialogue_text\"]\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your one sentence reference summary.\n",
    "# You can edit this string. The notebook will still run if you do not.\n",
    "\n",
    "REFERENCE_SUMMARY = \"Jack arrives and learns Algernon is visiting, then Algernon teases Jack and reveals he plans to marry Jack's cousin Cecily.\"\n",
    "\n",
    "print(REFERENCE_SUMMARY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6406b9ca",
   "metadata": {},
   "source": [
    "## 2. Baseline. Extractive TextRank summarization\n",
    "\n",
    "Before using an LLM, build a baseline that is fast, cheap, and interpretable. TextRank selects the most central sentences using a similarity graph and PageRank.\n",
    "\n",
    "This baseline is language agnostic, as long as you can split text into sentences. That is why it is valuable for low resource languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2aed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Very simple sentence splitter.\n",
    "    For robust multilingual splitting, consider spaCy or Stanza.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    sents = re.split(r\"(?<=[\\.\\?\\!])\\s+\", text)\n",
    "    return [s.strip() for s in sents if s.strip()]\n",
    "\n",
    "def textrank_summarize(dialogue_text: str, max_sentences: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Extractive summarization using TextRank on sentence similarity.\n",
    "    \"\"\"\n",
    "    content = re.sub(r\"^[A-Z][A-Z\\s'\\-]+:\\s*\", \"\", dialogue_text, flags=re.MULTILINE)\n",
    "    sentences = split_sentences(content)\n",
    "    if not sentences:\n",
    "        return \"\"\n",
    "    if len(sentences) <= max_sentences:\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    sim = cosine_similarity(X)\n",
    "    np.fill_diagonal(sim, 0.0)\n",
    "\n",
    "    graph = nx.from_numpy_array(sim)\n",
    "    scores = nx.pagerank(graph, max_iter=200)\n",
    "\n",
    "    ranked = sorted(range(len(sentences)), key=lambda i: scores.get(i, 0.0), reverse=True)\n",
    "    picked = sorted(ranked[:max_sentences])\n",
    "    return \" \".join([sentences[i] for i in picked])\n",
    "\n",
    "baseline_summary = textrank_summarize(sample, max_sentences=2)\n",
    "print(\"Baseline summary:\\n\", baseline_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a7d4f",
   "metadata": {},
   "source": [
    "### 2.1 Quick evaluation. ROUGE\n",
    "\n",
    "ROUGE is imperfect, but it is a quick sanity check. We will compute ROUGE 1, ROUGE 2, and ROUGE L against your reference summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ed187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def rouge_scores(pred: str, ref: str) -> Dict[str, float]:\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    scores = scorer.score(ref, pred)\n",
    "    return {k: v.fmeasure for k, v in scores.items()}\n",
    "\n",
    "print(\"ROUGE (baseline vs reference):\")\n",
    "rouge_scores(baseline_summary, REFERENCE_SUMMARY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97e0e1",
   "metadata": {},
   "source": [
    "## 3. Mini quiz. What makes dialogue summarization harder?\n",
    "\n",
    "Try to answer before running the cell. Then run it for instant feedback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c54153",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    widgets = None\n",
    "\n",
    "QUESTION = \"Which factor is most specific to dialogue summarization, compared to single speaker summarization?\"\n",
    "OPTIONS = [\n",
    "    \"A. Dialogues contain named entities.\",\n",
    "    \"B. Dialogues include speaker turns and pragmatic intent.\",\n",
    "    \"C. Dialogues use punctuation.\",\n",
    "    \"D. Dialogues are always longer than articles.\",\n",
    "]\n",
    "CORRECT = 1\n",
    "EXPLANATION = \"Speaker turns and pragmatic intent are core. You often need to resolve who said what and why.\"\n",
    "\n",
    "def run_quiz():\n",
    "    if widgets is None:\n",
    "        print(QUESTION)\n",
    "        for opt in OPTIONS:\n",
    "            print(opt)\n",
    "        print(\"\\nCorrect:\", OPTIONS[CORRECT])\n",
    "        print(\"Explanation:\", EXPLANATION)\n",
    "        return\n",
    "\n",
    "    radio = widgets.RadioButtons(options=OPTIONS, description=\"Your answer:\")\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def on_change(change):\n",
    "        if change[\"name\"] != \"value\":\n",
    "            return\n",
    "        with out:\n",
    "            out.clear_output()\n",
    "            idx = OPTIONS.index(change[\"new\"])\n",
    "            if idx == CORRECT:\n",
    "                print(\"Correct.\")\n",
    "            else:\n",
    "                print(\"Not quite.\")\n",
    "            print(\"Explanation:\", EXPLANATION)\n",
    "\n",
    "    radio.observe(on_change)\n",
    "    display(radio, out)\n",
    "\n",
    "run_quiz()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258f279",
   "metadata": {},
   "source": [
    "üîç **Checkpoint 1: You now have a solid baseline!** \n",
    "\n",
    "**‚úÖ What you've accomplished:**\n",
    "- Built a dialogue dataset from raw text\n",
    "- Implemented TextRank extractive summarization  \n",
    "- Evaluated with ROUGE metrics\n",
    "- Learned what makes dialogue summarization challenging\n",
    "\n",
    "**üéØ Next up:** We'll simulate low-resource conditions and learn adaptation strategies.\n",
    "\n",
    "**üí° For LLM-based summarization:** Check out Session 2 on Prompt Engineering!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9612676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Quick Results Summary\n",
    "\n",
    "# Display summary of our baseline approach\n",
    "results_summary = {\n",
    "    \"Approach\": \"TextRank (Extractive)\",\n",
    "    \"Model Size\": \"No model required\",\n",
    "    \"Hardware\": \"CPU sufficient\", \n",
    "    \"Language Support\": \"Any language\",\n",
    "    \"Training Data\": \"None required\",\n",
    "    \"Key Advantage\": \"Fast, interpretable, language-agnostic\"\n",
    "}\n",
    "\n",
    "print(\"üéØ BASELINE APPROACH SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for key, value in results_summary.items():\n",
    "    print(f\"{key:15}: {value}\")\n",
    "    \n",
    "print(f\"\\n‚úÖ Your ROUGE score: {rouge_scores(baseline_summary, REFERENCE_SUMMARY)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc256477",
   "metadata": {},
   "source": [
    "## 4. Low Resource Mode üåç\n",
    "\n",
    "Now we'll simulate low-resource conditions and learn adaptation strategies.\n",
    "\n",
    "**What makes a language \"low-resource\"?**\n",
    "- Very little labeled data\n",
    "- Limited preprocessing tools  \n",
    "- Domain mismatch with training data\n",
    "- Orthographic variation and noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f93aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1 Simulate Low-Resource Conditions\n",
    "\n",
    "#We'll corrupt our clean English dialogue to simulate challenges faced by low-resource languages:\n",
    "\n",
    "def low_resource_corrupt(text: str, drop_punct_prob: float = 0.5, typo_prob: float = 0.08) -> str:\n",
    "    \"\"\"Simulate low-resource conditions by adding noise\"\"\"\n",
    "    import random\n",
    "    rng = random.Random(842)\n",
    "    out_chars = []\n",
    "    for ch in text:\n",
    "        # Randomly drop punctuation\n",
    "        if ch in \".?!,\" and rng.random() < drop_punct_prob:\n",
    "            continue\n",
    "        # Add random typos\n",
    "        if ch.isalpha() and rng.random() < typo_prob:\n",
    "            if rng.random() < 0.5:\n",
    "                out_chars.append(ch.swapcase())  # Case error\n",
    "            else:\n",
    "                out_chars.append(chr(((ord(ch.lower()) - 97 + 1) % 26) + 97))  # Letter shift\n",
    "        else:\n",
    "            out_chars.append(ch)\n",
    "    return \"\".join(out_chars)\n",
    "\n",
    "# Apply corruption to our sample\n",
    "low_resource_sample = low_resource_corrupt(sample, drop_punct_prob=0.6, typo_prob=0.04)\n",
    "print(\"üåç SIMULATED LOW-RESOURCE TEXT:\")\n",
    "print(\"=\"*60)\n",
    "print(low_resource_sample[:400] + \"...\")\n",
    "print(\"\\nüí° Notice: Missing punctuation, typos, inconsistent casing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c400a2f",
   "metadata": {},
   "source": [
    "### 4.2 Prompt remix playground\n",
    "\n",
    "You will remix a prompt by selecting options. This is a safe way to teach prompt engineering without making it feel abstract.\n",
    "\n",
    "Pick your settings, then run the cell. Try to make the summary both concise and faithful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2015b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_OPTIONS = [\"neutral\", \"bullet\", \"tweet\", \"meeting_minutes\"]\n",
    "FOCUS_OPTIONS = [\"decisions\", \"conflict\", \"relationships\", \"actions\"]\n",
    "\n",
    "def build_prompt(style: str, focus: str, max_sentences: int) -> str:\n",
    "    style = style.lower().strip()\n",
    "    focus = focus.lower().strip()\n",
    "\n",
    "    base = f\"Summarize the conversation in at most {max_sentences} sentences.\"\n",
    "    if focus == \"decisions\":\n",
    "        base += \" Focus on decisions and commitments.\"\n",
    "    elif focus == \"conflict\":\n",
    "        base += \" Focus on disagreements and what caused them.\"\n",
    "    elif focus == \"relationships\":\n",
    "        base += \" Focus on who relates to whom and the social situation.\"\n",
    "    elif focus == \"actions\":\n",
    "        base += \" Focus on actions and next steps.\"\n",
    "\n",
    "    if style == \"bullet\":\n",
    "        base += \" Use 2 to 4 bullet points.\"\n",
    "    elif style == \"tweet\":\n",
    "        base += \" Write it as a single tweet style sentence, under 240 characters.\"\n",
    "    elif style == \"meeting_minutes\":\n",
    "        base += \" Format as meeting minutes with sections: Context, Key Points, Next Steps.\"\n",
    "\n",
    "    base += \" Do not invent facts. Preserve names.\"\n",
    "    return base\n",
    "\n",
    "def run_playground(style=\"neutral\", focus=\"relationships\", max_sentences=2):\n",
    "    prompt = build_prompt(style, focus, max_sentences)\n",
    "    print(\"Prompt:\\n\", prompt, \"\\n\")\n",
    "    out = generate_summary_t5(sample, prompt, max_new_tokens=120, temperature=0.0)\n",
    "    print(\"Model output:\\n\", out)\n",
    "    return out\n",
    "\n",
    "llm_play = run_playground(style=\"meeting_minutes\", focus=\"relationships\", max_sentences=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44589539",
   "metadata": {},
   "source": [
    "### üö® Note: LLM Content Moved to Session 2\n",
    "\n",
    "**Looking for prompt engineering and LLM-based summarization?**\n",
    "\n",
    "All LLM-related content (zero-shot, few-shot, Chain-of-Thought prompting) has been moved to **Session 2: Pretrained Models and Prompt Engineering**.\n",
    "\n",
    "This keeps Session 1 focused on robust baseline methods that work without large language models.\n",
    "\n",
    "**üéØ Session 1 Focus:** TextRank, extractive methods, low-resource adaptations  \n",
    "**üéØ Session 2 Focus:** LLM prompting, generation parameters, cross-lingual transfer\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è Sections 4.2-4.4 below contain placeholder LLM code** that will show \"[This functionality moved to Session 2]\". \n",
    "\n",
    "**For working LLM examples:** Use **Session 2: Pretrained Models and Prompt Engineering**\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 Normalization Strategies for Low-Resource Languages\n",
    "\n",
    "Let's explore practical techniques for handling noisy, inconsistent text:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c375adbb",
   "metadata": {},
   "source": [
    "### 4.3 One shot and few shot prompts\n",
    "\n",
    "When you have little data, examples are powerful. We will create a small in notebook prompt set.\n",
    "\n",
    "You can replace the examples with your own dialogues later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9939ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_DIALOGUE = \"\"\"ALICE: Are we still meeting at 3?\n",
    "BOB: Yes, but I will be 10 minutes late.\n",
    "ALICE: Ok. Please bring the slides.\n",
    "BOB: Will do.\"\"\"\n",
    "\n",
    "EXAMPLE_SUMMARY = \"Alice and Bob confirm a 3 pm meeting. Bob will arrive 10 minutes late and will bring the slides.\"\n",
    "\n",
    "ONE_SHOT_PROMPT = f\"\"\"Summarize the conversation in 1 to 2 sentences. Do not invent facts.\n",
    "\n",
    "Example.\n",
    "DIALOGUE:\n",
    "{EXAMPLE_DIALOGUE}\n",
    "\n",
    "SUMMARY:\n",
    "{EXAMPLE_SUMMARY}\n",
    "\n",
    "Now summarize this dialogue.\n",
    "\"\"\"\n",
    "\n",
    "llm_one = generate_summary_t5(sample, ONE_SHOT_PROMPT, max_new_tokens=120, temperature=0.0)\n",
    "print(llm_one)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395098e",
   "metadata": {},
   "source": [
    "### 4.4 Generation parameters. Temperature and length\n",
    "\n",
    "Temperature can change factuality. Length controls how much detail you get.\n",
    "\n",
    "Use the sliders if available. Otherwise, edit the numbers and rerun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_generation_controls(temperature: float = 0.0, max_new_tokens: int = 80):\n",
    "    prompt = build_prompt(style=\"neutral\", focus=\"actions\", max_sentences=2)\n",
    "    out = generate_summary_t5(sample, prompt, max_new_tokens=max_new_tokens, temperature=temperature, top_p=0.95)\n",
    "    print(\"temperature:\", temperature, \"max_new_tokens:\", max_new_tokens)\n",
    "    print(out)\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    if tokenizer is None or model is None:\n",
    "        raise RuntimeError(\"Model not available, skipping widgets.\")\n",
    "    ui = widgets.interactive(\n",
    "        demo_generation_controls,\n",
    "        temperature=widgets.FloatSlider(min=0.0, max=1.0, step=0.1, value=0.0),\n",
    "        max_new_tokens=widgets.IntSlider(min=30, max=200, step=10, value=80),\n",
    "    )\n",
    "    display(ui)\n",
    "except Exception:\n",
    "    demo_generation_controls(temperature=0.0, max_new_tokens=80)\n",
    "    demo_generation_controls(temperature=0.7, max_new_tokens=120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7860c7",
   "metadata": {},
   "source": [
    "## 5. Compare baselines vs LLM\n",
    "\n",
    "We compare summaries and compute ROUGE against your reference.\n",
    "\n",
    "In real work, you should also do human evaluation. For example factuality checks, missing action items, and speaker attribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "results.append((\"TextRank baseline\", baseline_summary))\n",
    "results.append((\"LLM zero shot\", llm_zero))\n",
    "results.append((\"LLM one shot\", llm_one))\n",
    "results.append((\"LLM prompt remix\", llm_play))\n",
    "\n",
    "rows = []\n",
    "for name, pred in results:\n",
    "    rows.append({\"system\": name, \"summary\": pred, **rouge_scores(pred, REFERENCE_SUMMARY)})\n",
    "\n",
    "pd.DataFrame(rows).sort_values(\"rougeL\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e6fb1",
   "metadata": {},
   "source": [
    "## 6. Low resource mode. Make English behave like a low resource language\n",
    "\n",
    "Low resource usually means one or more of the following.\n",
    "- Very little labeled data.\n",
    "- Limited tools for tokenization, sentence splitting, and normalization.\n",
    "- Domain mismatch. Your data looks different from what models saw during pre training.\n",
    "- Orthography variation and borrowing, including code switching.\n",
    "\n",
    "We will simulate these constraints in English by.\n",
    "1) Reducing the available context.\n",
    "2) Corrupting the text with noise and inconsistent spelling.\n",
    "3) Removing punctuation, which hurts naive sentence splitting.\n",
    "\n",
    "Then we apply strategies that transfer to true low resource settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_resource_corrupt(text: str, drop_punct_prob: float = 0.5, typo_prob: float = 0.08) -> str:\n",
    "    rng = random.Random(842)\n",
    "    out_chars = []\n",
    "    for ch in text:\n",
    "        if ch in \".?!,\" and rng.random() < drop_punct_prob:\n",
    "            continue\n",
    "        if ch.isalpha() and rng.random() < typo_prob:\n",
    "            if rng.random() < 0.5:\n",
    "                out_chars.append(ch.swapcase())\n",
    "            else:\n",
    "                out_chars.append(chr(((ord(ch.lower()) - 97 + 1) % 26) + 97))\n",
    "        else:\n",
    "            out_chars.append(ch)\n",
    "    return \"\".join(out_chars)\n",
    "\n",
    "low_text = low_resource_corrupt(sample, drop_punct_prob=0.8, typo_prob=0.05)\n",
    "print(low_text[:600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc3964",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline on clean text:\")\n",
    "print(textrank_summarize(sample, max_sentences=2))\n",
    "print(\"\\nBaseline on low resource corrupted text:\")\n",
    "print(textrank_summarize(low_text, max_sentences=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df866059",
   "metadata": {},
   "source": [
    "### 6.1 Strategy toolkit\n",
    "\n",
    "Here are practical tactics that often help in low resource dialogue summarization.\n",
    "\n",
    "1. Normalize input.\n",
    "   - Fix common punctuation issues.\n",
    "   - Normalize whitespace.\n",
    "   - Normalize speaker labels.\n",
    "\n",
    "2. Use robust segmentation.\n",
    "   - If sentence splitting fails, summarize at turn level.\n",
    "\n",
    "3. Constrain generation.\n",
    "   - Use explicit length limits.\n",
    "   - Instruct the model to preserve names, numbers, and decisions.\n",
    "\n",
    "4. Add lightweight context.\n",
    "   - Provide a glossary of names and places.\n",
    "   - Provide a domain hint, such as \"family conversation\" or \"customer support\".\n",
    "\n",
    "5. Evaluate with targeted checks.\n",
    "   - Did we preserve who wants to marry whom.\n",
    "   - Did we hallucinate actions that never happened.\n",
    "\n",
    "We will implement 1 and 2 now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a203e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dialogue(text: str) -> str:\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"([A-Z][A-Z\\s'\\-]+:)\\s*\", r\"\\n\\1 \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def turn_level_summarize(dialogue_text: str, max_turns: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Extractive turn level summarization, more robust than sentence splitting.\n",
    "    \"\"\"\n",
    "    lines = [ln.strip() for ln in dialogue_text.splitlines() if ln.strip()]\n",
    "    lines = [ln for ln in lines if len(ln) > 10]\n",
    "    if not lines:\n",
    "        return \"\"\n",
    "    if len(lines) <= max_turns:\n",
    "        return \" \".join(lines)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(lines)\n",
    "    sim = cosine_similarity(X)\n",
    "    np.fill_diagonal(sim, 0.0)\n",
    "    graph = nx.from_numpy_array(sim)\n",
    "    scores = nx.pagerank(graph, max_iter=200)\n",
    "    ranked = sorted(range(len(lines)), key=lambda i: scores.get(i, 0.0), reverse=True)\n",
    "    picked = sorted(ranked[:max_turns])\n",
    "    return \" \".join([lines[i] for i in picked])\n",
    "\n",
    "print(\"Before normalization:\\n\", low_text[:250], \"\\n\")\n",
    "norm_low = normalize_dialogue(low_text)\n",
    "print(\"After normalization:\\n\", norm_low[:250])\n",
    "print(\"\\nTurn level summary on corrupted text:\\n\", turn_level_summarize(norm_low, max_turns=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8837fd",
   "metadata": {},
   "source": [
    "### 6.2 Low resource prompting\n",
    "\n",
    "If you can use an instruction model, you can push it to behave better on noisy input. The key is to add constraints.\n",
    "\n",
    "We will.\n",
    "- Ask for short output.\n",
    "- Ask it to avoid inventing facts.\n",
    "- Ask it to preserve names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd8227",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOW_RESOURCE_PROMPT = \"\"\"Summarize the conversation in 1 sentence.\n",
    "Rules.\n",
    "1) Do not invent facts.\n",
    "2) Preserve names exactly as they appear.\n",
    "3) If the text is noisy, infer only what is obvious.\"\"\"\n",
    "\n",
    "if tokenizer is None or model is None:\n",
    "    print(\"Model not available, skipping.\")\n",
    "else:\n",
    "    print(generate_summary_t5(norm_low, LOW_RESOURCE_PROMPT, max_new_tokens=60, temperature=0.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f285af",
   "metadata": {},
   "source": [
    "## Optional mini dataset hook. Try a real non-English case in two minutes\n",
    "\n",
    "This workshop is designed to start in English, then transfer the same workflow to a low-resource language.\n",
    "\n",
    "Below are two quick options.\n",
    "\n",
    "1. **MiniLux micro-set (synthetic)**. A small set of short Luxembourgish and LU, FR mixed snippets created for teaching. It is intentionally tiny and imperfect, so that you can iterate fast and discuss typical issues, like code-switching, named entities, and spelling variation.\n",
    "\n",
    "2. **Hugging Face low-resource sample (real text)**. Pull 20 examples from a multilingual summarization dataset and run the same prompt, plus the same evaluation, to see how performance changes outside English.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1. MiniLux micro-set (synthetic)\n",
    "# This is only for the workshop. You can replace it with your own low-resource dialogues later.\n",
    "\n",
    "mini_lux = [\n",
    "    {\n",
    "        \"id\": \"lux_001\",\n",
    "        \"dialogue\": \"A: Moien. Hues du Z√§it fir e Kaffi?\\nB: Jo, m√§ just z√©ng Minutten. Ech muss gl√§ich op d'Aarbecht.\\nA: Ok. Mir treffen eis beim Gare.\\nB: Super, ech kommen direkt.\",\n",
    "        \"reference_summary_en\": \"They agree to meet for a quick coffee at the station before B goes to work.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_002\",\n",
    "        \"dialogue\": \"A: W√©i war d'Reunioun haut?\\nB: Ganz laang. Mir hu just d'Agenda diskut√©iert.\\nA: An hu mir eng Decisioun?\\nB: Nee, mir maachen et n√§chste Woch nach eng K√©ier.\",\n",
    "        \"reference_summary_en\": \"The meeting was long, they only discussed the agenda, and no decision was made.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_003\",\n",
    "        \"dialogue\": \"A: Kanns du mer de Rapport sch√©cken?\\nB: Jo. Ech sch√©cken en elo per Mail.\\nA: Merci. Ech muss en nach haut ofginn.\\nB: Kloer, ech maachen et direkt.\",\n",
    "        \"reference_summary_en\": \"B will email A the report immediately because A must submit it today.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_004\",\n",
    "        \"dialogue\": \"A: Ech sinn am Stau op der A6.\\nB: Ok, dann f√§nke mir ouni dech un.\\nA: Gitt mir z√©ng Minutten.\\nB: Passt. Mir halen dir e S√´tz fr√§i.\",\n",
    "        \"reference_summary_en\": \"A is stuck in traffic but will arrive in about ten minutes, and the others will start and save a seat.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_005\",\n",
    "        \"dialogue\": \"A: Ech hu muer en rendez-vous chez le m√©decin.\\nB: Bass du ok?\\nA: Jo, just e Check-up.\\nB: Ok, soen mer dono w√©i et gaangen ass.\",\n",
    "        \"reference_summary_en\": \"A has a doctor appointment tomorrow for a check-up and will update B afterward.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_006\",\n",
    "        \"dialogue\": \"A: Wou si mir mam Projet?\\nB: Mir hu 80 Prozent f√§erdeg.\\nA: Wat feelt nach?\\nB: D'Dokumentatioun an d'Tester.\",\n",
    "        \"reference_summary_en\": \"The project is about 80 percent done, but documentation and testing are still missing.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_007\",\n",
    "        \"dialogue\": \"A: Ech kr√©ien √´mmer eng Fehlermeldung.\\nB: W√©i eng?\\nA: 'Permission denied'.\\nB: Dann hues du wahrscheinlech keng Rechter. Prob√©ier et mat sudo oder fro den Admin.\",\n",
    "        \"reference_summary_en\": \"A gets a permission error, and B suggests using sudo or asking the admin for access.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_008\",\n",
    "        \"dialogue\": \"A: Mir treffen eis um 14:00.\\nB: Ech sinn um 14:15 do.\\nA: Ok, ech waarden am Caf√©.\\nB: Merci. Bis gl√§ich.\",\n",
    "        \"reference_summary_en\": \"They planned to meet at 14:00, but B will arrive at 14:15 and A will wait at a caf√©.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_009\",\n",
    "        \"dialogue\": \"A: Hues du d'Presentatioun gesinn?\\nB: Jo, si ass gutt, m√§ d'Grafike sinn ze kleng.\\nA: Ok, ech maachen se m√©i grouss.\\nB: Super, dann ass et perfekt.\",\n",
    "        \"reference_summary_en\": \"B thinks the presentation is good but the charts are too small, so A will enlarge them.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_010\",\n",
    "        \"dialogue\": \"A: Ech sinn haut am Homeoffice.\\nB: Ok, k√´nns du trotzdem an de Call?\\nA: Jo, ech sinn do um 10:00.\\nB: Top, ech sch√©cken de Link.\",\n",
    "        \"reference_summary_en\": \"A works from home but will join the 10:00 call, and B will send the link.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_011\",\n",
    "        \"dialogue\": \"A: Mir brauche nach e Beispill fir d'Course.\\nB: Wat fir ee Beispill?\\nA: E klengt Dialog-Set fir Zesummefaassung.\\nB: Ok, ech schreiwen 20 kuerz Dialogen.\",\n",
    "        \"reference_summary_en\": \"They need a small dialogue dataset for a summarization course, and B will write 20 short dialogues.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_012\",\n",
    "        \"dialogue\": \"A: Kanns du den Text nach eng K√©ier kontroll√©ieren?\\nB: Jo, ech kucken no Tippfeeler.\\nA: An och Punktuatioun.\\nB: Maachen ech.\",\n",
    "        \"reference_summary_en\": \"B will proofread the text for typos and punctuation.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_013\",\n",
    "        \"dialogue\": \"A: Ech hu meng Schl√´sselen vergiess.\\nB: Wou bass du?\\nA: Virun der Dier.\\nB: Ech kommen, ginn mer f√´nnef Minutten.\",\n",
    "        \"reference_summary_en\": \"A forgot their keys and is locked out, and B will come in five minutes.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_014\",\n",
    "        \"dialogue\": \"A: De Bus k√´nnt net.\\nB: Hues du d'App gekuckt?\\nA: Jo, et steet 'retard'.\\nB: Dann huele mir en Taxi.\",\n",
    "        \"reference_summary_en\": \"The bus is delayed, so they decide to take a taxi.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_015\",\n",
    "        \"dialogue\": \"A: Ech muss nach d'Fichieren eroplueden.\\nB: Wou?\\nA: Op Hugging Face.\\nB: Ok, vergiss net d'Lizens an d'Readme.\",\n",
    "        \"reference_summary_en\": \"A needs to upload files to Hugging Face, and B reminds them to include a license and README.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_016\",\n",
    "        \"dialogue\": \"A: D'GPU ass fr√§i.\\nB: Super, dann starte mir den Training.\\nA: Ech setzen batch size op 4.\\nB: Ok, da maache mir gradient accumulation.\",\n",
    "        \"reference_summary_en\": \"They have GPU availability and will start training with a small batch size and gradient accumulation.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_017\",\n",
    "        \"dialogue\": \"A: Kanns du mir den Deadline soen?\\nB: Et ass Freideg um 18:00.\\nA: Merci, ech maachen et haut nach.\\nB: Gutt Iddi.\",\n",
    "        \"reference_summary_en\": \"The deadline is Friday at 18:00, and A plans to finish today.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_018\",\n",
    "        \"dialogue\": \"A: Ech hunn d'Donn√©e√´n gereinegt.\\nB: Super. Hues du och d'Nummeren normalis√©iert?\\nA: Jo, ech hunn se an Wierder √´mgewandelt.\\nB: Perfekt.\",\n",
    "        \"reference_summary_en\": \"A cleaned the data and normalized numbers by converting them into words.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_019\",\n",
    "        \"dialogue\": \"A: Ech verstinn d'Resultater net.\\nB: Wat ass komesch?\\nA: D'Accuracy ass h√©ich, m√§ d'F1 ass niddreg.\\nB: Dann ass et wahrscheinlech Klassen-Imbalance.\",\n",
    "        \"reference_summary_en\": \"Accuracy is high but F1 is low, suggesting class imbalance.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_020\",\n",
    "        \"dialogue\": \"A: Tu peux me rappeler le plan?\\nB: Oui. D'abord on teste en anglais, apr√®s on passe au luxembourgeois.\\nA: An de Prompt bleift √§hnlech.\\nB: Genau.\",\n",
    "        \"reference_summary_en\": \"They will test in English first, then switch to Luxembourgish while keeping a similar prompt.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_021\",\n",
    "        \"dialogue\": \"A: Ech sinn net s√©cher ob 'Zentrum' richteg ass.\\nB: Et h√§nkt vum Dialektgebiet of.\\nA: Ok, ech kontroll√©ieren d'Metadata.\\nB: Gutt, d'Labels mussen konsistent sinn.\",\n",
    "        \"reference_summary_en\": \"They will verify the metadata because dialect labels must be consistent.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_022\",\n",
    "        \"dialogue\": \"A: D'Audio ass ze laang.\\nB: W√©i laang?\\nA: 25 Sekonnen.\\nB: Dann schneiden mir et op 10 Sekonnen fir d'Training.\",\n",
    "        \"reference_summary_en\": \"The audio is 25 seconds long, so they will trim it to 10 seconds for training.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_023\",\n",
    "        \"dialogue\": \"A: Ech hu keng Internet um Laptop.\\nB: Prob√©ier d'WLAN nei.\\nA: Ok, ech maachen restart.\\nB: Wann et net geet, huele mir en Hotspot.\",\n",
    "        \"reference_summary_en\": \"A has no internet, B suggests restarting Wi-Fi, and they may use a hotspot if needed.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_024\",\n",
    "        \"dialogue\": \"A: D'Zesummefaassung ass ze laang.\\nB: Setz eng Limit.\\nA: W√©i vill?\\nB: Prob√©ier 2 S√§tz an maximal 60 Wierder.\",\n",
    "        \"reference_summary_en\": \"They will constrain the summary length to two sentences and at most 60 words.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lux_025\",\n",
    "        \"dialogue\": \"A: Ech w√´ll eng neutral Zesummefaassung.\\nB: Da schreiwe mir am Prompt: 'neutral, factual, no opinion'.\\nA: Ok, ech testen dat.\\nB: Gutt, a kuck ob Bias k√´nnt.\",\n",
    "        \"reference_summary_en\": \"They want a neutral factual summary and will encode that in the prompt and then test for bias.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "def sample_and_summarize(dialogue_set, k=1, seed=7, prompt=ZERO_SHOT_PROMPT):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    items = random.sample(dialogue_set, k=k)\n",
    "    for ex in items:\n",
    "        print(\"ID:\", ex[\"id\"])\n",
    "        print(\"\\nDIALOGUE:\\n\", ex[\"dialogue\"])\n",
    "        pred = generate_summary_t5(ex[\"dialogue\"], prompt=prompt, max_new_tokens=80, temperature=0.0)\n",
    "        print(\"\\nMODEL SUMMARY:\\n\", pred)\n",
    "        print(\"\\nREFERENCE (EN):\\n\", ex[\"reference_summary_en\"])\n",
    "        print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "\n",
    "sample_and_summarize(mini_lux, k=2)\n",
    "\n",
    "# Option 2. Pull a tiny real low-resource sample from Hugging Face\n",
    "# This uses XL-Sum (multilingual news summarization). Not a dialogue dataset.\n",
    "# For the workshop, we convert each article into a \"pseudo-dialogue\" so we can reuse the same pipeline.\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "def article_to_pseudo_dialogue(article_text: str, max_turns: int = 6) -> str:\n",
    "    # Lightweight sentence split. Good enough for teaching.\n",
    "    sentences = [s.strip() for s in article_text.replace(\"\\n\", \" \").split(\".\") if s.strip()]\n",
    "    sentences = sentences[:max_turns]\n",
    "    turns = []\n",
    "    for i, s in enumerate(sentences):\n",
    "        speaker = \"ANCHOR\" if i % 2 == 0 else \"REPORTER\"\n",
    "        turns.append(f\"{speaker}: {s}.\")\n",
    "    return \"\\n\".join(turns)\n",
    "\n",
    "def load_low_resource_hf_sample(language_subset: str = \"yoruba\", n: int = 20):\n",
    "    ds = load_dataset(\"csebuetnlp/xlsum\", language_subset, split=f\"train[:{n}]\")\n",
    "    # XL-Sum fields are typically: \"text\" and \"summary\"\n",
    "    out = []\n",
    "    for i, ex in enumerate(ds):\n",
    "        dialogue = article_to_pseudo_dialogue(ex[\"text\"], max_turns=8)\n",
    "        out.append(\n",
    "            {\n",
    "                \"id\": f\"xlsum_{language_subset}_{i:03d}\",\n",
    "                \"dialogue\": dialogue,\n",
    "                \"reference_summary\": ex[\"summary\"],\n",
    "            }\n",
    "        )\n",
    "    return out\n",
    "\n",
    "xlsum_yoruba = load_low_resource_hf_sample(language_subset=\"yoruba\", n=5)\n",
    "print(\"Example pseudo-dialogue from XL-Sum (yoruba subset):\")\n",
    "print(xlsum_yoruba[0][\"dialogue\"])\n",
    "print(\"\\nReference summary (yoruba):\")\n",
    "print(xlsum_yoruba[0][\"reference_summary\"])\n",
    "\n",
    "print(\"\\nNow run the same English prompt on the pseudo-dialogue. It will usually struggle, and that is the point.\")\n",
    "pred = generate_summary_t5(xlsum_yoruba[0][\"dialogue\"], prompt=ZERO_SHOT_PROMPT, max_new_tokens=80, temperature=0.0)\n",
    "print(\"\\nMODEL SUMMARY:\\n\", pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d5ff6",
   "metadata": {},
   "source": [
    "## 7. Challenge. Adapt to your own low resource language\n",
    "\n",
    "Now you have an English pipeline. The next step is to replace the English dialogue with data from your target language.\n",
    "\n",
    "If you work on a language with limited resources, use the same structure.\n",
    "1) Create turns with speaker labels.\n",
    "2) Normalize and segment.\n",
    "3) Start with an extractive baseline.\n",
    "4) Add a multilingual model or a translation pivot only if you need it.\n",
    "5) Evaluate with a small set of human references.\n",
    "\n",
    "The next cell includes a ready to use template. It runs as is. Replace `MY_DIALOGUE` with your own data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0897db",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_DIALOGUE = \"\"\"SPEAKER1: Replace this with your own dialogue in any language.\n",
    "SPEAKER2: Keep speaker labels. Keep short lines if possible.\n",
    "SPEAKER1: Then rerun the cells below.\"\"\"\n",
    "\n",
    "clean = normalize_dialogue(MY_DIALOGUE)\n",
    "summary_baseline = turn_level_summarize(clean, max_turns=3)\n",
    "print(\"Baseline summary:\\n\", summary_baseline)\n",
    "\n",
    "if tokenizer is not None and model is not None:\n",
    "    prompt = build_prompt(style=\"neutral\", focus=\"actions\", max_sentences=2)\n",
    "    summary_llm = generate_summary_t5(clean, prompt, max_new_tokens=80, temperature=0.0)\n",
    "    print(\"\\nLLM summary:\\n\", summary_llm)\n",
    "else:\n",
    "    print(\"\\nLLM not available. Baseline is your default.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f06d1",
   "metadata": {},
   "source": [
    "## 8. Wrap up\n",
    "\n",
    "You now have a reproducible dialogue summarization pipeline that is usable with.\n",
    "- No LLM, via TextRank and turn level extraction.\n",
    "- A small instruction model, via prompt engineering.\n",
    "- Low resource conditions, via normalization and constraints.\n",
    "\n",
    "If you want to push further for true low resource languages.\n",
    "- Swap English stopwords for a custom list, or disable stopwords.\n",
    "- Use character n gram TF IDF for languages without whitespace.\n",
    "- Add a small glossary and a retrieval step, then feed only the relevant turns to the model.\n",
    "- Build a tiny evaluation set, 50 to 200 dialogues with one reference summary each.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
