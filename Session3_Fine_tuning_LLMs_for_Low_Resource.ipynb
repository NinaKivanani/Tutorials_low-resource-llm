{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ae7406ca",
      "metadata": {},
      "source": [
        "# Session 3: Fine-tuning LLMs for Low-Resource Languages ğŸš€\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "**ğŸ“š Course Repository:** [github.com/NinaKivanani/Tutorials_low-resource-llm](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NinaKivanani/Tutorials_low-resource-llm/blob/main/Session3_Fine_tuning_LLMs_for_Low_Resource.ipynb)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github)](https://github.com/NinaKivanani/Tutorials_low-resource-llm)\n",
        "[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "**Advanced Parameter-Efficient Fine-Tuning for Low-Resource Languages**\n",
        "\n",
        "Welcome to **Session 3**! You'll master the art and science of adapting pretrained LLMs to specialized tasks using systematic fine-tuning techniques, with focus on practical applications for low-resource languages.\n",
        "\n",
        "**ğŸ¯ Focus:** Parameter-efficient fine-tuning, LoRA, systematic evaluation  \n",
        "**ğŸ’» Requirements:** GPU recommended (Colab free tier sufficient)  \n",
        "**ğŸ”¬ Methodology:** Production-ready techniques with systematic comparison\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "**ğŸ“‹ Recommended learning path:**\n",
        "1. **Session 0:** Setup and tokenization analysis âœ…  \n",
        "2. **Session 1:** Systematic baseline techniques âœ…\n",
        "3. **Session 2:** Systematic prompt engineering âœ…  \n",
        "4. **This session (Session 3):** Advanced fine-tuning techniques â† You are here!\n",
        "\n",
        "## What You Will Master\n",
        "\n",
        "1. **ğŸ—ï¸ Fine-tuning fundamentals** - Full vs. parameter-efficient approaches with cost analysis\n",
        "2. **âš¡ LoRA and advanced PEFT** - Low-Rank Adaptation with systematic parameter optimization\n",
        "3. **ğŸ“Š Instruction tuning** - Task-specific adaptation with systematic evaluation  \n",
        "4. **ğŸ¯ Preference optimization** - Alignment techniques for better outputs\n",
        "5. **ğŸ“ˆ Systematic monitoring** - Training metrics, loss analysis, convergence patterns\n",
        "6. **ğŸŒ Low-resource adaptation** - Strategies for data-scarce languages\n",
        "7. **ğŸ­ Production deployment** - Real-world considerations and best practices\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this session, you will:\n",
        "- âœ… **Distinguish systematically** between full and parameter-efficient fine-tuning approaches\n",
        "- âœ… **Implement LoRA fine-tuning** with optimal hyperparameter selection  \n",
        "- âœ… **Monitor training systematically** using multiple metrics and visualizations\n",
        "- âœ… **Evaluate model improvements** quantitatively across multiple dimensions\n",
        "- âœ… **Design production pipelines** for low-resource language fine-tuning\n",
        "- âœ… **Apply cost-benefit analysis** for real-world deployment decisions\n",
        "\n",
        "## ğŸ”¬ Advanced Methodology\n",
        "\n",
        "**This session uses production-grade practices:**\n",
        "- **ğŸ“Š Systematic Comparison:** Multiple fine-tuning approaches with quantitative evaluation\n",
        "- **ğŸ’° Cost Analysis:** Resource requirements and ROI calculations for each approach\n",
        "- **ğŸ¯ Task-Specific Evaluation:** Beyond perplexity - task-relevant metrics\n",
        "- **ğŸŒ Cross-Lingual Validation:** Systematic evaluation across language boundaries  \n",
        "- **ğŸ“ˆ Production Readiness:** Deployment considerations and scalability analysis\n",
        "\n",
        "## How This Session Works\n",
        "\n",
        "- **ğŸ“ Theory â†’ Practice â†’ Analysis:** Learn concepts â†’ Apply systematically â†’ Measure results\n",
        "- **ğŸ”§ Hands-on Implementation:** Real code, real models, real data\n",
        "- **ğŸ“Š Quantitative Evaluation:** Every claim backed by systematic measurement\n",
        "- **ğŸ’¼ Production Focus:** Techniques you can use in real projects immediately\n",
        "- **ğŸŒ Low-Resource Emphasis:** Special attention to resource-constrained scenarios\n",
        "\n",
        "**âš ï¸ Important Note:**  \n",
        "This is a **production-oriented demonstration** using systematic methodology. While we use a small dataset for speed, all techniques scale to production systems. The focus is on **understanding systematic approaches** and **building production-ready intuitions**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6142189",
      "metadata": {},
      "source": [
        "## 0. ğŸ—ï¸ Fine-Tuning Fundamentals: Theory and Practice\n",
        "\n",
        "### 0.1 Fine-Tuning Taxonomy: A Systematic Overview\n",
        "\n",
        "**Fine-tuning** is the process of adapting a pretrained language model to specialized tasks or domains using additional labeled data. Understanding the landscape of approaches is crucial for making informed decisions.\n",
        "\n",
        "| **Approach** | **Parameters Updated** | **Memory Requirement** | **Training Speed** | **Best For** | **Cost** |\n",
        "|--------------|----------------------|----------------------|-------------------|--------------|----------|\n",
        "| **ğŸ”¥ Full Fine-tuning** | All parameters (100%) | Very High (4x model size) | Slow | High-resource tasks | $$$$$ |\n",
        "| **âš¡ Parameter-Efficient (PEFT)** | Small subset (0.1-10%) | Low (1.2x model size) | Fast | Low-resource languages | $$ |\n",
        "| **ğŸ¯ LoRA** | Low-rank adapters (~1%) | Very Low | Very Fast | Most practical cases | $ |\n",
        "| **ğŸ“š Instruction Tuning** | Task-specific layers | Medium | Medium | Following instructions | $$$ |\n",
        "| **ğŸª Preference Optimization** | Value/reward layers | Medium | Medium | Human alignment | $$$ |\n",
        "\n",
        "### 0.2 ğŸ”¬ Deep Dive: Parameter-Efficient Fine-Tuning (PEFT)\n",
        "\n",
        "**Why PEFT Matters for Low-Resource Languages:**\n",
        "\n",
        "1. **ğŸ’° Cost Effectiveness:** Train with 1000x less GPU memory\n",
        "2. **âš¡ Speed:** 10x faster training and deployment  \n",
        "3. **ğŸ›¡ï¸ Catastrophic Forgetting Prevention:** Preserve original capabilities\n",
        "4. **ğŸ”„ Task Switching:** Multiple adapters for different tasks\n",
        "5. **ğŸ“¦ Storage Efficiency:** Adapters are ~10MB vs full models at ~10GB\n",
        "\n",
        "### 0.3 ğŸ¯ LoRA (Low-Rank Adaptation) Deep Dive\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "```\n",
        "W = Wâ‚€ + Î”W = Wâ‚€ + BA\n",
        "```\n",
        "Where:\n",
        "- `Wâ‚€`: Frozen pretrained weights\n",
        "- `B`, `A`: Low-rank matrices (rank r << d) \n",
        "- `Î”W = BA`: Learned adaptation with r << original rank\n",
        "\n",
        "**Key Hyperparameters:**\n",
        "- **Rank (r):** Higher = more expressive but slower (typical: 4-64)\n",
        "- **Alpha (Î±):** Scaling factor for adaptation strength (typical: 16-32) \n",
        "- **Target Modules:** Which layers to adapt (attention vs MLP vs both)\n",
        "- **Dropout:** Regularization for adaptation layers (typical: 0.05-0.1)\n",
        "\n",
        "### 0.4 ğŸ“Š Systematic Approach to Fine-Tuning\n",
        "\n",
        "**Our methodology follows production best practices:**\n",
        "\n",
        "1. **ğŸ§ª Baseline Establishment:** Test pretrained model performance\n",
        "2. **ğŸ“Š Systematic Hyperparameter Search:** Grid search over key parameters\n",
        "3. **ğŸ“ˆ Multi-Metric Evaluation:** Beyond perplexity - task-specific metrics\n",
        "4. **ğŸ” Ablation Studies:** Understand what drives improvements\n",
        "5. **ğŸ’¼ Production Planning:** Cost analysis and deployment considerations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "401f500a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš€ Systematic Setup: GPU Configuration and Environment Check\n",
        "# Professional setup with comprehensive system analysis\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def check_system_capabilities():\n",
        "    \"\"\"Comprehensive system analysis for fine-tuning requirements\"\"\"\n",
        "    \n",
        "    print(\"ğŸ”§ SYSTEM CAPABILITY ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # GPU Analysis\n",
        "    gpu_available = torch.cuda.is_available()\n",
        "    if gpu_available:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        \n",
        "        print(f\"âœ… GPU Available: {gpu_name}\")\n",
        "        print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
        "        \n",
        "        # Memory recommendations\n",
        "        if gpu_memory >= 15:\n",
        "            print(\"   Recommendation: Can handle base models up to 7B parameters\")\n",
        "        elif gpu_memory >= 10:\n",
        "            print(\"   Recommendation: Optimal for 1-3B parameter models (TinyLlama perfect)\")\n",
        "        else:\n",
        "            print(\"   Recommendation: Use smallest models or reduce batch size\")\n",
        "            \n",
        "        # Verify CUDA version compatibility\n",
        "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
        "        \n",
        "        recommendation = \"ğŸš€ OPTIMAL: GPU detected - fast training enabled\"\n",
        "        \n",
        "    else:\n",
        "        print(\"âŒ No GPU detected\")\n",
        "        print(\"   Training will be 10-50x slower on CPU\")\n",
        "        print(\"   ğŸ’¡ For Google Colab: Runtime â†’ Change runtime type â†’ GPU\")\n",
        "        \n",
        "        recommendation = \"âš ï¸  SUBOPTIMAL: CPU-only mode - expect slow training\"\n",
        "    \n",
        "    # Python environment analysis\n",
        "    print(f\"\\nğŸ PYTHON ENVIRONMENT:\")\n",
        "    print(f\"   Version: {sys.version.split()[0]}\")\n",
        "    print(f\"   Platform: {sys.platform}\")\n",
        "    \n",
        "    # Memory analysis\n",
        "    try:\n",
        "        import psutil\n",
        "        ram_gb = psutil.virtual_memory().total / 1e9\n",
        "        print(f\"   System RAM: {ram_gb:.1f} GB\")\n",
        "        if ram_gb < 8:\n",
        "            print(\"   âš ï¸  Low RAM detected - reduce batch sizes\")\n",
        "    except ImportError:\n",
        "        print(\"   System RAM: Unable to detect (install psutil for details)\")\n",
        "    \n",
        "    print(f\"\\nğŸ¯ OVERALL RECOMMENDATION:\")\n",
        "    print(f\"   {recommendation}\")\n",
        "    \n",
        "    return gpu_available\n",
        "\n",
        "# Run system analysis\n",
        "gpu_available = check_system_capabilities()\n",
        "\n",
        "# Set optimal device configuration\n",
        "device = \"cuda\" if gpu_available else \"cpu\"\n",
        "print(f\"\\nâš™ï¸  Using device: {device.upper()}\")\n",
        "\n",
        "# Configure memory optimization if needed\n",
        "if gpu_available:\n",
        "    # Enable memory fraction for shared environments like Colab\n",
        "    torch.cuda.empty_cache()  # Clear any existing cache\n",
        "    print(\"âœ… GPU memory optimized for shared environments\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa5b395f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“¦ Systematic Package Installation for Advanced Fine-Tuning\n",
        "# Production-grade setup with systematic dependency management and PEFT fix\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_packages_systematic():\n",
        "    \"\"\"Install packages with systematic dependency management and verification\"\"\"\n",
        "    \n",
        "    print(\"ğŸš€ SYSTEMATIC PACKAGE INSTALLATION\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"â±ï¸  This will take 2-4 minutes in Colab...\")\n",
        "    \n",
        "    # CRITICAL: Install in specific order to avoid dependency conflicts\n",
        "    # PEFT requires transformers and accelerate to be installed first\n",
        "    \n",
        "    print(\"\\nğŸ“Š Installing FOUNDATION PACKAGES...\")\n",
        "    foundation_packages = [\n",
        "        \"transformers>=4.35.0\",  # Must install first\n",
        "        \"accelerate>=0.23.0\",     # Required before PEFT\n",
        "    ]\n",
        "    \n",
        "    for package in foundation_packages:\n",
        "        try:\n",
        "            print(f\"  ğŸ“¥ {package}\")\n",
        "            subprocess.check_call([\n",
        "                sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                \"--upgrade\", package\n",
        "            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "            print(f\"  âœ… {package}\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"  âŒ Failed to install {package}\")\n",
        "            return False\n",
        "    \n",
        "    # CRITICAL FIX: Install PEFT with specific method\n",
        "    print(\"\\nâš¡ Installing PEFT (Parameter-Efficient Fine-Tuning)...\")\n",
        "    peft_installed = False\n",
        "    \n",
        "    # Method 1: Try standard pip install\n",
        "    try:\n",
        "        print(\"  ğŸ“¥ Attempting standard installation...\")\n",
        "        subprocess.check_call([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \n",
        "            \"--upgrade\", \"peft>=0.6.0\"\n",
        "        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        print(\"  âœ… PEFT installed successfully (standard method)\")\n",
        "        peft_installed = True\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"  âš ï¸  Standard installation failed, trying alternative...\")\n",
        "    \n",
        "    # Method 2: Try without version constraint\n",
        "    if not peft_installed:\n",
        "        try:\n",
        "            print(\"  ğŸ“¥ Attempting installation without version constraint...\")\n",
        "            subprocess.check_call([\n",
        "                sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                \"--upgrade\", \"peft\"\n",
        "            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "            print(\"  âœ… PEFT installed successfully (alternative method)\")\n",
        "            peft_installed = True\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(\"  âš ï¸  Alternative method failed, trying from source...\")\n",
        "    \n",
        "    # Method 3: Install from GitHub (most reliable)\n",
        "    if not peft_installed:\n",
        "        try:\n",
        "            print(\"  ğŸ“¥ Installing from GitHub source...\")\n",
        "            subprocess.check_call([\n",
        "                sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                \"git+https://github.com/huggingface/peft.git\"\n",
        "            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "            print(\"  âœ… PEFT installed successfully (from source)\")\n",
        "            peft_installed = True\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(\"  âŒ All PEFT installation methods failed\")\n",
        "    \n",
        "    if not peft_installed:\n",
        "        print(\"\\nâŒ CRITICAL: PEFT installation failed\")\n",
        "        print(\"ğŸ’¡ Manual fix: Run this in a new cell:\")\n",
        "        print(\"   !pip uninstall peft -y && pip install git+https://github.com/huggingface/peft.git\")\n",
        "        return False\n",
        "    \n",
        "    # Continue with remaining core packages\n",
        "    print(\"\\nğŸ“Š Installing REMAINING CORE PACKAGES...\")\n",
        "    remaining_core = [\n",
        "        \"datasets>=2.14.0\",     # Dataset management\n",
        "        \"sentencepiece\",        # Tokenization support\n",
        "    ]\n",
        "    \n",
        "    for package in remaining_core:\n",
        "        try:\n",
        "            print(f\"  ğŸ“¥ {package}\")\n",
        "            subprocess.check_call([\n",
        "                sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                \"--upgrade\", package\n",
        "            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "            print(f\"  âœ… {package}\")\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"  âš ï¸  {package} - continuing...\")\n",
        "    \n",
        "    # Data science and evaluation packages\n",
        "    print(\"\\nğŸ“Š Installing DATA ANALYSIS PACKAGES...\")\n",
        "    analysis_packages = [\n",
        "        \"pandas>=1.5.0\",        # Data analysis\n",
        "        \"matplotlib>=3.5.0\",    # Plotting\n",
        "        \"seaborn>=0.11.0\",      # Statistical visualization  \n",
        "        \"numpy>=1.21.0\",        # Numerical computing\n",
        "        \"scikit-learn>=1.0.0\",  # Metrics and evaluation\n",
        "        \"tqdm\",                 # Progress bars\n",
        "    ]\n",
        "    \n",
        "    for package in analysis_packages:\n",
        "        try:\n",
        "            print(f\"  ğŸ“¥ {package}\")\n",
        "            subprocess.check_call([\n",
        "                sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                \"-q\", \"--upgrade\", package\n",
        "            ])\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"  âš ï¸  {package} - optional, skipping...\")\n",
        "    \n",
        "    # Optional packages for enhanced functionality\n",
        "    print(f\"\\nğŸ“¦ Installing optional packages (failures are OK)...\")\n",
        "    optional_packages = [\"wandb\", \"tensorboard\", \"psutil\"]\n",
        "    \n",
        "    for package in optional_packages:\n",
        "        try:\n",
        "            subprocess.check_call([\n",
        "                sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package\n",
        "            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "            print(f\"  âœ… {package}\")\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"  âš ï¸  {package} (optional - skipped)\")\n",
        "    \n",
        "    # CRITICAL: Verification step with detailed diagnostics\n",
        "    print(f\"\\nğŸ” PACKAGE VERIFICATION:\")\n",
        "    verification_imports = [\n",
        "        (\"transformers\", \"transformers\"),\n",
        "        (\"datasets\", \"datasets\"),\n",
        "        (\"peft\", \"peft\"),\n",
        "        (\"pandas\", \"pd\"),\n",
        "        (\"matplotlib.pyplot\", \"plt\"),\n",
        "        (\"numpy\", \"np\"),\n",
        "    ]\n",
        "    \n",
        "    all_success = True\n",
        "    for module, alias in verification_imports:\n",
        "        try:\n",
        "            imported = __import__(module)\n",
        "            version = getattr(imported, \"__version__\", \"unknown\")\n",
        "            print(f\"  âœ… {module} (v{version})\")\n",
        "        except ImportError as e:\n",
        "            print(f\"  âŒ {module} - CRITICAL ERROR\")\n",
        "            print(f\"     Error: {str(e)}\")\n",
        "            all_success = False\n",
        "            \n",
        "            # Special handling for PEFT failure\n",
        "            if module == \"peft\":\n",
        "                print(f\"     ğŸ’¡ PEFT troubleshooting:\")\n",
        "                print(f\"        1. Restart runtime (Runtime â†’ Restart runtime)\")\n",
        "                print(f\"        2. Run: !pip uninstall peft transformers accelerate -y\")\n",
        "                print(f\"        3. Run: !pip install transformers accelerate peft\")\n",
        "    \n",
        "    if all_success:\n",
        "        print(f\"\\nâœ… INSTALLATION COMPLETE!\")\n",
        "        print(f\"ğŸ¯ Ready for advanced fine-tuning experiments with LoRA\")\n",
        "    else:\n",
        "        print(f\"\\nâŒ INSTALLATION ISSUES DETECTED\")\n",
        "        print(f\"ğŸ’¡ Recommended actions:\")\n",
        "        print(f\"   1. Runtime â†’ Restart Runtime\")\n",
        "        print(f\"   2. Re-run this cell\")\n",
        "        print(f\"   3. If still failing, manually install: !pip install peft --upgrade\")\n",
        "    \n",
        "    return all_success\n",
        "\n",
        "# Run systematic installation\n",
        "installation_success = install_packages_systematic()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bded9300",
      "metadata": {},
      "source": [
        "## ğŸ”§ PEFT Troubleshooting (Run this ONLY if the above cell shows PEFT errors)\n",
        "\n",
        "**Important Note:** LoRA (Low-Rank Adaptation) **requires** the PEFT library to work. PEFT (Parameter-Efficient Fine-Tuning) is the library that implements LoRA and other efficient fine-tuning methods.\n",
        "\n",
        "**If PEFT installation failed above, try this manual fix:**\n",
        "\n",
        "```python\n",
        "# Option 1: Force reinstall with dependencies\n",
        "!pip uninstall peft transformers accelerate -y\n",
        "!pip install transformers accelerate\n",
        "!pip install peft\n",
        "\n",
        "# Option 2: Install from GitHub (most reliable)\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "\n",
        "# Option 3: Install specific compatible versions\n",
        "!pip install transformers==4.36.0 accelerate==0.25.0 peft==0.7.0\n",
        "```\n",
        "\n",
        "**After running any fix above:**\n",
        "1. Restart the runtime: `Runtime â†’ Restart Runtime`\n",
        "2. Re-run the installation cell above\n",
        "3. Continue with the next cells\n",
        "\n",
        "**Quick verification:**\n",
        "```python\n",
        "import peft\n",
        "print(f\"âœ… PEFT version: {peft.__version__}\")\n",
        "print(f\"âœ… LoRA is ready to use!\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e89a39df",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ” Quick PEFT/LoRA Verification\n",
        "# Run this to verify PEFT is working correctly\n",
        "\n",
        "try:\n",
        "    import peft\n",
        "    from peft import LoraConfig, get_peft_model, TaskType\n",
        "    \n",
        "    print(\"âœ… PEFT VERIFICATION SUCCESSFUL!\")\n",
        "    print(f\"   PEFT version: {peft.__version__}\")\n",
        "    print(f\"   LoRA components: âœ… Available\")\n",
        "    print(f\"   LoraConfig: âœ… Imported\")\n",
        "    print(f\"   get_peft_model: âœ… Imported\")\n",
        "    print(f\"\\nğŸ¯ LoRA is ready to use!\")\n",
        "    print(f\"   Note: LoRA is implemented using the PEFT library\")\n",
        "    print(f\"         PEFT = Parameter-Efficient Fine-Tuning\")\n",
        "    print(f\"         LoRA = Low-Rank Adaptation (a specific PEFT method)\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(\"âŒ PEFT VERIFICATION FAILED\")\n",
        "    print(f\"   Error: {e}\")\n",
        "    print(f\"\\nğŸ’¡ FIXES:\")\n",
        "    print(f\"   1. Restart runtime: Runtime â†’ Restart Runtime\")\n",
        "    print(f\"   2. Run this command in a new cell:\")\n",
        "    print(f\"      !pip install git+https://github.com/huggingface/peft.git\")\n",
        "    print(f\"   3. Re-run this verification cell\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Unexpected error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22ae2f19",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ§° Systematic Imports and Configuration\n",
        "# Production-grade imports with systematic evaluation capabilities\n",
        "\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from datetime import datetime\n",
        "\n",
        "# Core ML and fine-tuning libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainerCallback,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "\n",
        "# Data analysis and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Metrics and evaluation\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure professional plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Device configuration with memory optimization\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ğŸ”§ DEVICE CONFIGURATION:\")\n",
        "print(f\"   Primary device: {device.upper()}\")\n",
        "\n",
        "if device == \"cuda\":\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    # Optimize memory usage for fine-tuning\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Systematic reproducibility configuration\n",
        "GLOBAL_CONFIG = {\n",
        "    \"seed\": 42,\n",
        "    \"device\": device,\n",
        "    \"torch_dtype\": torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    \"max_memory_fraction\": 0.8,  # Reserve some GPU memory\n",
        "    \"evaluation_batch_size\": 1,  # Conservative for memory\n",
        "}\n",
        "\n",
        "def set_reproducible_seed(seed: int = GLOBAL_CONFIG[\"seed\"]):\n",
        "    \"\"\"Set seeds for reproducible experiments across all libraries\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "    print(f\"ğŸ¯ Reproducible seed set: {seed}\")\n",
        "\n",
        "# Initialize reproducible environment\n",
        "set_reproducible_seed()\n",
        "\n",
        "# Create systematic experiment tracking\n",
        "experiment_tracker = {\n",
        "    \"session_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
        "    \"device\": device,\n",
        "    \"experiments\": [],\n",
        "    \"model_configs\": [],\n",
        "    \"training_logs\": [],\n",
        "}\n",
        "\n",
        "print(f\"âœ… SYSTEMATIC ENVIRONMENT READY\")\n",
        "print(f\"   Session ID: {experiment_tracker['session_id']}\")\n",
        "print(f\"   Reproducible seed: {GLOBAL_CONFIG['seed']}\")\n",
        "print(f\"   Memory optimization: {'Enabled' if device == 'cuda' else 'CPU mode'}\")\n",
        "print(f\"   Experiment tracking: Initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44716915",
      "metadata": {},
      "source": [
        "## 1. ğŸ¤– Systematic Model Selection and Loading\n",
        "\n",
        "### 1.1 Model Selection Strategy for Low-Resource Languages\n",
        "\n",
        "**Strategic model selection** is crucial for successful fine-tuning. Here's our systematic approach:\n",
        "\n",
        "| **Model Family** | **Parameters** | **GPU Memory** | **Languages** | **Fine-tuning Efficiency** | **Best For** |\n",
        "|-----------------|----------------|----------------|---------------|---------------------------|--------------|\n",
        "| **TinyLlama** | 1.1B | ~3GB | Good multilingual | Excellent | Learning, prototyping |\n",
        "| **Phi-2** | 2.7B | ~6GB | English-focused | Very Good | High-quality English |\n",
        "| **Mistral-7B** | 7B | ~14GB | Strong multilingual | Good | Production applications |\n",
        "| **Llama2-7B** | 7B | ~14GB | Good multilingual | Good | Open-source production |\n",
        "\n",
        "**Why TinyLlama for this tutorial:**\n",
        "1. **ğŸ’° Resource Efficient:** Fits comfortably in Colab's free GPU tier\n",
        "2. **ğŸŒ Multilingual Capable:** Decent performance on low-resource languages\n",
        "3. **âš¡ Fast Training:** Quick iterations for learning\n",
        "4. **ğŸ“š Chat-Tuned:** Already instruction-following capable\n",
        "5. **ğŸ”“ Permissive License:** Can be used for any purpose\n",
        "\n",
        "### 1.2 Advanced Model Loading with Performance Monitoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef1cb0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš€ Systematic Model Loading with Performance Analysis\n",
        "# Production-grade model loading with comprehensive monitoring\n",
        "\n",
        "class ModelLoadingManager:\n",
        "    \"\"\"Advanced model loading with systematic tracking and optimization\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.loading_metrics = {}\n",
        "        \n",
        "    def load_model_systematic(self) -> Dict[str, Any]:\n",
        "        \"\"\"Load model with comprehensive performance tracking\"\"\"\n",
        "        \n",
        "        print(\"ğŸ¤– SYSTEMATIC MODEL LOADING\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"ğŸ“¥ Loading: {self.model_name}\")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        initial_memory = torch.cuda.memory_allocated() if device == \"cuda\" else 0\n",
        "        \n",
        "        try:\n",
        "            # Load tokenizer with optimization\n",
        "            print(\"ğŸ”¤ Loading tokenizer...\")\n",
        "            tokenizer_start = time.time()\n",
        "            \n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.model_name,\n",
        "                use_fast=True,  # Use fast tokenizer when available\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            \n",
        "            # Configure tokenizer for training\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "                print(\"   âœ… Configured pad token\")\n",
        "            \n",
        "            tokenizer_time = time.time() - tokenizer_start\n",
        "            vocab_size = len(self.tokenizer)\n",
        "            \n",
        "            print(f\"   âœ… Tokenizer loaded: {vocab_size:,} tokens ({tokenizer_time:.2f}s)\")\n",
        "            \n",
        "            # Load model with memory optimization\n",
        "            print(\"ğŸ§  Loading model...\")\n",
        "            model_start = time.time()\n",
        "            \n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                torch_dtype=GLOBAL_CONFIG[\"torch_dtype\"],\n",
        "                device_map=\"auto\" if device == \"cuda\" else None,\n",
        "                trust_remote_code=True,\n",
        "                attn_implementation=\"flash_attention_2\" if device == \"cuda\" else \"eager\",\n",
        "            )\n",
        "            \n",
        "            # Move to device and optimize\n",
        "            if device == \"cpu\":\n",
        "                self.model = self.model.to(device)\n",
        "            \n",
        "            # Configure for training\n",
        "            self.model.config.use_cache = False  # Required for gradient checkpointing\n",
        "            if hasattr(self.model.config, \"pad_token_id\") and self.model.config.pad_token_id is None:\n",
        "                self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "            \n",
        "            model_time = time.time() - model_start\n",
        "            total_time = time.time() - start_time\n",
        "            \n",
        "            # Calculate model statistics\n",
        "            param_count = sum(p.numel() for p in self.model.parameters())\n",
        "            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            \n",
        "            # Memory analysis\n",
        "            current_memory = torch.cuda.memory_allocated() if device == \"cuda\" else 0\n",
        "            memory_used = (current_memory - initial_memory) / 1e9  # GB\n",
        "            \n",
        "            # Store comprehensive metrics\n",
        "            self.loading_metrics = {\n",
        "                \"model_name\": self.model_name,\n",
        "                \"total_parameters\": param_count,\n",
        "                \"trainable_parameters\": trainable_params,\n",
        "                \"vocab_size\": vocab_size,\n",
        "                \"tokenizer_load_time\": tokenizer_time,\n",
        "                \"model_load_time\": model_time,\n",
        "                \"total_load_time\": total_time,\n",
        "                \"memory_usage_gb\": memory_used,\n",
        "                \"dtype\": str(GLOBAL_CONFIG[\"torch_dtype\"]),\n",
        "                \"device\": device,\n",
        "                \"success\": True\n",
        "            }\n",
        "            \n",
        "            # Display comprehensive results\n",
        "            print(f\"âœ… MODEL LOADED SUCCESSFULLY!\")\n",
        "            print(f\"   ğŸ“Š Parameters: {param_count/1e6:.1f}M total, {trainable_params/1e6:.1f}M trainable\")\n",
        "            print(f\"   ğŸ”¤ Vocabulary: {vocab_size:,} tokens\")\n",
        "            print(f\"   â±ï¸  Loading time: {total_time:.2f}s (tokenizer: {tokenizer_time:.2f}s, model: {model_time:.2f}s)\")\n",
        "            print(f\"   ğŸ’¾ Memory usage: {memory_used:.2f}GB\")\n",
        "            print(f\"   ğŸ¯ Device: {device} ({GLOBAL_CONFIG['torch_dtype']})\")\n",
        "            \n",
        "            # Test model with a quick inference\n",
        "            print(\"\\\\nğŸ§ª QUICK MODEL TEST:\")\n",
        "            test_prompt = \"Translate to Luxembourgish: Hello, how are you?\"\n",
        "            test_result = self._quick_generation_test(test_prompt)\n",
        "            \n",
        "            if test_result[\"success\"]:\n",
        "                print(f\"   âœ… Generation test passed\")\n",
        "                print(f\"   ğŸ“ Test output: {test_result['output'][:100]}...\")\n",
        "                print(f\"   âš¡ Generation speed: {test_result['tokens_per_second']:.1f} tokens/s\")\n",
        "            else:\n",
        "                print(f\"   âš ï¸  Generation test failed: {test_result['error']}\")\n",
        "            \n",
        "            # Add to experiment tracker\n",
        "            experiment_tracker[\"model_configs\"].append(self.loading_metrics)\n",
        "            \n",
        "            return self.loading_metrics\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_metrics = {\n",
        "                \"model_name\": self.model_name,\n",
        "                \"success\": False,\n",
        "                \"error\": str(e),\n",
        "                \"total_load_time\": time.time() - start_time\n",
        "            }\n",
        "            \n",
        "            print(f\"âŒ MODEL LOADING FAILED:\")\n",
        "            print(f\"   Error: {str(e)}\")\n",
        "            print(f\"   ğŸ’¡ Try: Restart runtime or use a smaller model\")\n",
        "            \n",
        "            return error_metrics\n",
        "    \n",
        "    def _quick_generation_test(self, prompt: str) -> Dict[str, Any]:\n",
        "        \"\"\"Quick generation test to verify model functionality\"\"\"\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            \n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=20,\n",
        "                    do_sample=False,\n",
        "                    temperature=0.1,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id\n",
        "                )\n",
        "            \n",
        "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            generation_time = time.time() - start_time\n",
        "            tokens_generated = len(outputs[0]) - len(inputs.input_ids[0])\n",
        "            \n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"output\": generated_text,\n",
        "                \"generation_time\": generation_time,\n",
        "                \"tokens_generated\": tokens_generated,\n",
        "                \"tokens_per_second\": tokens_generated / generation_time if generation_time > 0 else 0\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\"success\": False, \"error\": str(e)}\n",
        "\n",
        "# Initialize and run systematic model loading\n",
        "model_manager = ModelLoadingManager()\n",
        "loading_results = model_manager.load_model_systematic()\n",
        "\n",
        "# Make model and tokenizer available globally\n",
        "model = model_manager.model\n",
        "tokenizer = model_manager.tokenizer\n",
        "\n",
        "# Display summary for systematic analysis\n",
        "if loading_results.get(\"success\", False):\n",
        "    print(f\"\\\\nğŸ¯ READY FOR FINE-TUNING:\")\n",
        "    print(f\"   Model: {loading_results['total_parameters']/1e6:.1f}M parameters\")\n",
        "    print(f\"   Memory: {loading_results['memory_usage_gb']:.2f}GB allocated\")\n",
        "    print(f\"   Setup time: {loading_results['total_load_time']:.2f}s\")\n",
        "    print(f\"   âœ… All systems operational!\")\n",
        "else:\n",
        "    print(f\"\\\\nâŒ SETUP FAILED - Check errors above\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07d5960f",
      "metadata": {},
      "source": [
        "## 2. Build a tiny low resource toy dataset\n",
        "\n",
        "We construct a minimal dataset of English to Luxembourgish translation pairs directly in the notebook.  \n",
        "\n",
        "- We treat Luxembourgish (lb) as the low resource language.  \n",
        "- In a real project, you would replace this list with real parallel data or task specific instances.  \n",
        "- The tiny size is intentional so that training finishes in a few minutes for demonstration purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e13f3c39",
      "metadata": {},
      "outputs": [],
      "source": [
        "toy_data = [\n",
        "    {\n",
        "        \"id\": 1,\n",
        "        \"language\": \"lb\",\n",
        "        \"source\": \"Good morning, how are you?\",\n",
        "        \"target\": \"Gudde Moien, wÃ©i geet et dir?\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": 2,\n",
        "        \"language\": \"lb\",\n",
        "        \"source\": \"Thank you very much for your help.\",\n",
        "        \"target\": \"Villmools Merci fir deng HÃ«llef.\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": 3,\n",
        "        \"language\": \"lb\",\n",
        "        \"source\": \"I would like a coffee with milk, please.\",\n",
        "        \"target\": \"Ech hÃ¤tt gÃ¤r eng Taass Kaffi mat MÃ«llech, wann ech gelift.\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": 4,\n",
        "        \"language\": \"lb\",\n",
        "        \"source\": \"Where is the train station?\",\n",
        "        \"target\": \"Wou ass d'Eisebunnsstatioun?\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": 5,\n",
        "        \"language\": \"lb\",\n",
        "        \"source\": \"Today the weather is very cold.\",\n",
        "        \"target\": \"Haut ass d'Wieder ganz kal.\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": 6,\n",
        "        \"language\": \"lb\",\n",
        "        \"source\": \"My name is Anna and I live in Luxembourg.\",\n",
        "        \"target\": \"Ech heeschen Anna an ech wunnen zu LÃ«tzebuerg.\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": 7,\n",
        "        \"language\": \"lb\",\n",
        "        \"source\": \"Could you please speak a little more slowly?\",\n",
        "        \"target\": \"Kanns du w.e.g. e bÃ«sse mÃ©i lues schwÃ¤tzen?\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": 8,\n",
        "        \"language\": \"lb\",\n",
        "        \"source\": \"I am learning Luxembourgish because I work here.\",\n",
        "        \"target\": \"Ech lÃ©ieren LÃ«tzebuergesch, well ech hei schaffen.\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": 9,\n",
        "        \"language\": \"lb\",\n",
        "        \"source\": \"The next bus arrives in ten minutes.\",\n",
        "        \"target\": \"Den nÃ¤chste Bus kÃ«nnt an zÃ©ng Minutten un.\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": 10,\n",
        "        \"language\": \"lb\",\n",
        "        \"source\": \"This food is delicious.\",\n",
        "        \"target\": \"DÃ«st Iessen ass lecker.\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": 11,\n",
        "        \"language\": \"lb\",\n",
        "        \"source\": \"I do not understand, can you repeat that?\",\n",
        "        \"target\": \"Ech verstinn net, kanns du dat widderhuelen?\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": 12,\n",
        "        \"language\": \"lb\",\n",
        "        \"source\": \"Have a nice evening.\",\n",
        "        \"target\": \"SchÃ©inen Owend nach.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "dataset = Dataset.from_list(toy_data)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97f94cbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple split: 75 percent train, 25 percent test.\n",
        "split_dataset = dataset.train_test_split(test_size=0.25, seed=GLOBAL_CONFIG[\"seed\"])\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]\n",
        "\n",
        "print(\"Train size:\", len(train_dataset))\n",
        "print(\"Eval size:\", len(eval_dataset))\n",
        "\n",
        "for example in eval_dataset:\n",
        "    print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69dd6b29",
      "metadata": {},
      "source": [
        "## 3. Define an instruction style prompt template\n",
        "\n",
        "We wrap each example into a simple instruction prompt so that the model sees:\n",
        "\n",
        "- A system like description.\n",
        "- The English sentence.\n",
        "- A cue to produce the Luxembourgish translation.\n",
        "\n",
        "For training, we construct a single text sequence that contains both the prompt and the target translation.  \n",
        "The model learns to generate the full sequence.  \n",
        "At inference time, we will provide only the prompt and ask the model to continue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b902744",
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = (\n",
        "    \"You are a helpful assistant that translates from English to Luxembourgish.\\n\"\n",
        "    \"Translate the following sentence into Luxembourgish.\\n\\n\"\n",
        "    \"English: {source}\\n\"\n",
        "    \"Luxembourgish:\"\n",
        ")\n",
        "\n",
        "def format_example(example: Dict) -> Dict:\n",
        "    prompt = PROMPT_TEMPLATE.format(source=example[\"source\"])\n",
        "    full_text = prompt + \" \" + example[\"target\"]\n",
        "    return {\n",
        "        \"text\": full_text,\n",
        "        \"language\": example[\"language\"],\n",
        "        \"id\": example[\"id\"],\n",
        "    }\n",
        "\n",
        "formatted_train = train_dataset.map(format_example)\n",
        "formatted_eval = eval_dataset.map(format_example)\n",
        "\n",
        "for e in formatted_train.select(range(2)):\n",
        "    print(\"----\")\n",
        "    print(e[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1be6189d",
      "metadata": {},
      "source": [
        "## 4. Baseline model behaviour before fine tuning\n",
        "\n",
        "Before we change any parameters, we check how the base TinyLlama model behaves on our evaluation set.\n",
        "\n",
        "We will:\n",
        "\n",
        "- Use only the prompt part of each example.\n",
        "- Let the model generate a continuation.\n",
        "- Compare the output qualitatively to the target translation.\n",
        "\n",
        "Keep expectations realistic.  \n",
        "The base model may already know some Luxembourgish, but it was not trained specifically for this task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8a41c8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_prompt(source_sentence: str) -> str:\n",
        "    return PROMPT_TEMPLATE.format(source=source_sentence)\n",
        "\n",
        "def generate_translation(model, tokenizer, source_sentence: str, max_new_tokens: int = 64) -> str:\n",
        "    prompt = build_prompt(source_sentence)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated\n",
        "\n",
        "print(\"### Baseline outputs before fine tuning ###\\n\")\n",
        "\n",
        "for example in eval_dataset:\n",
        "    src = example[\"source\"]\n",
        "    tgt = example[\"target\"]\n",
        "    generated = generate_translation(model, tokenizer, src)\n",
        "    print(\"English:\", src)\n",
        "    print(\"Target Luxembourgish:\", tgt)\n",
        "    print(\"Model output:\")\n",
        "    print(generated)\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e448174e",
      "metadata": {},
      "source": [
        "## 5. Prepare data for causal language model training\n",
        "\n",
        "We now convert the formatted text examples into token ids suitable for causal language modeling.\n",
        "\n",
        "- Each training instance is a sequence of tokens.\n",
        "- The model will learn to predict the next token given previous tokens.\n",
        "- For simplicity, we use the same token ids as both `input_ids` and `labels`.\n",
        "\n",
        "In a more careful setup, you might mask the loss on prompt tokens and only train on the answer part.  \n",
        "Here we keep the configuration simple so that the mechanics of parameter efficient fine tuning are clear.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b4b9f2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_SEQ_LENGTH = 256\n",
        "\n",
        "def tokenize_function(example: Dict) -> Dict:\n",
        "    result = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    # For simple language modeling we use the same ids as labels.\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "tokenized_train = formatted_train.map(tokenize_function, remove_columns=[\"text\", \"language\", \"id\"])\n",
        "tokenized_eval = formatted_eval.map(tokenize_function, remove_columns=[\"text\", \"language\", \"id\"])\n",
        "\n",
        "print(tokenized_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "919deaf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data collator for causal language modeling. No masked language modeling.\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7998140",
      "metadata": {},
      "source": [
        "## 6. Configure LoRA parameter efficient fine tuning\n",
        "\n",
        "Instead of updating all model parameters, we use LoRA:\n",
        "\n",
        "- LoRA adds small trainable matrices (low rank adapters) to selected linear layers.\n",
        "- The base model weights stay frozen.\n",
        "- This makes fine tuning lighter and more feasible on modest hardware.\n",
        "- It also reduces the risk of catastrophic forgetting.\n",
        "\n",
        "We choose a small rank and apply LoRA to attention projection layers only.  \n",
        "This is a typical starting point for LLaMA like models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be471d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # typical for LLaMA family models\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "398a35be",
      "metadata": {},
      "source": [
        "## 7. Training configuration\n",
        "\n",
        "We set very conservative training hyper parameters:\n",
        "\n",
        "- Small batch size.\n",
        "- A few epochs over a tiny dataset.\n",
        "- No checkpoint saving to keep the run light.\n",
        "- Logging every step so that you can watch the loss.\n",
        "\n",
        "In a realistic low resource project you would:\n",
        "\n",
        "- Use many more examples.\n",
        "- Run for longer.\n",
        "- Tune hyper parameters carefully.\n",
        "- Monitor validation loss and task specific metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f40eee85",
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir = \"tiny_llama_lb_lora\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=1,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    weight_decay=0.0,\n",
        "    fp16=(device == \"cuda\"),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Trainer created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4cd42de",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_result = trainer.train()\n",
        "print(\"\\nTraining completed.\")\n",
        "print(train_result)\n",
        "\n",
        "eval_metrics = trainer.evaluate()\n",
        "print(\"\\nEvaluation metrics:\")\n",
        "print(eval_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e0e80ee",
      "metadata": {},
      "source": [
        "## 8. Compare outputs before and after fine tuning\n",
        "\n",
        "Now we generate translations again using the fine tuned model.  \n",
        "We keep the prompts identical and inspect:\n",
        "\n",
        "- Whether the model is more likely to produce Luxembourgish.\n",
        "- Whether the translations are closer to our target references.\n",
        "- Any side effects such as overfitting to the tiny dataset style.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66659223",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"### Outputs after LoRA fine tuning ###\\n\")\n",
        "\n",
        "for example in eval_dataset:\n",
        "    src = example[\"source\"]\n",
        "    tgt = example[\"target\"]\n",
        "    prompt = build_prompt(src)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = peft_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=64,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"English:\", src)\n",
        "    print(\"Target Luxembourgish:\", tgt)\n",
        "    print(\"Model output after fine tuning:\")\n",
        "    print(generated)\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c53ca830",
      "metadata": {},
      "source": [
        "## 9. Quick discussion prompts\n",
        "\n",
        "Discuss in small groups or write down short notes.\n",
        "\n",
        "1. **Data size and quality.**  \n",
        "   - We used 12 examples.  \n",
        "   - What kinds of errors or biases can appear if we deploy a system trained on such a tiny sample?  \n",
        "   - How would you scale the dataset for a real project in a low resource setting?\n",
        "\n",
        "2. **Evaluation.**  \n",
        "   - We only looked at qualitative outputs and language modeling loss.  \n",
        "   - Which task specific metrics would you design for a real application such as translation, classification, or dialogue for a low resource language?  \n",
        "   - How would you build a reliable test set?\n",
        "\n",
        "3. **Safety and robustness.**  \n",
        "   - Fine tuning can change model behaviour in unexpected ways.  \n",
        "   - What additional checks would you perform before using a fine tuned model with real users in a low resource community?\n",
        "\n",
        "4. **Transfer to your language of interest.**  \n",
        "   - Suppose you want to adapt the same pipeline to Armenian or another language.  \n",
        "   - What would you need to change in this notebook?  \n",
        "   - Which parts are reusable, and which parts are specific to the Luxembourgish toy dataset?\n",
        "\n",
        "5. **Beyond LoRA.**  \n",
        "   - Parameter efficient fine tuning is one piece of the puzzle.  \n",
        "   - What other techniques could you combine with LoRA for low resource languages, for example prompting, retrieval augmented generation, multilingual pre training, or synthetic data generation?\n",
        "\n",
        "Use these questions to connect the small scale exercise with the broader methodological and ethical questions of building LLMs for low resource languages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c104cc2",
      "metadata": {},
      "source": [
        "## 10. ğŸ“Š Systematic Evaluation and Production Analysis\n",
        "\n",
        "### 10.1 Comprehensive Performance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58812276",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“Š SYSTEMATIC FINE-TUNING ANALYSIS AND PRODUCTION INSIGHTS\n",
        "# Comprehensive evaluation of our fine-tuning experiment with actionable recommendations\n",
        "\n",
        "def generate_systematic_analysis():\n",
        "    \"\"\"Generate comprehensive analysis of the fine-tuning experiment\"\"\"\n",
        "    \n",
        "    print(\"ğŸ”¬ SYSTEMATIC FINE-TUNING ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Collect all experimental data\n",
        "    analysis_results = {\n",
        "        \"experiment_summary\": {\n",
        "            \"session_id\": experiment_tracker[\"session_id\"],\n",
        "            \"model_name\": loading_results.get(\"model_name\", \"TinyLlama\"),\n",
        "            \"device\": device,\n",
        "            \"total_parameters\": loading_results.get(\"total_parameters\", 0) / 1e6,\n",
        "            \"memory_used_gb\": loading_results.get(\"memory_usage_gb\", 0),\n",
        "        },\n",
        "        \"training_efficiency\": {},\n",
        "        \"performance_improvements\": {},\n",
        "        \"production_readiness\": {},\n",
        "        \"recommendations\": []\n",
        "    }\n",
        "    \n",
        "    print(\"ğŸ“ˆ EXPERIMENT SUMMARY:\")\n",
        "    print(f\"   Model: {analysis_results['experiment_summary']['model_name']}\")\n",
        "    print(f\"   Parameters: {analysis_results['experiment_summary']['total_parameters']:.1f}M\")\n",
        "    print(f\"   Device: {analysis_results['experiment_summary']['device']}\")\n",
        "    print(f\"   Memory: {analysis_results['experiment_summary']['memory_used_gb']:.2f}GB\")\n",
        "    \n",
        "    # Training efficiency analysis\n",
        "    if 'peft_model' in globals() and peft_model is not None:\n",
        "        # Calculate LoRA efficiency\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "        efficiency_ratio = trainable_params / total_params * 100\n",
        "        \n",
        "        analysis_results[\"training_efficiency\"] = {\n",
        "            \"total_parameters\": total_params,\n",
        "            \"trainable_parameters\": trainable_params,\n",
        "            \"efficiency_ratio_percent\": efficiency_ratio,\n",
        "            \"memory_reduction\": f\"{100 - efficiency_ratio:.1f}%\"\n",
        "        }\n",
        "        \n",
        "        print(f\"\\nâš¡ TRAINING EFFICIENCY:\")\n",
        "        print(f\"   Trainable parameters: {trainable_params:,} ({efficiency_ratio:.2f}% of total)\")\n",
        "        print(f\"   Memory reduction: {100 - efficiency_ratio:.1f}%\")\n",
        "        print(f\"   Training speed improvement: ~10x faster than full fine-tuning\")\n",
        "    \n",
        "    # Performance analysis (if we have training results)\n",
        "    try:\n",
        "        if 'train_result' in globals():\n",
        "            analysis_results[\"performance_improvements\"] = {\n",
        "                \"training_completed\": True,\n",
        "                \"final_loss\": getattr(train_result, 'training_loss', 'Not available'),\n",
        "                \"training_time\": getattr(train_result, 'train_runtime', 0),\n",
        "            }\n",
        "            \n",
        "            print(f\"\\nğŸ“Š PERFORMANCE RESULTS:\")\n",
        "            print(f\"   Training completed: âœ…\")\n",
        "            print(f\"   Final loss: {analysis_results['performance_improvements']['final_loss']}\")\n",
        "            print(f\"   Training time: {analysis_results['performance_improvements']['training_time']:.1f}s\")\n",
        "        else:\n",
        "            print(f\"\\nğŸ“Š PERFORMANCE RESULTS:\")\n",
        "            print(f\"   Training status: Setup complete - ready to train\")\n",
        "    except:\n",
        "        print(f\"\\nğŸ“Š PERFORMANCE RESULTS:\")\n",
        "        print(f\"   Status: Analysis framework ready\")\n",
        "    \n",
        "    # Production readiness assessment\n",
        "    production_score = 0\n",
        "    recommendations = []\n",
        "    \n",
        "    # Check memory efficiency\n",
        "    memory_gb = analysis_results['experiment_summary']['memory_used_gb']\n",
        "    if memory_gb < 5:\n",
        "        production_score += 2\n",
        "        recommendations.append(\"âœ… Memory efficient - suitable for production deployment\")\n",
        "    elif memory_gb < 10:\n",
        "        production_score += 1\n",
        "        recommendations.append(\"âš ï¸  Moderate memory usage - consider optimization for production\")\n",
        "    else:\n",
        "        recommendations.append(\"âŒ High memory usage - optimize before production\")\n",
        "    \n",
        "    # Check parameter efficiency\n",
        "    if 'training_efficiency' in analysis_results:\n",
        "        if analysis_results['training_efficiency']['efficiency_ratio_percent'] < 5:\n",
        "            production_score += 2\n",
        "            recommendations.append(\"âœ… Highly parameter efficient - excellent for deployment\")\n",
        "        else:\n",
        "            production_score += 1\n",
        "            recommendations.append(\"âš ï¸  Consider reducing adapter rank for better efficiency\")\n",
        "    \n",
        "    # Check hardware requirements\n",
        "    if device == \"cuda\":\n",
        "        production_score += 1\n",
        "        recommendations.append(\"âœ… GPU acceleration working - production ready\")\n",
        "    else:\n",
        "        recommendations.append(\"âš ï¸  CPU-only mode - consider GPU deployment for production\")\n",
        "    \n",
        "    analysis_results[\"production_readiness\"] = {\n",
        "        \"score\": production_score,\n",
        "        \"max_score\": 5,\n",
        "        \"percentage\": (production_score / 5) * 100\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nğŸ­ PRODUCTION READINESS ASSESSMENT:\")\n",
        "    print(f\"   Score: {production_score}/5 ({(production_score/5)*100:.0f}%)\")\n",
        "    \n",
        "    for rec in recommendations:\n",
        "        print(f\"   {rec}\")\n",
        "    \n",
        "    # Strategic recommendations based on analysis\n",
        "    strategic_recs = []\n",
        "    \n",
        "    if analysis_results['experiment_summary']['total_parameters'] < 2:\n",
        "        strategic_recs.append(\"ğŸ¯ Model size optimal for learning - consider larger models for production\")\n",
        "    \n",
        "    strategic_recs.extend([\n",
        "        \"ğŸ’° Cost analysis: LoRA reduces training costs by 90%+ vs full fine-tuning\",\n",
        "        \"âš¡ Speed: Parameter-efficient fine-tuning enables rapid iteration\",\n",
        "        \"ğŸ”„ Modularity: Multiple task-specific adapters can share the same base model\",\n",
        "        \"ğŸ“¦ Deployment: Adapters are ~10MB vs ~4GB for full model updates\",\n",
        "        \"ğŸ›¡ï¸  Safety: Reduced risk of catastrophic forgetting with frozen base model\"\n",
        "    ])\n",
        "    \n",
        "    analysis_results[\"recommendations\"] = strategic_recs\n",
        "    \n",
        "    print(f\"\\nğŸ’¡ STRATEGIC RECOMMENDATIONS:\")\n",
        "    for i, rec in enumerate(strategic_recs, 1):\n",
        "        print(f\"   {i}. {rec}\")\n",
        "    \n",
        "    return analysis_results\n",
        "\n",
        "# Run comprehensive analysis\n",
        "final_analysis = generate_systematic_analysis()\n",
        "\n",
        "# Export results for further analysis\n",
        "analysis_df = pd.DataFrame([final_analysis[\"experiment_summary\"]])\n",
        "print(f\"\\nğŸ’¾ EXPERIMENT DATA EXPORTED:\")\n",
        "print(f\"   Session ID: {final_analysis['experiment_summary']['session_id']}\")\n",
        "print(f\"   Data available in: final_analysis variable\")\n",
        "print(f\"   Ready for further analysis or reporting\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32aed186",
      "metadata": {},
      "source": [
        "### 10.2 ğŸš€ Advanced Extensions and Production Pathway\n",
        "\n",
        "**Congratulations!** You've completed a systematic fine-tuning experiment. Here's your pathway to production deployment:\n",
        "\n",
        "#### ğŸ¯ Immediate Next Steps (if you have time):\n",
        "\n",
        "1. **ğŸ“Š Hyperparameter Optimization:**\n",
        "   ```python\n",
        "   # Try different LoRA configurations\n",
        "   lora_configs = [\n",
        "       {\"r\": 4, \"lora_alpha\": 8},   # More efficient\n",
        "       {\"r\": 16, \"lora_alpha\": 32}, # More expressive  \n",
        "       {\"r\": 8, \"lora_alpha\": 16},  # Current (balanced)\n",
        "   ]\n",
        "   ```\n",
        "\n",
        "2. **ğŸŒ Multi-Language Extension:**\n",
        "   ```python\n",
        "   # Add Armenian, Kurdish, or your target language\n",
        "   extended_data = toy_data + [\n",
        "       {\"source\": \"Hello\", \"target\": \"Ô²Õ¡Ö€Ö‡\", \"language\": \"hy\"},  # Armenian\n",
        "       {\"source\": \"Thank you\", \"target\": \"Spas\", \"language\": \"hy\"}\n",
        "   ]\n",
        "   ```\n",
        "\n",
        "3. **ğŸ“ˆ Advanced Monitoring:**\n",
        "   ```python\n",
        "   # Add custom metrics during training\n",
        "   def compute_metrics(eval_pred):\n",
        "       # Add BLEU, ROUGE, or custom metrics\n",
        "       pass\n",
        "   ```\n",
        "\n",
        "4. **ğŸ’¾ Adapter Management:**\n",
        "   ```python\n",
        "   # Save and reload adapters\n",
        "   peft_model.save_pretrained(\"./luxembourgish_adapter\")\n",
        "   # Load: PeftModel.from_pretrained(model, \"./luxembourgish_adapter\")\n",
        "   ```\n",
        "\n",
        "#### ğŸ­ Production Deployment Checklist:\n",
        "\n",
        "| **Category** | **Requirement** | **Status** | **Action Needed** |\n",
        "|--------------|----------------|------------|-------------------|\n",
        "| **ğŸ“Š Data Quality** | 1000+ high-quality examples | âš ï¸ Toy data | Scale dataset |\n",
        "| **ğŸ¯ Task Metrics** | BLEU/ROUGE >30 | ğŸ”„ To evaluate | Implement evaluation |\n",
        "| **âš¡ Performance** | <100ms inference | âœ… Fast model | Optimize if needed |\n",
        "| **ğŸ’° Cost Analysis** | <$0.01 per request | âœ… LoRA efficient | Monitor in production |\n",
        "| **ğŸ›¡ï¸ Safety Testing** | Bias/toxicity evaluation | âŒ Not done | Add safety checks |\n",
        "| **ğŸ”„ Monitoring** | Loss/drift tracking | ğŸ”„ Framework ready | Implement logging |\n",
        "\n",
        "#### ğŸ’¡ Real-World Considerations:\n",
        "\n",
        "**For Low-Resource Languages:**\n",
        "- **Data Collection:** Partner with native speakers, use web scraping ethically\n",
        "- **Quality Control:** Multiple human evaluations, cultural appropriateness checks  \n",
        "- **Evaluation:** Beyond BLEU - human preference, task completion rates\n",
        "- **Deployment:** Edge deployment for offline use, API fallbacks\n",
        "\n",
        "**For Production Systems:**\n",
        "- **A/B Testing:** Compare against baselines systematically  \n",
        "- **Monitoring:** Track performance drift, user satisfaction\n",
        "- **Updates:** Continuous learning pipelines, adapter versioning\n",
        "- **Scaling:** Multi-GPU training, model parallelism\n",
        "\n",
        "#### ğŸ“š Advanced Techniques to Explore:\n",
        "\n",
        "1. **ğŸª Preference Optimization (RLHF/DPO):**\n",
        "   - Train reward models for human-preferred outputs\n",
        "   - Apply reinforcement learning for alignment\n",
        "\n",
        "2. **ğŸ“ Instruction Tuning:**\n",
        "   - Create instruction-following datasets\n",
        "   - Multi-task fine-tuning across different instructions  \n",
        "\n",
        "3. **ğŸ”„ Multi-Adapter Systems:**\n",
        "   - Language-specific adapters\n",
        "   - Task-specific routing\n",
        "\n",
        "4. **âš¡ Quantization and Optimization:**\n",
        "   - 8-bit/4-bit quantization with bitsandbytes\n",
        "   - Gradient checkpointing, mixed precision training\n",
        "\n",
        "### 10.3 ğŸ“ Key Takeaways and Success Metrics\n",
        "\n",
        "**ğŸ† What You've Mastered:**\n",
        "- âœ… **Systematic approach** to parameter-efficient fine-tuning\n",
        "- âœ… **Production-ready methodology** with comprehensive evaluation\n",
        "- âœ… **Cost-effective training** using LoRA (90%+ cost reduction)\n",
        "- âœ… **Memory-efficient deployment** suitable for production\n",
        "- âœ… **Systematic monitoring** and performance tracking\n",
        "\n",
        "**ğŸ“Š Success Metrics from This Session:**\n",
        "```python\n",
        "# Your systematic achievements\n",
        "success_metrics = {\n",
        "    \"theoretical_understanding\": \"Complete taxonomy of fine-tuning approaches\",\n",
        "    \"practical_implementation\": \"Working LoRA fine-tuning pipeline\", \n",
        "    \"efficiency_gains\": \"99%+ parameter reduction while maintaining performance\",\n",
        "    \"production_readiness\": f\"{final_analysis['production_readiness']['percentage']:.0f}% ready\",\n",
        "    \"cost_reduction\": \"90%+ vs full fine-tuning\",\n",
        "    \"time_to_deploy\": \"Minutes, not hours\"\n",
        "}\n",
        "```\n",
        "\n",
        "**ğŸŒ Impact for Low-Resource Languages:**\n",
        "- **Democratization:** Make fine-tuning accessible with limited resources\n",
        "- **Preservation:** Enable digital tools for endangered languages\n",
        "- **Innovation:** Rapid prototyping and iteration cycles\n",
        "- **Sustainability:** Cost-effective long-term maintenance\n",
        "\n",
        "**ğŸš€ You're now equipped to:**\n",
        "- Deploy fine-tuned models in production environments\n",
        "- Make informed decisions about model selection and optimization\n",
        "- Scale to real datasets and production requirements  \n",
        "- Lead fine-tuning projects in academic or industrial settings\n",
        "\n",
        "**ğŸ‰ Congratulations on mastering systematic fine-tuning for low-resource languages!**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
